 Furthermore, we present evaluations on two corpora. cable, variable). Probabilities are smoothed by successive abstraction. /Tilt) which are obtained by Bayesian inversion. This leaves room for interpretation. This is an empirically determined choice. 0.10. This information improves the tagging results. Instead of and equations (3) to (5) are updated accordingly. We evaluate the tagger's performance under several aspects. It was developed at the Saarland University in Saarbriicken2. Part of it was tagged at the IMS Stuttgart. This evaluation only uses the partof-speech annotation and ignores structural annotations. acc. acc. percentage known unknown overall unknowns acc. acc. acc. This evaluation only uses the part-ofspeech annotation. 50,000 sentences (1.2 million tokens). 89.0%). They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. Many thanks go to Hans Uszkoreit for his support during the development of TnT. .
 This is especially relevant for languages with free or flexible word order. The rest of the paper is structured as follows. In section 5, we present our conclusions and suggestions for further research. We write [n] to refer to the set of positive integers up to and including n. In particular, we refer to the elements of the set V as nodes, and to the elements of the set E as edges. We write i --> j to mean that there is an edge from the node i to the node j (i.e., (i, j) E E), and i -->* j to mean that the node i dominates the node j, i.e., that there is a (possibly empty) path from i to j. For a given node i, the set of nodes dominated by i is the yield of i. We use the notation 3r(i) to refer to the projection of i: the yield of i, arranged in ascending order. This ensures that the extended graph always is a tree. The planar graphs Gi are called planes. Moving either edge to a separate graph partitions the original graph into two planes. Bodirsky et al. More precisely, let 7ri be the projection of the node i. Both Graph 3a and Graph 3b are well-nested. Graph 3c is not well-nested. To see this, let T1 be the subtree rooted at the node labelled i, and let T2 be the subtree rooted at j. • The edge degree of G, ed(G), is the maximum among the degrees of the edges in G. To illustrate the notion of edge degree, we return to Figure 3. The non-projective dependency grammar of Kahane et al. or more edges (i, k) in T by edges (j, k), where j ! * i. We discuss these two groups in turn. This difference is illustrated by the graphs displayed in Figure 4. In these situations, planarity reduces to projectivity, so nothing is gained. This particular property does not seem to be mirrored in any linguistic prediction. Well-nestedness also brings computational benefits. For DDT, we see that about 15% of all analyses are non-projective. For both measures, the number of structures drops quickly as the degree increases. This scheme does not restrict the remaining discontinuities at all. Acknowledgements We thank three anonymous reviewers of this paper for their comments. The work of Joakim Nivre is partially supported by the Swedish Research Council. .
 The two traditions complement each other. On the sixsense task, the classifiers averaged 74% correct answers. Each consisted of the sentence containing the target and one sentence preceding it. The resulting strings had an average length of 49 items. 2. Being unambiguous, they do not need to be disambiguated. tuation. As in (2) above, the local window does not extend beyond a sentence boundary. 4. This was done on the assumption that these words are not ambiguous. Removing them undoubtedly made the task more difficult than it would normally be. How much more difficult? An estimate is possible. For example, a pseudoword was created by combining abused/escorted. 1993; Leacock, Towel!, and Voorhees 1993). The training was collected in the following manner. Similarly he collected sentences with names of animals from the ANIMAL category. In these samples, crane and drill appeared under both categories. This approach performs very well, especially with pseudowords and homographs. He repeats this step iteratively. In his paper, Yarowsky suggests WordNet as a source for the seed collocations—a suggestion that we pursue in the next section. The hyponym of a noun is its subordinate, and the relation between a hyponym and its hypernym is an is a kind of relation. For example, maple is a hyponym of tree, which is to say that a maple is a kind of tree. The verbal hierarchy is based on troponymy, the is a manner of relation. For example, stroll is a troponym of walk, which is to say that strolling is a manner of walking. Where direct antonyms exist, adjective synsets point to antonym synsets. Many adjectives, like sultry, have no direct antonyms. So, although sultry has no direct antonym, it has cold as its indirect antonym. Relational adjectives do not have antonyms; instead they point to nouns. Consider the difference between a nervous disorder and a nervous student. Adverbs have synonymy and antonymy relations. When the adverb is morphologically related to an adjective (when an -ly suffix is added to an adjective) and semantically related to the adjective as well, the adverb points to the adjective. The improvement was greater with the smaller training sets. A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms). The sampling process retrieves the &quot;closest&quot; relatives first. The remainder were used for testing. These are shown in Table 6. Line and work, however, showed a substantial decrease in performance. The components can be fit together in the following manner. 1R19528983 and by the Defense Advanced Research Projects Agency, Grant No. N00014-91-1634. .
 Recent years have been exhilarating ones for natural language understanding. 1997). 2001). We compare hand-built and automatically derived resources for providing this information. Finally we draw conclusions and discuss future directions in Section 10. We refer to the roles for a given frame as frame elements. Whereas the subject of blame is often the JUDGE, the direct object of blame can be an EVALUEE (e.g., the poor in “blaming the poor”) or a REASON (e.g., everything in “blame everything on coyotes”). The identity of the JUDGE can also be expressed in a genitive pronoun, (e.g., his in “his praise”) or even an adjective (e.g., critical in “critical praise”). We outline the process here; for more detail see Johnson et al. (2001). The kappa statistic (Siegel and Castellan 1988) varied from .67 to .82. The system will be extended in later sections. 4.1.1 Phrase Type. Different semantic roles tend to be realized by different syntactic categories. “make something up”; PRTs, 2%) and clauses (SBARs, 2%, and Ss, 2%). A sample sentence with parser output (above) and FrameNet annotation (below). Punctuation was ignored in this computation. NP nodes found under S nodes are generally grammatical subjects, and NP nodes under VP nodes are generally objects. For example, town in the sentence “He left town” and yesterday in the sentence “He left yesterday” will both be assigned a governing category of VP. Direct and indirect objects both appear directly under the VP node. For example, in the sentence “He gave me a new hose,” me and a new hose are both assigned a governing category of VP. 4.1.3 Parse Tree Path. Although the path is composed as a string of symbols, our system treats the string as an atomic value. The NP corresponding to He is found as described in Section 4.1.1. Treebank annotation of raising constructions. 4.1.4 Position. 4.1.5 Voice. Each of the patterns requires both a passive auxiliary (some form of to be or to get) and a past participle. 4.1.6 Head Word. Prepositions are considered to be the head words of prepositional phrases. The distributions calculated were simply the empirical distributions from the training data. The variable gov is defined only for noun phrases. had been abducted with him”), and MANNER (MANR). of observations for each conditioning event. Changing the interpolation weights rarely changes the probabilities of the roles enough to change their ranking. The less-specific distributions were used only when no data were present for any more-specific distribution. We return to an analysis of which roles are hardest to classify in Section 9.1. tem. Results are shown in Table 7. in the lattice of Figure 8b, corresponding to the second column of Table 7. It further makes no use of the voice feature. We chose the path feature as the representation of grammatical function in this case. Some sample values from these distributions are shown in Table 8. Results for partial matching are shown in Table 9. [1999]; for application to language modeling, see Gildea and Hofmann [1999]. Deterministic annealing was used to prevent overfitting of the training data. Other grammatical relations besides direct object could be used, as could a set of relations. The second most common category is prepositional phrases. Over the entire test set, this translates into an improvement from 80.4% to 81.2%. For example, person has as hypernyms both life form and causal agent. Pronouns that refer to inanimate, or both animate and inanimate, objects were not included. Only one iteration of training on the unannotated data was performed. An example of null instantiation is the sentence “Have you eaten?” where food is understood. Results were relatively insensitive to the exact value of A. The relevant production in the parse tree is highlighted. 5,719). For example, the verb open assigns different roles to the syntactic subject in He opened the door and The door opened. We suspect that the two seemingly different approaches in fact provide similar information. For example, in our hypothetical example of the sentence He opened the door vs. the sentence The door opened, the verb open would have high priors for the FEGs {AGENT, THEME} and {THEME}, but a low prior for {AGENT}. No lower-probability subtrees will ever be used in a complete parse, and they can be thrown away. 1 100). In theory, the complexity of n-best variations of the Viterbi chart-parsing algorithm is quadratic in n. To this end, we developed a correspondence from frame-specific roles to a set of abstract thematic roles. Even with this enriched set, not all framespecific roles fit neatly into one category. This breakdown is shown in Table 18. Performance broken down by abstract role. The results show a familiar trade-off between coverage and accuracy. Thus, it may not be surprising that they do not correspond to significant generalizations about argument structure. Domain information does not seem to help a great deal, given no information about the frame. The mapping used is necessarily somewhat arbitrary. Many aspects of our system are still quite preliminary. It is not clear how difficult this problem is and how much it overlaps with the general problem of word-sense disambiguation. Penn Treebank constituent (or nonterminal) labels. .
 This measureis neutral with respect to the branching factor. 335-342. Figure1 shows a derivation taken from CCGbank. Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier. We replace all rare words in the training data with their POS-tag. We discuss a simple, but imperfect, solution to this problem in section 7.
 4 Conclusion The experiments above demonstrate a number of important points. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. While substantial work on noun compounds exists in both linguistics (e.g. A similar architecture may be applied to noun compounds. In the experiments below the accuracy of such a system is measured. A range of windowed training schemes are employed below. In fact all authors appear unaware of the other three proposals. I will therefore briefly describe these algorithms. The simplest of these is reported in Pustejovsky et al (1993). Whichever is found is then chosen as the more closely bracketed pair. It uses what I will call the DEPENDENCY MODEL. There does not seem to be any reason why calcium ion should be any more frequent than ion exchange. Both are plausible compounds and regardless of the bracketing, ions are the object of an exchange. Instead, the correct parse depends on whether calcium characterises the ions or mediates the exchange. of left and right-branching compounds. This renders their experiment inconclusive. Table 1 shows the number of each kind and an example of each. In this study, a variety of window sizes are used. Note that windowed counts are asymmetric. The normaliser ensures that all parameters for a head noun sum to unity. If this ratio is greater than unity, then the left-branching analysis is chosen. This will conceal any preference given by the parameters involving 11. The equations above assume these probabilities are uniformly constant. This is true across the whole spectrum of training schemes. For all larger windows, neither model is ever forced to guess. In no case do any of the windowed training schemes outperform the pattern scheme. These are: tion in the probability of categories. Five training schemes have been applied with these extensions. Conceptual association outperforms lexical association, presumably because of its ability to generalise. The experiments above demonstrate a number of important points. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. .
 The paper is organized as follows. 167). 2.1.1 Recoverability. 314 Mitchell P. Marcus et al. 2.1.2 Consistency. 2.1.3 Syntactic Function. 2.1.4 Indeterminacy. 316 Mitchell P. Marcus et al. CC Coordinating conjunction 25. CD Cardinal number 26. UH Interjection 3. DT Determiner 27. EX Existential there 28. FW Foreign word 29. VBG Verb, gerund/present 6. IN Preposition/subordinating participle conjunction 30. JJ Adjective 31. VBP Verb, non-3rd ps. JJR Adjective, comparative 32. VBZ Verb, 3rd ps. JJS Adjective, superlative 33. WDT wh-determiner 10. LS List item marker 34. WP wh-pronoun 11. WP$ Possessive wh-pronoun 12. NN Noun, singular or mass 36. WRB wh-adverb 13. NNS Noun, plural 37. # Pound sign 14. NNP Proper noun, singular 38. $ Dollar sign 15. NNPS Proper noun, plural 39.. Sentence-final punctuation 16. PDT Predeterminer 40. , Comma 17. POS Possessive nding 41. : Colon, semi-colon 18. PRP Personal pronoun 42. PP$ Possessive pronoun 43. RB Adverb 44. " Straight double quote 21. RBR Adverb, comparative 45. Left open single quote 22. RBS Adverb, superlative 46. RP Particle 47. Right close single quote 24. SYM Symbol (mathematical or scientific) 48. 2.3.2 Manual Correction Stage. 318 Mitchell P. Marcus et al. No other effects or interactions were signif- icant. 44 minutes per 1,000 words. 320 Mitchell P. Marcus et al. X Null elements 2. always errs on the side of caution. and boatload of... is not. Marcus et al. The interface correctly reindents the structure whenever necessary. Delete a pair of constituent brackets. Change the label of a constituent. 326 Mitchell P. Marcus et al. E. B. Dubois, and Ralph Waldo Emerson. 1991; Pereira and Schabes 1992). These structures need to be regularized. Marcus et al. N0014-85-K0018, byDARPA and AFOSR jointly under grant No. AFOSR-90-0066 and by ARO grant No. DAAL 03-89-C0031 PRI. We gratefully acknowledge this support. References Brill, Eric (1991). June 1990, 275-282. Church, Kenneth W. (1980). Church, Kenneth W. (1988). Francis, W. Nelson (1964). Brown University, Providence RI. Francis, W. Nelson, and Ku~era, Henry (1982). Houghton Mifflin. Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey (1987). Hindle, Donald (1983). Hindle, Donald (1989). The GNU Emacs Lisp reference manual. Free Software Foundation, Cambridge, MA. (1990). Niv, Michael (1991). Pereira, Fernando, and Schabes, Yves (1992). Santorini, Beatrice (1990). Veilleux, N. M., and Ostendorf, Mari (1992). .
 At this time, we do not c.onsider the imerrelationships among tile concepts. Finally, we end this paper with a conclusion. Figure 1: A Nov. 496 sulnlllary. of nonrelewmt exl;s for a given topic. :;(;n(:(~ of t i indi(:~Lt(.~. These flagments are each judged 1)y a hmnan judge. Several conclusions can be drawn directly. ) , ~ , ,wd ln~;  :?12:1. i  67  ,3(~t~ ~h+. highest sentence score. original l;(}xI. Kenneth Church and Patrick Hanks. Thomas Cover and Joy A. Thomas. An overview of the FRUMP system. In ~2mdy G. Lehnert and Martin H. Lawrence Erlbaum A.s- so(lares. A~i:eurate methods for the statistics of surprise and coincidence. Computa- tional Linguistics, 19:61--74. Compu- tational Linguistics, 23:33-64. Eduard Hovy and Chin-Yew Lin. Automated text summarization i SUMMAIRIST. Kevin Knight and Steve K. Luk. 500 -~  ..~:,., . - -=-"  _..  _ . 1,* +  .~  *+-  . ; -5; , :~:;  . :~.7~.~ ~ ~ ^ - -~- . o 400OO f ,  " +- ~-" + ~ -, "~2x-+, [ ? : [ - ...... ; ""7 ........ -a  + -a--  -#. -~-- - .a  ~  . =-~++:7:: ~ -:~ +--~ ....... " ~5_~Ztt::~:ll;: ; i I " , ; . A  / , -?~-  <F" ~. summary length for all fimr topics. I <,at tT_.._,~. ~~~~:~- i~-  T -  o.ae~ ] , ) . Values in the 1)aselin(,. rows are F-measure s(:ores. %~(:hnical I/,ol)orl; MTR98W0000138, The MITRE Corporation. Christopher Manning and Hinrich Schiitzc. 1999. Kathh~(m M(:K(!own and l)rag(mfir R. I ladev. I I (  ra t ,  i l l g  S l l l l l l l l ; l l  i ( : s  o f  I l t l l l t ,  i  [ ) l  [~  l l ( !~vs  articles. In hMtu.iet~t Mani and Mark T. Ellen Riloff and Jeffrey Lorenzen. Kluwer Academic Publishc, r.q. Artificial Intelligence ,Journal, 85, August. Introduction to information extraction. http://www.mu(:.sai(:.(:om. Jinxi Xu and W.
 Consider the two newspaper articles in Figure 1. Discovering these links automatically is clearly non-trivial. Thus, they perform poorly on comparable, non-parallel texts. We demonstrate the quality of our A pair of comparable texts. Before concluding, we discuss related work. This step of the process emphasizes recall rather than precision. For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents. We first index all the English documents into a database. However, having no window at all leads to a decrease in the overall performance of the system. The resulting model has free parameters λj, the feature weights. 1990). Each figure contains two alignments. was computed automatically. shortcomings of the model used to generate the alignments. We follow Brown et al. Most likely, these connections were produced because of a lack of better alternatives. Such a span may contain a few words without any connection (a small percentage of the length of the span), but no word with a connection outside the span. 1993). Both numbers are expressed as percentages. We performed evaluation experiments to account for both these factors. From each initial, out-of-domain corpus, we learn a dictionary. We train two classifiers (one on each training set) and evaluate both of them on the test set. Precision and recall of the Chinese-English classifiers. For each initial corpus size, the first column shows the coverage of that initial corpus, and the second column shows the coverage of the initial corpus plus the extracted corpus. We used the AgentBuilder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling. In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7. 2002) on four reference translations. Clearly, we have access to no UpperBound system in this case. features. 2004). Utiyama et. Utiyama et al. (2003) use the BM25 (Robertson and Walker 1994) similarity measure. Utiyama et al. Therefore, they pose hard problems for the dynamic programming alignment approach. In contrast, our method is more robust. The approach of Fung and Cheung (2004) is a simpler version of ours. The evaluation methodologies of these previous approaches are less direct than ours. Utiyama et al. A line of research that is both complementary and related to ours is that of Resnik and Smith (2003). The computational processes involved in our system are quite modest. The task can be easily parallelized for increased speed. For example, if the domain were that of technical manuals, one would cluster printer manuals and aircraft manuals separately. This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276. Any remaining errors are of course our own. .
 and literary analysis, and information retrieval. The following sections survey the approaches applied to date. ), locations (city, country, continent, etc. ), and other similar distinctions, in their networks. 1990). Wilks et al. chance: 39%) (Veronis and Ide 1995). Guthrie et al. (p. 238). 2.3.2 Thesauri. to inhale, breathe in, sniff, etc. ), and question (to doubt vs. to ask a question) with high reliability. Resnik 1995b). [1997] and Mahesh, Nirenburg, and Beale [1997]). They report a success rate of 97%. 1956; etc.). 313). For a discussion, see Ide and Walker (1992). (1994), Niwa and Nitta (1994), Lehman (1994), among others. 1997). 1997). Brown et al. Brown et al. 16 See the survey of methods in Chen and Goodman (1996). data. 3.1.1 Microcontext. We examine below some of the other parameters. Distance. Collocation. The term was popularized by J. R. . . . .&quot; is more workable in computational terms. . . is greater than chance (Berry-Rogghe 1973). [1982], De Groot [1983], Lupker [19841). Syntactic Relations. (Hayes 1977a, 1997b; Wilks 1973 and 1975b; Hirst 1987). topical context and attempt to assess the contribution of each. specialized usage, etc.). 3.2.1 The Bank Model. 3.2.2 Granularity. 43). 1992; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993). head). Harman [1993, 1995]). 3.3.1 Evaluation In Vitro. 3.3.2 Evaluation In Vivo. Hopefully, this will contribute to further progress on WSD. .
 An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. In contrast. Reichman [19811 and Grosz 110811 discuss some of these. Rosa, not 'Rosa' is the Cb(2b). Consider. 4. 4.1. is required to be at least old. it every day. I he played some role in the House. husband is kind to her. NO. isn't. The can you're referring to isn't her husband. Her husband to He her isn't her husband. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). 5. Ferreting out the in a given situation requires of belief and the like. A discussion of issues is beyond the this paper. They are principles that must be elicited from the study of itself. (he=John) (8b) He vas annoyed by John's call. An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. In contrast. Rosa, not 'Rosa' is the Cb(2b). Consider. He plays with it every day. (11 preferred; VT. s2: I thought he played some important role in the House. (6a) Her husband is kind to her. (61)) NO. he isn't. The can you're referring to isn't her husband. (7a) Her husband is kind to her. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). (he=John) (8b) He vas annoyed by John's call. (9-2)Carl thinks he's studying for his exams. (9-3)But I think he went to the Cape with Linda. On our account, Jeff is Cb(9-2) and there is no problem. We will consider each case separately. .
 We will write event slots in shorthand as (X pleads) or (pleads X) for (pleads, subject) and (pleads, object). Our previous work, however, has two major limitations. Second, the model only represents one participant (the protagonist). We discuss both of these extensions next. Take this example of an actual chain from an article in our training data. However, (charge X) shares many arguments with (accuse X), (search X) and (suspect X) (e.g., criminal and suspect). As an illustration, consider the verb arrest. The following sections describe a model that addresses both goals. We now constrain the protagonist to be of a certain type or role. workers, the troubles really set in when they apply for unemployment benefits. Many workers find their benefits challenged. The four bolded terms are coreferential and (hopefully) identified by coreference. Our algorithm chooses the head word of each phrase and ignores the pronouns. It then chooses the most frequent head word as the most salient mention. In this example, the most salient term is workers. Growing a chain by one adds the highest scoring event. If (push X) is in one chain, (Y push) is in another. Further, each c must be unique for each slot of a single verb. The next section describes how to learn these schemas. However, the object of (pull over A) is not present in any of the other chains. Police pull over cars, but this schema does not have a chain involving cars. In contrast, (Y search) scores well with the ‘police’ chain and (search X) scores well in the ‘defendant’ chain too. We parse the text into dependency graphs and resolve coreferences. A low Q was chosen to limit chain splitting. This amounted to approximately 1800 verbs from which we show the top 20. Each node shape is a chain in the schema. We examined the 33% of verbs that occurred in a different frame. The remaining arguments were considered incorrect. shows some of these errors. They should have been split into their own chain within the schema. In our example, drug and product are correct Product arguments. An incorrect argument is test, as it was judged that a test is not a product. We evaluated all 20 schemas. This number includes Person and Organization names as correct fillers. Most of the errors appear to be from parsing mistakes. Several resulted from confusing objects with adjuncts. Others misattached modifiers, such as including most as an argument. We use the OpenNLP1 coreference engine to resolve entity mentions. We used a smaller development set of size 17 to tune parameters. We see a 6.9% gain at 2004 when both lines trend upwards. We see a 3.3% gain at 2004. The tasks mutually inform each other. This work is funded in part by NSF (IIS-0811974). We thank the reviewers and the Stanford NLP Group for helpful suggestions. .
 We evaluate the technique for named-entity tagging. At a recent meeting, we presented name-tagging technology to a potential user. Nevertheless, it did not meet the user's needs. However, this user's problems were too dynamic for that much setup time. Fortunately, a second line of recent research provided a potential solution. conditional random fields) to estimate the model parameters. We implemented this algorithm twice as part of our work. The root node defines a cluster containing the entire vocabulary. Our decision was based on three criteria. algorithm exactly as described by Collins. However, we did not implement cross-validation to determine when to stop training. We suspect that these simplifications may have cost several tenths of a point in performance. Our model uses a total of 19 classes of features. The remaining twelve encode cluster membership. Long prefixes specify long paths and small clusters. We used 4 different prefix lengths: 8 bit, 12 bit, 16 bit, and 20 bit. Thus, the clusters decrease in size by about a factor of 16 at each level. The complete set of features is given in Table 2. Scoring was performed using the MUC scorer. This portion of the collection contains approximately 100 million words. Results are shown in Figure 1. Second, we consider the impact of word clusters. Third, we consider the impact of active learning. Much work remains to be done. conditional random fields estimated with conjugate-gradient training). For the moment, we find the initial results encouraging. .
 254)). 434). IZxamples are "kind" and "careful". GtzM: gradable. Matt: manually identilied. Auto: automalically identified. 4.3671.. References I)ouglas M. Bates and 1)onald G. Watts. Wiley, New York. Edwin L. Battistella. Markedness: 7he Evahiative Siq~etwtructure qfLanguage. Rebecca Bruce and ,lanyce Wiebe. Kenneth W. Church. Association for Computa- tional Linguistics. Vasileios Hatzivassiloglou and Kathlcen R. McKeown. Association re," Computational Linguistics. Jerrold J. Kalz. Sema,tic Theory. Harper and Row, New York. Kevin Knight and Steve K. Luk. Adrienne Lehrer. Sema,tics, volume 1. Cambridge University Press, Cambridge, England. K. Mahesh and S. Nirenburg. Coml;tttatioltal Lin,~?uistics, 19(2):313-330, June. A Complvhe,sive Grammar elthe English l.cmguage. Longman, London and New York. Sanlner and Diane E. l)uffy. Springer-Verlag, New York. l~hilosol;hy qfScie,ce, 2:93-116. Reprinted in (Sapir, 1949). University of California Press, Be,keley, California. Edited by David G. Mat> delbat, m. John M. Sinclair (editor in chiet). Collins, London. J. Wiebe, R. Bruce, and T. OHara. .
 There are 45 distinct grammatical tags. In this set-up, there are no unknown words. Figure 1 shows prior results for this problem. Goldberg et al. For the rest of this paper, we will limit ourselves to a 2-gram tag model. We also look at things more globally. Bayesian sparse priors aim to create small models. The answer is 459. We create a network of possible taggings, and we assign a binary variable to each link in the network. We accomplish this by requiring the sum of the links entering each node to equal to the sum of the links leaving each node. We constrain link variable assignments to respect those grammar and dictionary variables. We leave this as part of future work. Fortunately, we can use EM for that. So we see a benefit to our explicit small-model approach. In our experiments, we choose the first solution returned by CPLEX. Figure 4 shows some examples. The precision/recall of the observed dictionary on the other hand, is not affected by much. (2008). The InitEM-HMM system from Goldberg et al. We do this for every unknown word, and eventually we have a dictionary containing entries for all the words. State Department”. .
 Our prosperity and stability underpin our way of life. firdtittifirg.g. I fully support his views. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. Figure 4: Example translation outputs. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. However, the order-of-magnitude improvements are immediately apparent. The improvement in speed does not appear to impair accuracy significantly. The generalized version permits such strategies. (Dagan, Church, and Gale, 1993). However, the search space remains the same. The model we describe in this paper, like Dagan et al. In this section we summarize the main properties of BTGs and ITGs. (Dagan, Church, and Gale, 1993). These parameterizations were only intended to crudely model word-order variation. In a SBTG, a probability is associated with each production. This in turn often depends on what the target language is. The algorithm is given below. Our work differs from the ID/LP work in several important respects. Our prosperity and stability underpin our way of life. firdtittifirg.g. I fully support his views. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. However, the order-of-magnitude improvements are immediately apparent. The improvement in speed does not appear to impair accuracy significantly. The generalized version permits such strategies. .
 Levinger et al. ambiguating 35% of the ambiguous words with an accuracy of 99.6%. After this stage, the accuracy is about 93%. Segal reports an accuracy of 95%. Bar-Haim et al. The segmentation step reaches an accuracy of 96.74%. (2004). We first formalize the output representation and then describe the algorithms. Fig. An emission is denoted in this figure by its symbol, its state index, directed edges from its previous emissions, and directed edges to its next emissions. 1. 5,7). Fig. Fig. The test corpus contains about 30K words. (1995) to estimate the likelihood of each analysis. Unknown words account for a significant chunk of the errors. We extended the lexicon to include missing and none lexemes of the closed sets. The model was trained over this set. This strategy accounts for about half of the 7.5% unknown words. .
 It also dramatically increases the speed of the parser. 2.1 The Lexical Category Set. 2.2 The Tagging Model. The context is a 5-word window surrounding the target word. Features are defined for each word in the window and for the POS tag of each word. However, in the sentence Mr. 3.1 Model Estimation. . . , S m, to gether with gold standard normal-form derivations, d1, . . . , dm. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster. Section 4.2discusses the use of various settings on the supertag ger. 4.1 Normal-Form Constraints. 4.2 Results (Space Requirements). The disk usage is the space taken on disk by the charts,and the memory usage is the space taken in memory during the estimation process. The threshold was set at300,000 nodes in the chart. Osborne (2000). In this way the parser interacts much more closely with the supertagger. 5.1 Results (Parse Times). 5.2 Comparison with Other Work. Sarkar etal. The techniques we have presented inthis paper increase the speed of the parser by a factor of 77. This could further increase parsing effficiency. .
 The bacteria track consists of two tasks, BB and BI. The texts are Web pages about the state of the art knowledge about bacterial species. Their interactions are described through ten relation types. The main tasks are characterized in Table 1. The CO task seeks to address this problem. to the recognition of Site arguments (cf. The remaining 17 teams participated in only single tasks. GEa, GEf and GEp may be from different teams). This indicates a beneficial role from focused efforts like BioNLP-ST. .
 The system is not always so successful. Following Brown et al. We can compute neither factors precisely. We call the model from which we compute Pr (E) the language model and that from which we compute Pr (FIE) the translation model. Brown et al. In Il doute que les notres gagnent, which. If so, then is is less likely to be translated as est than if not. in the context of w. When a. People make decisions, speeches, and acquaintances, they do not take them. Figure 3 shows our results for the verb vouloir. In polite English, one says I would like so and so more commonly than I would want so and so. Finally, consider the English word cent. Since the entropy of the translation of a. Our method asks a. single question about a single word of context. .
 However, few of the above mentioned formalisms have large scale implementations. In a different approach, Hwa et al. The percentage for head crossings is 12.62% and that of modifier crossings is 9.22%. Each node in a tree stands for a lemma in a dependency tree. An illustration is given below (Chinese in pinyin form). The above heuristics are a set of real valued numbers. are not yet filled. A full-fledged SDIG remains a goal for future research. Note that this is a nondeterministic process. We have lation model” (TM) and the “language model” (LM). Each circle stands for an ET. The solid lines denote the syntactical dependencies while the dashed arrows denote the statistical dependencies. Given t , there could be multiple decompositions. We have: an HMM. For efficiency reasons, we use maximum approximation for (3). In Eq. In Eq. Hence, we have: which is linear to the input size. The parser was trained using the Penn English/Chinese Treebanks. 2001) to decode the IBM models. The results are shown in Figure 9. The score types “I” and “C” stand for individual and cumulative n-gram scores. .
 (2005), Galley et al. (2006), and Liu et al. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. Each cell in turn maintains a list of proven items. We converted pdf, doc, html, asp, php, etc. files into text, and preserved the directory structure of the web crawl. We filtered and de-duplcated the resulting parallel corpus. (2009b). HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. .
 But it resembles partial parsing in being robust, efficient and deterministic. In addition, arcs may be labeled with specific dependency types. graphs without labeled arcs, but the results will apply to labeled dependency graphs as well. wz H wj iff wz —* wj or wj —* wz. 2. 3. For Shift, the only condition is that the input list is non-empty. 4. AS = {(wi, wj) E A|wi, wj E 51. In experiments reported in Nivre et al. The results can be seen in Table 2. Thanks to three anonymous reviewers for constructive comments on the submitted paper. .
 Furthermore only event expressions that occur within the ETL areconsidered. The task is to supply this label. Tags the time expressions in the text. It is identical to the TIMEX3 tag in TimeML. EVENT. Tags the event expressions in the text. Events can be denoted by verbs,nouns or adjectives. He taught on Wednesday and Friday). Thiscomplication was not necessary for the Tem pEval data. ? TLINK. details. The strict scoring scheme only counts exact matches as success. This section gives a short description of the participating systems. But for task A, the winners barely edge out the rest of the field. elements that needed to be linked for the TempEval task. We expect both inter-annotator agreement and system per formance to be higher on this subtask. Instead the entire timeline must be evaluated. .
 Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. I(lavans, M. Liberman, M. Jl4arcus, S. Roukos, B. Santorini, T. (T intersection C) / T. our individuM goals, and our standard. is produced by hand. not split), i.e. (S (N (N Sue)) (V (V sees) (N (N Tom))))  = : ( ( (Sue)  ) ( ( sees) ( (Tom)  ) ) )e tc . I Ineer-do-welll 308 2. Example: (will probably have) (seen Milton) -> ( probably ) (seen Milton) -> (probably seen Milton) 3. of cutting back spending 4. cutting back spending 5.
 In this survey, we make an attempt at such an elaboration. Both these treatments provide different but valuable perspectives on paraphrasing. Paraphrases may occur at several levels. However, lexical paraphrasing cannot be restricted strictly to the concept of synonymy. Culicover (1968) describes some common forms of sentential paraphrases. 2004), handwritten rules (Fujita et al. Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3.1 Query and Pattern Expansion. 2006). 2002). R: We must bear in mind the community as a whole. 2006; Zhou, Lin, and Hovy 2006). Zhou et al. 1.3.3 Machine Translation. Madnani et al. Both of these task formulations fall under the category of paraphrase detection or recognition. 2007; Sekine et al. 2007; Giampiccolo et al. 2008). This must be an important consideration when building the proposed entailment engine. 2005; Garoufi 2007). We devote the following section to such an explanation. Algorithm 1 shows the details of the induction process. Summary. Extract all n-grams from the corpus longer than a pre-stipulated length. Compute a lexical anchor for each extracted n-gram. on both sides and they attach the nearest pair of entities to the anchor. Consider the first dependency tree in Figure 2. Next is the semantic relation object connecting a verb to a noun and we append that. For example, whereas verbs frequently tend to have several modifiers, nouns tend to have no more than one. Therefore, the algorithm tends to perform better for paths with verbal roots. 2007). Figure 4 shows the basic steps of the algorithm. S2: Emma cried and he tried to console her. (tried, tried), (her, her) may be extracted as positive examples and (tried, Emma), (tried, console) may be extracted as negative examples. The second tuple is identical for this case. Note that the tags of identical tokens are indicated as such by subscripts on the POS tags. Next, each sentence in each of the groups is parsed. All the parse trees are then iteratively merged into a shared forest. However, the merging only allows identical constituents to be considered as paraphrases. Note that for clarity, not all constituents are expanded fully. Leaf nodes with two entries represent paraphrases. (b) The word lattice generated by linearizing the forest in (a). sentences in the group. In general, it has several advantages. However, no experiments are performed with the automatic translations. 1990). em1 , E2 = e12e22 ... en2. Algorithm 3 shows how to Algorithm 3 (Quirk, Dolan, and Brockett 2004). Summary. This computation uses the lexical replacement probabilities computed in Step 2. The probability values associated with each edge are not shown for the sake of clarity. Shinyama et al. However, it has some obvious disadvantages. Algorithm 4 shows some details of how their technique works. Summary. Gather topically related sentences from C1 into clusters. Do the same for C2. Compare all lattice pairs and output those likely to be paraphrastic. Nodes with thick incoming edges occur in all sentences. multiple rewritings (paraphrases) for S. Shen et al. try to bring syntactic features into the mix. Shen et al. Algorithm 5 (Bannard and Callison-Burch 2005). Summary. The alignment notation from Algorithm 3 is employed. Examples: (ten ton, ten tons), (caused clouds, causing clouds). Madnani et al. In contrast, Madnani et al. Two nodes in the graph are connected to each other if they are aligned to each other. S2: The euro rose above $1.18, the highest level since its launch in January 1999. Despite the somewhat loosely defined guidelines, the inter-annotator agreement for the task was 84%. The claim of reduced annotation cost is not necessarily borne out by the observations. This led to a significant difference in annotation time—with some annotations taking almost twice as long as others. standard test sets and manual as well as automated metrics, the task of automated paraphrasing does not. 2007) and word sense disambiguation (Senseval), suggests otherwise. This usage pattern does not allow researchers in one community to share the lessons learned with those from other communities. Therefore, other alternatives are needed. Indeed, Callison-Burch et al. Addressing this deficiency should be a crucial consideration for any future community-wide evaluation effort. 2004; Dras and Yamamoto 2005; Sekine et al. The Influence of the Web. Combining Multiple Sources of Information. For example, Zhao et al. (2007). Zhao et al. Use of SMT Machinery. Domain-Specific Paraphrasing. .
 ,fj, ... , ei, ... , eI. Here, I is the length of the target string, and J is the length of the source string. substrings that are common enough to be observed on training data. Alshawi et al. Similarly, S(z) denotes the string in z. 3The notational convention will be as follows. We use lex(·) to denote lexical weighting. We approach the decoding problem as a bottom-up beam search. Our experiments were on Chinese-to-English translation. We modified it to conform to NIST’s current definition of the BLEU brevity penalty. We find that φ(f|e) (i.e. h2) is not a helpful feature for Lynx. 2004AA114010). Thanks to Dr. Yajuan Lv for many helpful comments on an earlier draft of this paper. .
 The CLTE task aims at prompting research to fill this gap. cross-lingual entailment rules such as “perro”→“animal”). Only pairs with agreement between two expert annotators were retained. English, Spanish, German, Italian, and French), and T2s are in English. the number of correct judgments out of the total number of judgments in the test set. Baseline results are reported in Table 2. Accuracy results are reported in Table 3. The comparison with baselines results leads to interesting observations. 0.43). less intermingled with the other classes). Similarity measures (e.g. Binary entailment decisions are then heuristically combined into single decisions. This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).
 D. Kernigan and Gale, 1990; Church and Gale, 1991; Mayes and F. Damerau, 1991). We build a letter-to-phone model automatically from a dictionary. Section 6 contains conclusions and ideas for future work. Since the denominator is constant, this is the same as maximizing P (r)P (wjr). We do not pursue this, here, however. The next section describes our letter-to-phone model. The number of letters in the context to the left and right varies. We used from 0 to 4 letters on each side. We trained the letter-to-phone models using the training splits and tested on the test splits. For example, of the two rules with contexts A.B. Adding a symbol for interior of word produced a gain in accuracy. Prior to adding this feature, we had features for beginning and end of word. As a comparison, the best accuracy achieved by Jiang et al. P (wjr, pron r) = P(wjpron r). Since we do not model these marginal probabilities, we drop the latter factor. The table shows N-best accuracy results. Larger context sizes neither helped nor hurt accuracy. .
 We present two algorithms. is a person). 97). and Microsoft are organizations). For example, in .., says Mr. Cooper, a vice president of.. Cooper is of type Person. We present two algorithms. 2. For example, ... (e.g., N.Y. would contribute this feature, IBM would not). Petry nonalpha= . , for A. T.&T. nonalpha.. . . ). context=x The context for the entity. 0). . . Xim, } associated with the ith example. step 3. This intuition is born out by the experimental results. Consider the case where IX]. The problem is a binary classification problem. (5) and ht into Equ. (6), with W+ > W_. We make the assumption that for each example, both xi,. (3)), with one term for each classifier. Put another way, the minimum of Equ. As in boosting, the algorithm works in rounds. Equ. 2. For t = 1, T and for j = 1, 2: where 4 = exp(-jg'(xj,i)). Note, however, that there might be situations in which Zco in fact increases. One implementation issue deserves some elaboration. Note that in our formalism a weakhypothesis can abstain. The algorithm in Fig. Again, this deserves further investigation. 123 examples fell into the noise category. This left 962 examples, of which 85 were noise. 2 for the accuracy of the different methods. Fig. (3) shows learning curves for CoBoost. The test accuracy more or less asymptotes. .
 The rest of this paper is organized as follows. (Devijver and Kittler, 1982)). a cluster of one. The two closest clusters are merged to form a new cluster that replaces the two merged clusters. Merging of the two closest clusters continues until only some specified number of clusters remain. However, our data does not immediately lend itself to a distance—based interpretation. Suppose that we have N observations in a sample where each observation has q features. For example, in Figure 1 we have four observations. We record the values of three nominal features for each observation. However, we will continue to investigate the appropriateness of this assumption. At the heart of the EM Algorithm lies the Qfunction. These, steps iterate until the parameter estimates 0 and 0i converge. The M-step simplifies to the calculation of new parameter estimates from these counts. The line data comes from both the ACL/DCI WSJ corpus and the American Printing House for the Blind corpus. Every experiment utilizes all of the sentences available for each word. Morphology The feature M represents the morphology of the ambiguous word. For nouns, M is binary indicating singular or plural. For verbs, the value of M indicates the tense of the verb and can have up to 7 possible values. PR, represents the POS of the word i positions to the right. Co—occurrences Features of the form Ci are binary co-occurrence features. Frequency based features like this one contain little information about low frequency classes. Note that million and company occur frequently. All features of this form have 21 possible values. This point will be elaborated on in Section 6.1. We return to this point in Section 6.2. This makes it difficult to learn their distributions without prior knowledge. This is true regardless of the feature set employed. In few cases is the standard deviation very small. This will be explored in future work. Bruce, 1997a)). For plant, the collocations manufacturing plant and living plant make such a distinction. (Li, Szpakowicz, and Matwin, 1995)). (Guthrie et al., 1991) propose that neighborhoods be subject dependent. (Levinson, Rabiner, and Sondhi, 1983), (Kupiec, 1992)), (Jelinek, 1990)). .
 Language varies significantly across different genres, topics, styles, etc. We assume two basic settings. sjK; and 9k is the translation of the kth target phrase tk. 4. We now describe each aspect of this algorithm in more detail. An alternative, which we have not explored, would be to cluster the corpus automatically according to topic. Given a set of metrics {D1, ... Here we assume that Qi absorbs a normalization constant, so that the Ac’s sum to 1. Table 1 summarizes the corpora used. All results given in this section are BLEU scores. newswire, sp = speeches, ed = editorial, ng = newsgroup, bn = broadcast news, and bc = broadcast conversation. Due to this result, all experiments we describe below involve linear mixtures only. We focus on this metric for most of the experiments that follow. the NIST04-nw development set. In additional tests we verified that this also holds for the test corpus. tion on the NIST04-nw development set. However, LM adaptation improves over the baseline by up to a BLEU point. Global weights were tuned specifically for each of these conditions. They rely on a perplexity heuristic to determine an optimal size for the relevant subset. This resulted in gains of around one BLEU point. .
 But Koehn et al. rels. rels. Rewriting begins with a pair of linked start symbols. 5. Now we must hypothesize weights for all the derivations. We prune the search space in several ways. This makes the decoding algorithm asymptotically linear-time. Our experiments were on Mandarin-to-English translation. Koehn et al. This would potentially improve both accuracy and efficiency. S. D. G. .
 Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. Also, WordNet enumerates some metaphorical senses of some verbs. Section 3 walks through CorMet’s process in two examples. CorMet obtains documents by submitting queries to the Google search engine. Each query incorporates only a few keywords in order to maximize the number of distinct possible queries. For the verb attack, for instance, acceptable forms are attacked and attacking, but not attack and attacks, which are more likely to be nouns. The documents are processed to remove embedded scripts and HTML tags. There are three constraints on CorMet’s selectional-preference-learning algorithm. Second, it should be able to work around WordNet’s lacunae. Finally, there should be a reasonable metric for comparing the similarity between selectional preferences. Resnik’s algorithm works around this problem by approximating a word class distribution from the word form distribution. There are typically fewer than 100 second-order clusters (i.e., clusters of clusters) per domain. Table 2 shows a MILITARY cluster. Nevertheless, there are many words that intuitively should have a common parent but do not. nodes that specifies a partition of the ontology’s leaf nodes, where a node stands for all the leaf nodes descended from it. The method chooses among possible tree cuts according to minimum-description-length criteria. There are similar objections to similar approaches such as that of Carroll and McCarthy (2000). In FINANCE, these verbs select for assets. Structure flow in the opposite direction is selection strength(X, β,A). The polarity for α and β is the difference in the two quantities. Instances like this are easy to filter out because their polarity is zero. In the idea domain, ideas are mapped to objects, minds are mapped to locations, and communications are mapped to paths. Confidence is a function of three things. See Mason (2002) for a more detailed account. The right-hand sides have clusters of characteristic nodes. The second represents a somewhat less intuitive mapping from liquids to institutions. This metaphor is driven primarily by institutions’ capacity to dissolve. Of course, this mapping is incorrect insofar as solids undergo dissolution, not liquids. The third mapping characterizes communication as a liquid. The fourth mapping is from containers to organizations. This mapping complements the first one: As liquids flow into containers, so money flows into organizations. The heterogeneity of the source concepts seems to be driven by the heterogeneity of possible military targets. The mapping fortification → illness represents the mapping of targetable strongholds to disease. Illnesses are conceived of as fortifications besieged by treatment. About a fifth of the Master Metaphor List meets this constraint. Some of these test cases are marked successes. Some test cases were disappointing. CorMet found no mapping between THEORY and ARCHITECTURE. This seems to be an artifact of the low-quality corpora obtained for these domains. In all cases, the polarization is zero. This can be interpreted as an encouraging lack of false positives. Met* discriminates among metonymic, metaphorical, literal, and anomalous language. The verb drink’s preference for an animal subject and a liquid object are represented as (animal, drink, liquid). For example, the car drinks gasoline maps to the vector (car, drink, gasoline). MetaBank starts with a knowledge base of metaphors based on the Master Metaphor List. MetaBank can search a corpus for one metaphor or scan a large corpus for any metaphorical content. The nodes publication and publisher, for instance, have paper, newspaper, and magazine as common descendants. It is reasonably accurate despite the noisiness of many of its components. .
 Much of this work, however, has occurred in monolingual contexts. A recent extended abstract, developed concurrently by Ni et al. Outside of the field of topic modeling, Kawaba et al. , L. Anew document tuple w = (w1, ... These tasks can either be accomplished by averaging over samples of Φ1, . . . , ΦL and αm from P(Φ1, ... , ΦL, αm  |W', β) or by evaluating a point estimate. . . These texts consist of roughly a decade of proceedings of the European parliament. The remaining collection consists of over 121 million words. Details by language are shown in Table 1. Hyperparameters αm are re-estimated every 10 Gibbs iterations. tokens in each of the languages. These tend to be very short, formulaic parliamentary responses, however. , ΦL and αm. However, recently developed methods provide efficient, accurate estimates of this probability. We use the “left-to-right” method of (Wallach et al., 2009). There is substantial variation between languages. English has a larger tail, with non-zero counts in all but 16 topics. For every tuple in S, we assign each document in that tuple to a new singledocument tuple. Hyperparameters αm are re-estimated every 10 iterations. The lower the divergence, the more similar the distributions are to each other. (2008). We do not consider multi-word terms. We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica. We use both Jensen-Shannon divergence and cosine distance. For each document in the query language we rank all documents in the target language and record the rank of the actual translation. Cosine-based rankings are significantly worse. It is important to note that the length of documents matters. Results vary by language. We preprocessed the data by removing tables, references, images and info-boxes. We dropped all articles in non-English languages that did not link to an English article. We present results for a PLTM with 400 topics. Subtle differences of sentiment may be below the granularity of the model. The authors thank Limin Yao, who was involved in early stages of this project. .
 equal. In this section, we formally define the grief of each component, and a mechanism for its minimization. However, the default aspect predictions 9[1] ... y[m] may not accord with the agreement model. Alternatively, we can predict some consensus y0 (i.e. See (Crammer and Singer, 2001) for explanation and analysis of this update rule. The rest of the instances are labeled as negative. More specifically, we extract all unigrams and bigrams, discarding those that appear fewer than three times. This process yields about 30,000 features. Agreement Model The agreement model also operates over lexicalized features. First we introduce some notation. The food was good, and so was the ambience. The food was bad, but not the ambience. The food was bad, and so was the ambience. Consider any model (w, b = (b)). Note that w · x1 < b and w · x2 > b together imply that w3 < 0, whereas w · x3 > b and w · x4 < b together imply that w3 > 0. These ranks are provided by consumers who wrote original reviews. We randomly select 3,488 reviews for training, 500 for development and 500 for testing. We tune the value of α on the development set. The most frequent rank set (5, 5, 5, 5, 5) accounts for 30.5% of the training set. However, no other rank set comprises more than 5% of the data. Thus an agreementbased approach is natural and relevant. Lower values of this measure cormN respond to a better performance of the algorithm. Both of these methods are described in detail in Section 2. The gain in performance is observed across all five aspects. Model Analysis We separately analyze our percomputed separately on cases of actual consensus and actual disagreement. As Table 2 shows, we outperform the PRANK baseline in both cases. Thanks also to Vasumathi Raman for programming assistance. .
 Each tier builds on the previous tier’s entity cluster output. [we]’re checking our facts on that one. ... sues. (2007) and Poon and Domingos (2008). The other corpora are reserved for testing. We refer the interested reader to (X. We first describe how each model selects candidate mentions, and then describe the models themselves. after a month-long courtship, [they] agreed to buy quaker oats... , mj�}; mi E Cj. The intuition behind this heuristic is two-fold. Search Pruning – Finally, we prune the search space using discourse salience. [AFP]. [Israeli]. We constrain this feature by enforcing a conjunction with the features below. intervene in the [Florida Supreme Court]’s move ... he had turned onto [the correct runway] but pilots behind him say he turned onto [the wrong runway]. Sanders Sauls}. These are crucial factors for pronominal coreference. Person – we assign person attributes only to pronouns. NER label – from the Stanford NER. The sieve architecture offers benefits beyond improved accuracy. The above analysis illustrates that our next effort should focus on improving recall. Not surprisingly, the causes are unique to each type. Pronoun errors come in two forms. FA8750-09-C-0181. .
 23(1):13–31. Cicchetti, Domenic V. and Alvan R. Feinstein. 1990. High agreement but low kappa: II. Resolving the paradoxes. of Clinical 43(6):551–558. Cohen, Jacob. 1960. A coefficient of for nominal scales. 1981). Consider the following situation. The contingency table directly mirrors our description. Agreement tables lose information. The agreement table does not change, but the contingency table does. In fact, Carletta et al. Step 1. However, r. 1981; Berry 1992; Goldman 1992). κS&C. .
 (04) found very little positive impact with this approach. Along similar lines, Alshawi et al. Recently Aue et al. This allows us to model important phenomena, such as not ... ne...pas. regardless of the number of intervening words. For the target language we only require word segmentation. (04). Our algorithm produces the dependency tree in Figure 2b. A second reattachment pass corrects this situation. We attempt to improve on these approaches by incorporating syntactic information. However, we found that this had litt of its modifiers la and Cancel are -1 and +1, respectively. We will initially approach the decoding problem as a bottom up, exhaustive search. Find all descendents s' of s that are not covered by x, but whose parent s&quot; is covered by x. A simpler alternate approach would be to compare bags-ofwords. The following optimizations do not preserve optimality, but work well in practice. The impact of this optimization is explored in Table 5.6. The complexity of the ordering step at each node grows with the factorial of the number of children to be ordered. Table 4.1 presents details about this dataset. Speed numbers are the end-to-end translation speed in sentences per minute. Results for our system and the comparison systems are presented in Table 5.1. Pharaoh monotone refers to Pharaoh with phrase reordering disabled. Constituents vs. Currently we only consider the top parse of an input sentence. .
 We c ? 2008. Some rights reserved. 2.1 Prepositions. 2.2 Determiners. They report 80% precision and 30% recall. Finally, Gamon etal. determiners 4.1 Feature set. . . ? phrase. For each. 5.1 Prepositions. American English, BNC vs. 4 Developed by James Curran. The reason may therefore be found else where, e.g. in the lexical properties of the contexts. book of recipes, book for recipes). condition for development condition of. travel to speed travel at. look at the USA look to. The example with travel, on the other hand, yields an ungrammatical result. also the ex ample of look at/to). 172 %of training data Prec. Our model outperforms their n-gram languagemodel approach by over 5%. 6.1 Working with L2 text. We envisage our model to. 6.2 Prepositions. It wasdeveloped jointly by Cambridge ESOL and Cambridge Uni versity Press. as correct or incorrect. The results from this taskare presented in Table 10. The corpus annotators, however, indicate on as the correct choice. 6.3 Determiners. On the incorrect instances (set size ca. 1200), however, accuracy is less than 10%. a for the or vice versa, doing so in overtwo-thirds of cases. .
 he or she) must refer to a person. For instance, U.S. average-link clustering). 4.1. 4.2. Output: A list of committees. Figure 1. Phase II of CBC. 4.3. key. 2. 3. a b e c d e a c d b e b a c d e a b c d e A) B) C) D) E) Figure 2. 6.1. Test Data. 1Available at www.cs.ualberta.ca/~lindek/minipar.htm. 6.2. Cluster Evaluation. 6.3. Manual Inspection. A description of the test sets in our experiments. We sorted them according to their precision. Table 3. .
 Walker et al. [forthcoming] and Boguraev and Briscoe [1988]). Hobbs et al. 1988; Charniak and Goldman 1988). First, let us turn to the issue of methodology. John forgot that he locked the door. b. John forgot to lock the door. Stump 1981), as shown in 8c and 8d. Partee [1985] for discussion). a. a fast typist: one who types quickly b. a fast car: one which can move quickly c. Dowty 1991). (cf. Levin 1989; Dowty 1991). a. Mary cut the bread. b. Mary cut at the bread. a. Mary broke the bread. b. *Mary broke at the bread. Jackendoff 1983). Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). a. The door is closed. b. The door closed. c. John closed the door. These problems are not, however, endemic to all decomposition approaches. Grimshaw 1990 and Pustejovsky 1991). (1987). Pustejovsky 1991; Moens and Steedman 1988). a. John baked the potato. b. John baked the cake. Gazdar et al. We return to this topic below. Dowty 1989 and Chierchia 1989). See footnote 20 for explanation. is a natural kind, while the latter is an artifact. Hinrichs 1985; Moens and Steedman 1987; and Krifka 1987). keep a secret, read a book, and play a record). Ia. Romeo gave the lecture. b.Hamlet mailed a letter. For discussion see Pustejovsky (in press). Dowty 1979). Pustejovsky [forthcoming] for discussion). Grimshaw 1979; Elliott 1974). Ingria and Pustejovsky 1990). To illustrate this, consider 42(c). Klein and Sag [1985] for discussion). in fact have values matching the correct type. a. John began to read a novel. b. John began to write a novel. (1990). The former takes both NP and a gerundive VP, while the latter takes only an NP (cf. for example, Freed [1979] for discussion). Ia. John finished the book. b. John finished writing the book. 2a. John completed the book. b. *John completed writing the book. a. *Mary began a rock. b. ? ?John finished the flower. That is, it has as its type, ([N Telic], N). Pustejovsky and Boguraev [1991] for discussion). Lakoff [1987] and Pustejovsky and Anick [19881): a. John crawled through the window. b. The window is closed. Copestake and Briscoe [1991] for details). hyponyms and hypernyms). (1985) and Evans and Gazdar (1989, 1990). a. The prisoner escaped last night. b. The prisoner ate dinner last night. Touretzky 1986). Pustejovsky 1991). Barselou 1983). drop the conjunct) to the concept of being confined. (1990) as well as Pustejovsky and Briscoe (1991). .
 Past approaches have resorted to heuristics. This disconnect between parses and derivations complicates both inference and learning. This approach is unsatisfying in some ways, however. We expect there to be fewer large subtrees than small ones. (2009), which we follow closely here. All DPs share parameters p$ and α. We emphasize that no head information is used by the sampler. The Gibbs sampler is an iterative procedure. The outcome probabilities are: where t = sub(c) o sub(c). We compare with three other grammars. the significantly larger “minimal subset” grammar. The sampled grammars outperform all of them. Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020. .
 Here, we improve on that model in several ways. First, we construct a generative model which utilizes the same features. Note that both approaches are local. However, parameter search methods have a potential advantage. Figure 2(a) and (b) are tree-equivalent. A bracketing is binary if it corresponds to a binary tree. Figure 2(b) is binary. We will induce trees by inducing tree-equivalent bracketings. Similarly for P(xij |Bij) and contexts. The rest of the paper, and results, would be unchanged except for notation to track the renormalization constant. This restriction is the basis for our algorithm. We now essentially have our induction algorithm. Relative frequency estimates (which are the ML estimates for this model) are used to set O'. To begin the process, we did not begin at the Estep with an initial guess at O. Rather, we began at the M-step, using an initial distribution over completions. This distribution was not used in the model itself, however. It seemed to bias too strongly against balanced structures, and led to entirely linear-branching structures. The smoothing used was straightforward. LBRANCH and RBRANCH choose the left- and right-branching structures, respectively. CCM is our system, as described above. Figure 5 shows that this is not true. This overproposal drops span-2 precision. However, it seemed to slightly hurt parsing accuracy overall. Figure 9 shows several of the 12 classes. Class 0 is the model’s distituent class. Class 5 is mainly composed of verb phrases and verb groups. No class corresponded neatly to PPs: perhaps because they have no signature contexts. This criticism could be two-fold. preposition (IN) or predeterminer (PDT) vs. These vectors were length-normalized, and then rank-reduced by an SVD, keeping the 50 largest singular vectors. These graphs stop at 40 iterations. .
 , fj, ... , ei, ... , M. , M. , S}. In the following, we give a short description of this baseline model. Finally, the sequence of phrases ˜eK1 constitutes the sequence of words eI1. Phrase Alignment This feature favors monotonic alignment at the phrase level. Word/Phrase Penalty This word penalty feature counts the length in words of the target sentence. Without this feature, the sentences produced tend to be too short. n-best list. The same issues affect the parser. This captures a sort of topic or semantic coherence in translations. As defined by Brown et al. By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. Relative positions are indicated for each Chinese tag. The model is a conditional probability p(f|T(e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. In addition, we processed short sentences only. We began with the tree-to-tree alignment model presented by Gildea (2003). Note that these aligned subtree pairs have properties similar to alignment templates. They can rearrange in complex ways between source and target. In this section, we consider another method for carving up the full parse tree. The unigram model gets a %BLEU score of 31.7 and the conditional model gets a %BLEU score of 31.9. ture added to the baseline features on its own, and a combination of new features. Potential reasons for this might be: tive to the grammaticality of MT output. Possibly, a comparable amount of annotated data (e.g. 0121285. .
 Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent. In this way, only true errors in the MT output are counted. These correct translations differ not only in their word choice but also in the order in which the words occur. within the hypothesis. For exact details on these constraints, see Snover et al. (2006). Instead, it scores the hypothesis against each reference individually. In practice this term will dominate the phrase substitution edit cost. These correlations are measured at the sentence, or segment, level. When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used. TER was also used in case insensitive mode. The differences between Adequacy and Fluency are smaller, but there are still significant differences. This line of research holds promise as an external evaluation method of various paraphrasing methods. This work was supported, in part, by BBN Technologies under the GALE Program, DARPA/IPTO Contract No. HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/. .
 From a software engineering perspective. The paper is structured as follows. Whenever it finds one, it executes the corresponding C code. with noun tags starting with N. etc. The symbols + and _ are delimiters. Double quotes indicate literal character symbols. the ,corisoriant is doubled (Procter, 1995). The generator executable occupies around 700Kb on disc. In addition. This threw up a small number of errors which we have now fixed. We did not count these cases as mistakes. but does not in the BNC. We did not count these cases as mistakes either. We categorised these as irrelevant for practical applications and so discarded them. Some orthographic phenomena span more than one word. We have therefore implemented a final orthographic postprocessing stage. unidimensional) found in the BNC. Karttunen et al. (1996)). Furthermore, although a number of finite-state compilation toolkits (e.g. However, ..we,..adopt. less • of ar, theoreti-7. Both of these systems interleave orthographic processing with other processes in realisation. In addition. KPML appears not to perform this type of processing at all. We have described a generator-for -English flectional morphology. See <http://www.cogs. susx.ac.uk/lab/nip/carroll/morph.html>. Thanks also to the anonymous reviewers for insightful comments. .
 But semantic information is difficult to obtain. Next, we describe experimental results for five categories. 4. 5. We then go back to Step 1 and repeat the process. In our experiments, we found that about eight iterations usually worked well. In the next section, we describe experiments to evaluate our system. However, we have not observed this to be a problem so far. For example, our number processor failed to remove numbers with commas (e.g., 2,000). 'The judges were members of our research group but not the authors. aLimon-Covenas refers to an oil pipeline. aLa_Aurora refers to an airport. For example, feathers and tails are parts of ANIMALS. 1: NO ASSOCIATION WITH THE CATEGORY: If a word has virtually no association with the category, then it deserves a 1. For example, tables and moons have virtually no association with ANIMALS. 0: UNKNOWN WORD: If you do not know what a word means, then it should be labeled with a 0. IMPORTANT! Many words have several distinct meanings. Finally, we graphed the results from the human judges. We counted the number of words judged as 5's by either judge, the number of words judged as 5's or 4's by either judge, the number of words judged as 5's, 4's, or 3's by either judge, and the number of words judged as either 5's, 4's, 3's, or 2's. Figure 9 illustrates an important benefit of the corpus-based approach. Fortunately, most of them worked fairly well, but some of them did not. The Person category produced mixed results. This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee. .
 Section 4 presents an evaluation of the proposed measures. Section 6 summarizes our contributions. 1983). 1994). While VNICs vary in their degree of flexibility (cf. Thus, . KL-divergence is argued to be problematic because it is not a symmetric measure. Otherwise, the pair is considered literal. (2) and in Eqn. (4). All frequency counts are over the entire BNC. , as well as that of the two baselines, and ; see Table 2. performs as well as the informed baseline ( error reduction). Nonetheless, the performance signifies a need for improvement. As expected, the performance of drops substantially for low frequency items. For example, the lexical representation of shoot, breeze should include shoot the breeze as a Cform. The average precision across the 100 test pairs is 81.7%, and the average recall is 88.0% (with 69 of the pairs having 100% precision and 100% recall). Thus, our method of detecting Cforms performs quite well. .
