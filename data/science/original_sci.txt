TnT - A Statistical Part-Of-Speech Tagger Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora. A large number of current language processing systems use a part-of-speech tagger for pre-processing. The tagger assigns a (unique or ambiguous) part-ofspeech tag to each token in the input and passes its output to the next processing level, usually a parser. Furthermore, there is a large interest in part-ofspeech tagging for corpus annotation projects, who create valuable linguistic resources by a combination of automatic processing and human correction. For both applications, a tagger with the highest possible accuracy is required. The debate about which paradigm solves the part-of-speech tagging problem best is not finished. Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996). They are only surpassed by combinations of different systems, forming a &quot;voting tagger&quot;. Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words. This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging. The tagger comparison was organized as a &quot;blackbox test&quot;: set the same task to every tagger and compare the outcomes. This paper describes the models and techniques used by TnT together with the implementation. The reader will be surprised how simple the underlying model is. The result of the tagger comparison seems to support the maxime &quot;the simplest is the best&quot;. However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application. We argue that it is not only the choice of the general model that determines the result of the tagger but also the various &quot;small&quot; decisions on alternatives. The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999). TnT uses second order Markov models for part-ofspeech tagging. The states of the model represent tags, outputs represent the words. Transition probabilities depend on the states, thus pairs of tags. Output probabilities only depend on the most recent category. To be explicit, we calculate for a given sequence of words w1 of length T. t1 tr are elements of the tagset, the additional tags t_1, to, and t7-,±1 are beginning-of-sequence and end-of-sequence markers. Using these additional tags, even if they stem from rudimentary processing of punctuation marks, slightly improves tagging results. This is different from formulas presented in other publications, which just stop with a &quot;loose end&quot; at the last word. If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of [.!? ;] as a token. Transition and output probabilities are estimated from a tagged corpus. As a first step, we use the maximum likelihood probabilities P which are derived from the relative frequencies: for all t1, t2, t3 in the tagset and w3 in the lexicon. N is the total number of tokens in the training corpus. We define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero. As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below). Trigram probabilities generated from a corpus usually cannot directly be used because of the sparsedata problem. This means that there are not enough instances for each trigram to reliably estimate the probability. Furthermore, setting a probability to zero because the corresponding trigram never occured in the corpus has an undesired effect. It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability. The smoothing paradigm that delivers the best results in TnT is linear interpolation of unigrams, bigrams, and trigrams. Therefore, we estimate a trigram probability as follows: P are maximum likelihood estimates of the probabilities, and A1 + A2 ± A3 = 1, SO P again represent probability distributions. We use the context-independent variant of linear interpolation, i.e., the values of the As do not depend on the particular trigram. Contrary to intuition, this yields better results than the context-dependent variant. Due to sparse-data problems, one cannot estimate a different set of As for each trigram. Therefore, it is common practice to group trigrams by frequency and estimate tied sets of As. However, we are not aware of any publication that has investigated frequency groupings for linear interpolation in part-of-speech tagging. All groupings that we have tested yielded at most equivalent results to contextindependent linear interpolation. Some groupings even yielded worse results. The tested groupings included a) one set of As for each frequency value and b) two classes (low and high frequency) on the two ends of the scale, as well as several groupings in between and several settings for partitioning the classes. The values of A1, A2, and A3 are estimated by deleted interpolation. This technique successively removes each trigram from the training corpus and estimates best values for the As from all other ngrams in the corpus. Given the frequency counts for uni-, bi-, and trigrams, the weights can be very efficiently determined with a processing time linear in the number of different trigrams. The algorithm is given in figure 1. Note that subtracting 1 means taking unseen data into account. Without this subtraction the model would overfit the training data and would generally yield worse results. Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993). Tag probabilities are set according to the word's ending. The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (.11) in 98% of the cases (e.g. fashionable, variable) , the rest of 2% are nouns (e.g. cable, variable). The probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum length. The term suffix as used here means &quot;final sequence of characters of a word&quot; which is not necessarily a linguistically meaningful suffix. Probabilities are smoothed by successive abstraction. This calculates the probability of a tag t given the last m letters i of an n letter word: P(t1/7„,+1,,..ln). The sequence of increasingly more general contexts omits more and more characters of the suffix, such that P(tlin-m+2, • • • P(tlin_m+3, ,i), , P(t) are used for smoothing. The recursion formula is set A = A2 = A3 = 0 foreach trigram t1,t2,t3 with f (ti,t2,t3) >0 depending on the maximum of the following three values: for i = m ... 0, using the maximum likelihood estimates P from frequencies in the lexicon, weights Oi and the initialization For the Markov model, we need the inverse conditional probabilities P(1,2_1+1, ... /Tilt) which are obtained by Bayesian inversion. A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0, (Samuelsson, 1993). This leaves room for interpretation. We use the longest suffix that we can find in the training set (i.e., for which the frequency is greater than or equal to 1), but at most 10 characters. This is an empirically determined choice. 2) We use a context-independent approach for 0„ as we did for the contextual weights A. It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10. 3) We use different estimates for uppercase and lowercase words, i.e., we maintain two different suffix tries depending on the capitalization of the word. This information improves the tagging results. 4) Another freedom concerns the choice of the words in the lexicon that should be used for suffix handling. Should we use all words, or are some of them better suited than others? Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words. Therefore, we restrict the procedure of suffix handling to words with a frequency smaller than or equal to some threshold value. Empirically, 10 turned out to be a good choice for this threshold. Additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information. Tags are usually not informative about capitalization, but probability distributions of tags around capitalized words are different from those not capitalized. The effect is larger for English, which only capitalizes proper names, and smaller for German, which capitalizes all nouns. We use flags ci that are true if wi is a capitalized word and false otherwise. These flags are added to the contextual probability distributions. Instead of and equations (3) to (5) are updated accordingly. This is equivalent to doubling the size of the tagset and using different tags depending on capitalization. The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam search. Each state that receives a 6 value smaller than the largest 6 divided by some threshold value 0 is excluded from further processing. While the Viterbi algorithm is guaranteed to find the sequence of states with the highest probability, this is no longer true when beam search is added. Nevertheless, for practical purposes and the right choice of 0, there is virtually no difference between the algorithm with and without a beam. Empirically, a value of 0 = 1000 turned out to approximately double the speed of the tagger without affecting the accuracy. The tagger currently tags between 30,000 and 60,000 tokens per second (including file I/O) on a Pentium 500 running Linux. The speed mainly depends on the percentage of unknown words and on the average ambiguity rate. We evaluate the tagger's performance under several aspects. First of all, we determine the tagging accuracy averaged over ten iterations. The overall accuracy, as well as separate accuracies for known and unknown words are measured. Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set). An important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments. We distinguish reliable from unreliable assignments by the quotient of the best and second best assignmentsl . All assignments for which this quotient is larger than some threshold are regarded as reliable, the others as unreliable. As we will see below, accuracies for reliable assignments are much higher. The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training. Each result is obtained by repeating the experiment 10 times with different partitions and averaging the single outcomes. In all experiments, contiguous test sets are used. The alternative is a round-robin procedure that puts every 10th sentence into the test set. We argue that contiguous test sets yield more realistic results because completely unseen articles are tagged. Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words. Therefore, we expect even 'By definition, this quotient is oo if there is only one possible tag for a given word. higher results when testing on every 10th sentence instead of a contiguous set of 10%. In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed. The tagger is allowed to assign exactly one tag to each token. We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens. The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon. The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997). It was developed at the Saarland University in Saarbriicken2. Part of it was tagged at the IMS Stuttgart. This evaluation only uses the partof-speech annotation and ignores structural annotations. Tagging accuracies for the NEGRA corpus are shown in table 2. Figure 3 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times, training and test sets were randomly chosen and disjoint, results were averaged. The training length is given on a logarithmic scale. It is remarkable that tagging accuracy for known words is very high even for very small training corpora. This means that we have a good chance of getting the right tag if a word is seen at least once during training. Average percentages of unknown tokens are shown in the bottom line of each diagram. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. If there is an alternative that has a probability &quot;close to&quot; that of the best assignment, this alternative can be viewed as almost equally well suited. The notion of &quot;close to&quot; is expressed by the distance of probabilities, and this in turn is expressed by the quotient of probabilities. So, the distance of the probabilities of a best tag tbest and an alternative tag tau is expressed by p(tbest)/P(talt)7 which is some value greater or equal to 1 since the best tag assignment has the highest probability. Figure 4 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments). As expected, we find that accuracies for percentage known unknown • overall unknowns acc. acc. acc. a Table 5: Part-of-speech tagging accuracy for the Penn Treebank. The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy. percentage known unknown overall unknowns acc. acc. acc. reliable assignments are much higher than for unreliable assignments. This distinction is, e.g., useful for annotation projects during the cleaning process, or during pre-processing, so the tagger can emit multiple tags if the best tag is classified as unreliable. We use the Wall Street Journal as contained in the Penn Treebank for our experiments. The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993). This evaluation only uses the part-ofspeech annotation. The Wall Street Journal part of the Penn Treebank consists of approx. 50,000 sentences (1.2 million tokens). Tagging accuracies for the Penn Treebank are shown in table 5. Figure 6 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times. Training and test sets were disjoint, results are averaged. The training length is given on a logarithmic scale. As for the NEGRA corpus, tagging accuracy is very high for known tokens even with small amounts of training data. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. Figure 7 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments). Again, we find that accuracies for reliable assignments are much higher than for unreliable assignments. Average part-of-speech tagging accuracy is between 96% and 97%, depending on language and tagset, which is at least on a par with state-of-the-art results found in the literature, possibly better. For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of results while Ratnaparkhi only makes one test run. The accuracy for known tokens is significantly higher than for unknown tokens. For the German newspaper data, results are 8.7% better when the word was seen before and therefore is in the lexicon, than when it was not seen before (97.7% vs. 89.0%). Accuracy for known tokens is high even with very small amounts of training data. As few as 1000 tokens are sufficient to achieve 95%-96% accuracy for them. It is important for the tagger to have seen a word at least once during training. Stochastic taggers assign probabilities to tags. We exploit the probabilities to determine reliability of assignments. For a subset that is determined during processing by the tagger we achieve accuracy rates of over 99%. The accuracy of the complement set is much lower. This information can, e.g., be exploited in an annotation project to give an additional treatment to the unreliable assignments, or to pass selected ambiguities to a subsequent processing step. We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature. For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor. The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words. Note that the decisions we made yield good results for both the German and the English Corpus. They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both. TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt). Many thanks go to Hans Uszkoreit for his support during the development of TnT. Most of the work on TnT was carried out while the author received a grant of the Deutsche Forschungsgemeinschaft in the Graduiertenkolleg Kognitionswissenschaft Saarbriicken. Large annotated corpora are the pre-requisite for developing and testing part-ofspeech taggers, and they enable the generation of high-quality language models. Therefore, I would like to thank all the people who took the effort to annotate the Penn Treebank, the Susanne Corpus, the Stuttgarter Referenzkorpus, the NEGRA Corpus, the Verbmobil Corpora, and several others. And, last but not least, I would like to thank the users of TnT who provided me with bug reports and valuable suggestions for improvements.
Mildly Non-Projective Dependency Structures Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency. In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree. While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity. In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints. The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data. Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech (Collins et al., 1999), Bulgarian (Marinov and Nivre, 2005), and Turkish (Eryi˘git and Oflazer, 2006). Many practical implementations of dependency parsing are restricted to projective structures, where the projection of a head word has to form a continuous substring of the sentence. While this constraint guarantees good parsing complexity, it is well-known that certain syntactic constructions can only be adequately represented by non-projective dependency structures, where the projection of a head can be discontinuous. This is especially relevant for languages with free or flexible word order. However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006). This raises the question of whether it is possible to characterize a class of mildly non-projective dependency structures that is rich enough to account for naturally occurring syntactic constructions, yet restricted enough to enable efficient parsing. In this paper, we review a number of proposals for classes of dependency structures that lie between strictly projective and completely unrestricted non-projective structures. These classes have in common that they can be characterized in terms of properties of the dependency structures themselves, rather than in terms of grammar formalisms that generate the structures. We compare the proposals from a theoretical point of view, and evaluate a subset of them empirically by testing their representational adequacy with respect to two dependency treebanks: the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001), and the Danish Dependency Treebank (DDT) (Kromann, 2003). The rest of the paper is structured as follows. In section 2, we provide a formal definition of dependency structures as a special kind of directed graphs, and characterize the notion of projectivity. In section 3, we define and compare five different constraints on mildly non-projective dependency structures that can be found in the literature: planarity, multiplanarity, well-nestedness, gap degree, and edge degree. In section 4, we provide an experimental evaluation of the notions of planarity, well-nestedness, gap degree, and edge degree, by investigating how large a proportion of the dependency structures found in PDT and DDT are allowed under the different constraints. In section 5, we present our conclusions and suggestions for further research. For the purposes of this paper, a dependency graph is a directed graph on the set of indices corresponding to the tokens of a sentence. We write [n] to refer to the set of positive integers up to and including n. Throughout this paper, we use standard terminology and notation from graph theory to talk about dependency graphs. In particular, we refer to the elements of the set V as nodes, and to the elements of the set E as edges. We write i --> j to mean that there is an edge from the node i to the node j (i.e., (i, j) E E), and i -->* j to mean that the node i dominates the node j, i.e., that there is a (possibly empty) path from i to j. For a given node i, the set of nodes dominated by i is the yield of i. We use the notation 3r(i) to refer to the projection of i: the yield of i, arranged in ascending order. Most of the literature on dependency grammar and dependency parsing does not allow arbitrary dependency graphs, but imposes certain structural constraints on them. In this paper, we restrict ourselves to dependency graphs that form forests. Definition 2 A dependency forest is a dependency graph with two additional properties: Figure 1 shows a dependency forest taken from PDT. It has two roots: node 2 (corresponding to the complementizer proto) and node 8 (corresponding to the final punctuation mark). Some authors extend dependency forests by a special root node with position 0, and add an edge (0, i) for every root node i of the remaining graph (McDonald et al., 2005). This ensures that the extended graph always is a tree. Although such a definition can be useful, we do not follow it here, since it obscures the distinction between projectivity and planarity to be discussed in section 3. In contrast to acyclicity and the indegree constraint, both of which impose restrictions on the dependency relation as such, the projectivity constraint concerns the interaction between the dependency relation and the positions of the nodes in the sentence: it says that the nodes in a subtree of a dependency graph must form an interval, where an interval (with endpoints i and j) is the set [i, j] := {kEV I i < k and k < j }. Definition 3 A dependency graph is projective, if the yields of its nodes are intervals. Since projectivity requires each node to dominate a continuous substring of the sentence, it corresponds to a ban on discontinuous constituents in phrase structure representations. Projectivity is an interesting constraint on dependency structures both from a theoretical and a practical perspective. Dependency grammars that only allow projective structures are closely related to context-free grammars (Gaifman, 1965; Obre¸bski and Grali´nski, 2004); among other things, they have the same (weak) expressivity. The projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time (Eisner, 1996); hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre, 2003). While the restriction to projective analyses has a number of advantages, there is clear evidence that it cannot be maintained for real-world data (Zeman, 2004; Nivre, 2006). For example, the graph in Figure 1 is non-projective: the yield of the node 1 (marked by the dashed rectangles) does not form an interval—the node 2 is ‘missing’. In this section, we present several proposals for structural constraints that relax projectivity, and relate them to each other. The notion of planarity appears in work on Link Grammar (Sleator and Temperley, 1993), where it is traced back to Mel’ˇcuk (1988). Informally, a dependency graph is planar, if its edges can be drawn above the sentence without crossing. We emphasize the word above, because planarity as it is understood here does not coincide with the standard graph-theoretic concept of the same name, where one would be allowed to also use the area below the sentence to disentangle the edges. Figure 2a shows a dependency graph that is planar but not projective: while there are no crossing edges, the yield of the node 1 (the set 11, 3}) does not form an interval. Using the notation linked(i, j) as an abbreviation for the statement ‘there is an edge from i to j, or vice versa’, we formalize planarity as follows: Definition 4 A dependency graph is planar, if it does not contain nodes a, b, c, d such that linked(a, c) A linked(b, d) A a < b < c < d . Yli-Jyrä (2003) proposes multiplanarity as a generalization of planarity suitable for modelling dependency analyses, and evaluates it experimentally using data from DDT. Definition 5 A dependency graph G = (V ; E) is m-planar, if it can be split into m planar graphs such that E = E1U- - -UEm. The planar graphs Gi are called planes. As an example of a dependency forest that is 2planar but not planar, consider the graph depicted in Figure 2b. In this graph, the edges (1, 4) and (3, 5) are crossing. Moving either edge to a separate graph partitions the original graph into two planes. Bodirsky et al. (2005) present two structural constraints on dependency graphs that characterize analyses corresponding to derivations in Tree Adjoining Grammar: the gap degree restriction and the well-nestedness constraint. A gap is a discontinuity in the projection of a node in a dependency graph (Plátek et al., 2001). More precisely, let 7ri be the projection of the node i. Then a gap is a pair (jk, jk+1) of nodes adjacent in 7ri such that Definition 6 The gap degree of a node i in a dependency graph, gd(i), is the number of gaps in 7ri. As an example, consider the node labelled i in the dependency graphs in Figure 3. In Graph 3a, the projection of i is an interval ((2, 3, 4)), so i has gap degree 0. In Graph 3b, 7ri = (2, 3, 6) contains a single gap ((3, 6)), so the gap degree of i is 1. In the rightmost graph, the gap degree of i is 2, since 7ri = (2, 4, 6) contains two gaps ((2, 4) and (4, 6)). Definition 7 The gap degree of a dependency graph G, gd(G), is the maximum among the gap degrees of its nodes. Thus, the gap degree of the graphs in Figure 3 is 0, 1 and 2, respectively, since the node i has the maximum gap degree in all three cases. The well-nestedness constraint restricts the positioning of disjoint subtrees in a dependency forest. Two subtrees are called disjoint, if neither of their roots dominates the other. Definition 8 Two subtrees T1, T2 interleave, if there are nodes l1, r1 E T1 and l2, r2 E T2 such that l1 < l2 < r1 < r2. A dependency graph is well-nested, if no two of its disjoint subtrees interleave. Both Graph 3a and Graph 3b are well-nested. Graph 3c is not well-nested. To see this, let T1 be the subtree rooted at the node labelled i, and let T2 be the subtree rooted at j. These subtrees interleave, as T1 contains the nodes 2 and 4, and T2 contains the nodes 3 and 5. The notion of edge degree was introduced by Nivre (2006) in order to allow mildly non-projective structures while maintaining good parsing efficiency in data-driven dependency parsing.2 Define the span of an edge (i, j) as the interval S((i, j)) W= [min(i, j),max(i, j)]. Definition 9 Let G = (V I E) be a dependency forest, let e = (i, j) be an edge in E, and let Ge be the subgraph of G that is induced by the nodes contained in the span of e. • The degree of an edge e 2 E, ed(e), is the number of connected components c in Ge such that the root of c is not dominated by the head of e. • The edge degree of G, ed(G), is the maximum among the degrees of the edges in G. To illustrate the notion of edge degree, we return to Figure 3. Graph 3a has edge degree 0: the only edge that spans more nodes than its head and its dependent is (1, 5), but the root of the connected component f2, 3, 4g is dominated by 1. Both Graph 3b and 3c have edge degree 1: the edge (3, 6) in Graph 3b and the edges (2, 4), (3, 5) and (4, 6) in Graph 3c each span a single connected component that is not dominated by the respective head. Apart from proposals for structural constraints relaxing projectivity, there are dependency frameworks that in principle allow unrestricted graphs, but provide mechanisms to control the actually permitted forms of non-projectivity in the grammar. The non-projective dependency grammar of Kahane et al. (1998) is based on an operation on dependency trees called lifting: a ‘lift’ of a tree T is the new tree that is obtained when one replaces one 2We use the term edge degree instead of the original simple term degree from Nivre (2006) to mark the distinction from the notion of gap degree. or more edges (i, k) in T by edges (j, k), where j ! * i. The exact conditions under which a certain lifting may take place are specified in the rules of the grammar. A dependency tree is acceptable, if it can be lifted to form a projective graph.3 A similar design is pursued in Topological Dependency Grammar (Duchier and Debusmann, 2001), where a dependency analysis consists of two, mutually constraining graphs: the ID graph represents information about immediate dominance, the LP graph models the topological structure of a sentence. As a principle of the grammar, the LP graph is required to be a lift of the ID graph; this lifting can be constrained in the lexicon. The structural conditions we have presented here naturally fall into two groups: multiplanarity, gap degree and edge degree are parametric constraints with an infinite scale of possible values; planarity and well-nestedness come as binary constraints. We discuss these two groups in turn. Parametric constraints With respect to the graded constraints, we find that multiplanarity is different from both gap degree and edge degree in that it involves a notion of optimization: since every dependency graph is m-planar for some sufficiently large m (put each edge onto a separate plane), the interesting question in the context of multiplanarity is about the minimal values for m that occur in real-world data. But then, one not only needs to show that a dependency graph can be decomposed into m planar graphs, but also that this decomposition is the one with the smallest number of planes among all possible decompositions. Up to now, no tractable algorithm to find the minimal decomposition has been given, so itis not clear how to evaluate the significance of the concept as such. The evaluation presented by Yli-Jyrä (2003) makes use of additional constraints that are sufficient to make the decomposition unique. The fundamental difference between gap degree and edge degree is that the gap degree measures the number of discontinuities within a subtree, while the edge degree measures the number of intervening constituents spanned by a single edge. This difference is illustrated by the graphs displayed in Figure 4. Graph 4a has gap degree 2 but edge degree 1: the subtree rooted at node 2 (marked by the solid edges) has two gaps, but each of its edges only spans one connected component not dominated by 2 (marked by the squares). In contrast, Graph 4b has gap degree 1 but edge degree 2: the subtree rooted at node 2 has one gap, but this gap contains two components not dominated by 2. Nivre (2006) shows experimentally that limiting the permissible edge degree to 1 or 2 can reduce the average parsing time for a deterministic algorithm from quadratic to linear, while omitting less than 1% of the structures found in DDT and PDT. It can be expected that constraints on the gap degree would have very similar effects. Binary constraints For the two binary constraints, we find that well-nestedness subsumes planarity: a graph that contains interleaving subtrees cannot be drawn without crossing edges, so every planar graph must also be well-nested. To see that the converse does not hold, consider Graph 3b, which is well-nested, but not planar. Since both planarity and well-nestedness are proper extensions of projectivity, we get the following hierarchy for sets of dependency graphs: projective C planar C well-nested C unrestricted The planarity constraint appears like a very natural one at first sight, as it expresses the intuition that ‘crossing edges are bad’, but still allows a limited form of non-projectivity. However, many authors use planarity in conjunction with a special representation of the root node: either as an artificial node at the sentence boundary, as we mentioned in section 2, or as the target of an infinitely long perpendicular edge coming ‘from the outside’, as in earlier versions of Word Grammar (Hudson, 2003). In these situations, planarity reduces to projectivity, so nothing is gained. Even in cases where planarity is used without a special representation of the root node, it remains a peculiar concept. When we compare it with the notion of gaps, for example, we find that, in a planar dependency tree, every gap .i; j/ must contain the root node r, in the sense that i < r < j: if the gap would only contain non-root nodes k, then the two paths from r to k and from i to j would cross. This particular property does not seem to be mirrored in any linguistic prediction. In contrast to planarity, well-nestedness is independent from both gap degree and edge degree in the sense that for every d > 0, there are both wellnested and non-well-nested dependency graphs with gap degree or edge degree d. All projective dependency graphs (d = 0) are trivially well-nested. Well-nestedness also brings computational benefits. In particular, chart-based parsers for grammar formalisms in which derivations obey the well-nestedness constraint (such as Tree Adjoining Grammar) are not hampered by the ‘crossing configurations’ to which Satta (1992) attributes the fact that the universal recognition problem of Linear Context-Free Rewriting Systems is X30-complete. In this section, we present an experimental evaluation of planarity, well-nestedness, gap degree, and edge degree, by examining how large a proportion of the structures found in two dependency treebanks are allowed under different constraints. Assuming that the treebank structures are sampled from naturally occurring structures in natural language, this provides an indirect evaluation of the linguistic adequacy of the different proposals. The experiments are based on data from the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001) and the Danish Dependency Treebank (DDT) (Kromann, 2003). PDT contains 1.5M words of newspaper text, annotated in three layers according to the theoretical framework of Functional Generative Description (Böhmová et al., 2003). Our experiments concern only the analytical layer, and are based on the dedicated training section of the treebank. DDT comprises 100k words of text selected from the Danish PAROLE corpus, with annotation property all structures gap degree 0 gap degree 1 gap degree 2 gap degree 3 gap degree 4 edge degree 0 edge degree 1 edge degree 2 edge degree 3 edge degree 4 edge degree 5 edge degree 6 projective planar well-nested of primary and secondary dependencies based on Discontinuous Grammar (Kromann, 2003). Only primary dependencies are considered in the experiments, which are based on the entire treebank.4 The results of our experiments are given in Table 1. For the binary constraints (planarity, well-nestedness), we simply report the number and percentage of structures in each data set that satisfy the constraint. For the parametric constraints (gap degree, edge degree), we report the number and percentage of structures having degree d (d > 0), where degree 0 is equivalent (for both gap degree and edge degree) to projectivity. For DDT, we see that about 15% of all analyses are non-projective. The minimal degree of non-projectivity required to cover all of the data is 2 in the case of gap degree and 4 in the case of edge degree. For both measures, the number of structures drops quickly as the degree increases. (As an example, only 7 or 0.17% of the analyses in DDT have gap 4A total number of 17 analyses in DDT were excluded because they either had more than one root node, or violated the indegree constraint. (Both cases are annotation errors.) degree 2.) Regarding the binary constraints, we find that planarity accounts for slightly more than the projective structures (86.41% of the data is planar), while almost all structures in DDT (99.89%) meet the well-nestedness constraint. The difference between the two constraints becomes clearer when we base the figures on the set of non-projective structures only: out of these, less than 10% are planar, while more than 99% are well-nested. For PDT, both the number of non-projective structures (around 23%) and the minimal degrees of non-projectivity required to cover the full data (gap degree 4 and edge degree 6) are higher than in DDT. The proportion of planar analyses is smaller than in DDT if we base it on the set of all structures (82.16%), but significantly larger when based on the set of non-projective structures only (22.93%). However, this is still very far from the well-nestedness constraint, which has almost perfect coverage on both data sets. As a general result, our experiments confirm previous studies on non-projective dependency parsing (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006): The phenomenon of non-projectivity cannot be ignored without also ignoring a significant portion of real-world data (around 15% for DDT, and 23% for PDT). At the same time, already a small step beyond projectivity accounts for almost all of the structures occurring in these treebanks. More specifically, we find that already an edge degree restriction of d < 1 covers 98.24% of DDT and 99.54% of PDT, while the same restriction on the gap degree scale achieves a coverage of 99.84% (DDT) and 99.57% (PDT). Together with the previous evidence that both measures also have computational advantages, this provides a strong indication for the usefulness of these constraints in the context of non-projective dependency parsing. When we compare the two graded constraints to each other, we find that the gap degree measure partitions the data into less and larger clusters than the edge degree, which may be an advantage in the context of using the degree constraints as features in a data-driven approach towards parsing. However, our purely quantitative experiments cannot answer the question, which of the two measures yields the more informative clusters. The planarity constraint appears to be of little use as a generalization of projectivity: enforcing it excludes more than 75% of the non-projective data in PDT, and 90% of the data in DDT. The relatively large difference in coverage between the two treebanks may at least partially be explained with their different annotation schemes for sentence-final punctuation. In DDT, sentence-final punctuation marks are annotated as dependents of the main verb of a dependency nexus. This, as we have discussed above, places severe restrictions on permitted forms of non-projectivity in the remaining sentence, as every discontinuity that includes the main verb must also include the dependent punctuation marks. On the other hand, in PDT, a sentencefinal punctuation mark is annotated as a separate root node with no dependents. This scheme does not restrict the remaining discontinuities at all. In contrast to planarity, the well-nestedness constraint appears to constitute a very attractive extension of projectivity. For one thing, the almost perfect coverage of well-nestedness on DDT and PDT (99.89%) could by no means be expected on purely combinatorial grounds—only 7% of all possible dependency structures for sentences of length 17 (the average sentence length in PDT), and only slightly more than 5% of all possible dependency structures for sentences of length 18 (the average sentence length in DDT) are well-nested.5 Moreover, a cursory inspection of the few problematic cases in DDT indicates that violations of the wellnestedness constraint may, at least in part, be due to properties of the annotation scheme, such as the analysis of punctuation in quotations. However, a more detailed analysis of the data from both treebanks is needed before any stronger conclusions can be drawn concerning well-nestedness. In this paper, we have reviewed a number of proposals for the characterization of mildly non-projective dependency structures, motivated by the need to find a better balance between expressivity and complexity than that offered by either strictly projective or unrestricted non-projective structures. Experimental evaluation based on data from two treebanks shows, that a combination of the wellnestedness constraint and parametric constraints on discontinuity (formalized either as gap degree or edge degree) gives a very good fit with the empirical linguistic data. Important goals for future work are to widen the empirical basis by investigating more languages, and to perform a more detailed analysis of linguistic phenomena that violate certain constraints. Another important line of research is the integration of these constraints into parsing algorithms for non-projective dependency structures, potentially leading to a better trade-off between accuracy and efficiency than that obtained with existing methods. Acknowledgements We thank three anonymous reviewers of this paper for their comments. The work of Marco Kuhlmann is funded by the Collaborative Research Centre 378 ‘Resource-Adaptive Cognitive Processes’ of the Deutsche Forschungsgemeinschaft. The work of Joakim Nivre is partially supported by the Swedish Research Council.
Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. An impressive array of statistical methods have been developed for word sense identification. They range from dictionary-based approaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994). We have drawn on these two traditions, using corpus-based co-occurrence and the lexical knowledge base that is embodied in the WordNet lexicon. The two traditions complement each other. Corpus-based approaches have the advantage of being generally applicable to new texts, domains, and corpora without needing costly and perhaps error-prone parsing or semantic analysis. They require only training corpora in which the sense distinctions have been marked, but therein lies their weakness. Obtaining training materials for statistical methods is costly and timeconsuming—it is a &quot;knowledge acquisition bottleneck&quot; (Gale, Church, and Yarowsky 1992a). To open this bottleneck, we use WordNet's lexical relations to locate unsupervised training examples. Section 2 describes a statistical classifier, TLC (Topical/Local Classifier), that uses topical context (the open-class words that co-occur with a particular sense), local context (the open- and closed-class items that occur within a small window around a word), or a combination of the two. The results of combining the two types of context to disambiguate a noun (line), a verb (serve), and an adjective (hard) are presented. The following questions are discussed: When is topical context superior to local context (and vice versa)? Is their combination superior to either type alone? Do the answers to these questions depend on the size of the training? Do they depend on the syntactic category of the target? Manually tagged training materials were used in the development of TLC and the experiments in Section 2. The Cognitive Science Laboratory at Princeton University, with support from NSF-ARPA, is producing textual corpora that can be used in developing and evaluating automatic methods for disambiguation. Examples of the different meanings of one thousand common, polysemous, open-class English words are being manually tagged. The results of this effort will be a useful resource for training statistical classifiers, but what about the next thousand polysemous words, and the next? In order to identify senses of these words, it will be necessary to learn how to harvest training examples automatically. Section 3 describes WordNet's lexical relations and the role that monosemous &quot;relatives&quot; of polysemous words can play in creating unsupervised training materials. TLC is trained with automatically extracted examples, its performance is compared with that obtained from manually tagged training materials. Work on automatic sense identification from the 1950s onward has been well summarized by Hirst (1987) and Dagan and Itai (1994). The discussion below is limited to work that is closely related to our research. Hearst (1991) represents local context with a shallow syntactic parse in which the context is segmented into prepositional phrases, noun phrases, and verb groups. The target noun is coded for the word it modifies, the word that modifies it, and the prepositions that precede and follow it. Open-class items within ±3 phrase segments of the target are coded in terms of their relation to the target (modifier or head) or their role in a construct that is adjacent to the target. Evidence is combined in a manner similar to that used by the local classifier component of TLC. With supervised training of up to 70 sentences per sense, performance on three homographs was quite good (88-100% correct); with fewer training examples and semantically related senses, performance on two additional words was less satisfactory (73-77% correct). Gale, Church, and Yarowsky (1992a) developed a topical classifier based on Bayesian decision theory. The only information the classifier uses is an unordered list of words that co-occur with the target in training examples. No other cues, such as part-of-speech tags or word order, are used. Leacock, Towel!, and Voorhees (1993) compared this Bayesian classifier with a content vector classifier as used in information retrieval and a neural network with backpropagation. The classifiers were compared using different numbers of senses (two, three, or six manually tagged senses of line) and different amounts of training material (50, 100, and 200 examples). On the sixsense task, the classifiers averaged 74% correct answers. Leacock, Towel!, and Voorhees (1993) found that the response patterns of the three classifiers converged, suggesting that each of the classifiers was extracting as much data as is available in purely topical approaches that look only at word counts from training examples. If this is the case, any technique that uses only topical information will not be significantly more accurate than the three classifiers tested. Leacock, Towell, and Voorhees (1996) showed that performance of the content vector topical classifier could be improved with the addition of local templates— specific word patterns that were recognized as being indicative of a particular sense— in an extension of an idea initially suggested by Weiss (1973). Although the templates proved to be highly reliable when they occurred, all too often, none were found. Yarowsky (1993) also found that template-like structures are very powerful indicators of sense. He located collocations by looking at adjacent words or at the first word to the left or right in a given part of speech and found that, with binary ambiguity, a word has only one sense in a given collocation with a probability of 90-99%.1 However, he had an average of only 29% recall (i.e., the collocations were found in only 29% of the cases). When local information occurred it was highly reliable, but all too often, it did not occur. Bruce and Wiebe (1994a, 1994b) have developed a classifier that represents local context by morphology (the inflection on the target word), the syntactic category of words within a window of ±2 words from the target, and collocation-specific items found in the sentence. The collocation-specific items are those determined to be the most informative, where an item is considered informative if the model for independence between it and a sense tag provided a poor fit to the training data. The relative probabilities of senses, available from the training corpus, are used in the decision process as prior probabilities. For each test example, the evidence in its local context is combined in a Bayesian-type model of the probability of each sense, and the most probable sense is selected. Performance ranges from 77-84% correct on the test words, where a lower bound for performance based on always selecting the most frequent sense for the same words (i.e., the sense with the greatest prior probability) would yield 53-80% correct. Yarowsky (1994), building on his earlier work, designed a classifier that looks at words within ±k positions from the target; lemma forms are obtained through morphological analysis; and a coarse part-of-speech assignment is performed by dictionary lookup. Context is represented by collocations based on words or parts of speech at specific positions within the window or, less specifically, in any position. Also coded are some special classes of words, such as WEEKDAY, that might serve to distinguish among word senses. For each type of local-context evidence found in the corpus, a log-likelihood ratio is constructed, indicating the strength of the evidence for one form of the homograph versus the other. These ratios are then arranged in a sorted decision list with the largest values (strongest evidence) first. A decision is made for a test sentence by scanning down the decision list until a match is found. Thus, only the single best piece of evidence is used. The classifier was tested on disambiguating the homographs that result from accent removal in Spanish and French (e.g., seria, seria). In tests with the number of training examples ranging from a few hundred to several thousand, overall accuracy was high, above 90%. Clearly, sense identification is an active area of research, and considerable ingenuity is apparent. But despite the promising results reported in this literature, the reality is that there still are no large-scale, operational systems for tagging the senses of words in text. The statistical classifier, TLC, uses topical context, local context, or a combination of the two, for word sense identification. TLC's flexibility in using both forms is an important asset for our investigations. A noun, a verb, and an adjective were tested in this study. Table 1 provides a synonym or brief gloss for each of the senses used. Training corpora and testing corpora were collected as follows: Wall Street Journal corpus and from the American Printing House for the Blind corpus.' Examples for hard were taken from the LDC San Jose Mercury News (SJM) corpus. Each consisted of the sentence containing the target and one sentence preceding it. The resulting strings had an average length of 49 items. 2. Examples where the target was the head of an unambiguous collocation were removed from the files. Being unambiguous, they do not need to be disambiguated. These collocations, for example, product line and hard candy were found using WordNet. In Section 3, we consider how they can be used for unsupervised training. Examples where the target was part of a proper noun were also removed; for example, Japan Air Lines was not taken as an example of line. first 25, 50, 100, and 200 examples of the least frequent sense, and examples from the other senses in numbers that reflected their relative frequencies in the corpus. As an illustration, in the smallest training set for hard, there were 25 examples of the least frequent sense, 37 examples of the second most frequent sense, and 256 examples of the most frequent sense. The test sets were of fixed size: each contained 150 of the least frequent sense and examples of the other senses in numbers that reflected their relative frequencies. The operation of TLC consists of preprocessing, training, and testing. During preprocessing, examples are tagged with a part-of-speech tagger (Brill 1994); special tags are inserted at sentence breaks; and each open-class word found in WordNet is replaced with its base form. This step normalizes across morphological variants without resorting to the more drastic measure of stemming. Morphological information is not lost, since the part-of-speech tag remains unchanged. Training consists of counting the frequencies of various contextual cues for each sense. Testing consists of taking a new example of the polysemous word and computing the most probable sense, based on the cues present in the context of the new item. A comparison is made to the sense assigned by a human judge, and the classifier's decision is scored as correct or incorrect. TLC uses a Bayesian approach to find the sense s, that is the most probable given the cues ci contained in a context window of ±k positions around the polysemous target word. For each s„ the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_k,. • • s,) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: In TLC, we have made this assumption and have estimated p(ci I si) from the training. Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj s,), including providing probabilities for cues that did not occur in the training. TLC actually uses the mean of the Good-Turing value and the training-derived value for p(cj s,). When cues do not appear in training, it uses the mean of the GoodTuring value and the global probability of the cue p(ci), obtained from a large text corpus. This approach to smoothing has yielded consistently better performance than relying on the Good-Turing values alone. tuation. For this cue type, p(cj I s,) is the probability that item cl appears precisely at location j for sense Si. Positions j = —2, —1, 1,2 are used. The global probabilities, for example p(the_i), are based on counts of closed-class items found at these positions relative to the nouns in a large text corpus. The local window width of ±2 was selected after pilot testing on the semantically tagged Brown corpus. As in (2) above, the local window does not extend beyond a sentence boundary. 4. Part-of-speech tags in the positions j = —2, —1,0, 1,2 are also used as cues. The probabilities for these tags are computed for specific positions (e.g., p(DT_i I p(DTA) in a manner similar to that described in (3) above. When TLC is configured to use only topical information, cue type (1) is employed. When it is configured for local information, cue types (2), (3), and (4) are used. Finally, in combined mode, the set of cues contains all four types. 2.3 Results Figures 1 to 3 show the accuracy of the classifier as a function of the size of the training set when using local context, topical context, and a combination of the two, averaged across three runs for each training set. To the extent that the words used are representative, some clear differences appear as a function of syntactic category. With the verb serve, local context was more reliable than topical context at all levels of training (78% versus 68% with 200 training examples for the least frequent sense). The combination of local and topical context showed improvement (83%) over either form alone (see Figure 1). With the adjective hard, local context was much more reliable as an indicator of sense than topical context for all training sizes (83% versus 60% with 200 training examples) and the combined classifier's performance (at 83%) was the same as for local (see Figure 2). In the case of the noun line, topical was slightly better than local at all set sizes, but with 200 training examples, their combination yielded 84% accuracy, greater than either topical (78%) or local (67%) alone (see Figure 3). To summarize, local context was more reliable than topical context as an indicator of sense for this verb and this adjective, but slightly less reliable for this noun. The combination of local and topical context showed improved or equal performance for all three words. Performance for all of the classifiers improved with increased training size. All classifiers performed best with at least 200 training examples per sense, but the learning curve tended to level off beyond a minimum 100 training examples. These results are consistent with those of Yarowsky (1993), based on his experiments with pseudowords, homophones, and homonyms (discussed below). He observed that performance for verbs and adjectives dropped sharply as the window increased, while distant context remained useful for nouns. Thus one is tempted to conclude that nouns depend more on topic than do verbs and adjectives. But such a conclusion is probably an overgeneralization, inasmuch as some noun senses are clearly nontopical. Thus, Leacock, Towell, and Voorhees (1993) found that some senses of the noun line are not susceptible to disambiguation with topical context. For example, the 'textual' sense of line can appear with any topic, whereas the 'product' sense of line cannot. When it happens that a nontopical sense accounts for a large proportion of occurrences (in our study, all senses of hard are nontopical), then adding topical context to local will have little benefit and may even reduce accuracy. One should not conclude from these results that the topical classifiers and TLC are inferior to the classifiers reviewed in Section 2. In our experiments, monosemous collocations in WordNet that contain the target word were systematically removed from the training and testing materials. This was done on the assumption that these words are not ambiguous. Removing them undoubtedly made the task more difficult than it would normally be. How much more difficult? An estimate is possible. We Classifier performance on four senses of the verb serve. Percentage accounted for by most frequent sense = 41%. searched through 7,000 sentences containing line and found 1,470 sentences contained line as the head of a monosemous collocation in WordNet, i.e., line could be correctly disambiguated in some 21% of those 7,000 sentences simply on the basis of the WordNet entries in which it occurred. In other words, if these sentences had been included in the experiment—and had been identified by automatic lookup—overall accuracy would have increased from 83% to 87%. Using topical context alone, TLC performs no worse than other topical classifiers. Leacock, Towell, and Voorhees (1993) report that the three topical classifiers tested averaged 74% accuracy on six senses of the noun line. With these same training and testing data, TLC performed at 73% accuracy. Similarly, when the content vector and neural network classifiers were run on manually tagged training and testing examples of the verb serve, they averaged 74% accuracy—as did TLC using only topical context. When local context is combined with topical, TLC is superior to the topical classifiers compared in the Leacock, Towel!, and Voorhees (1993) study. Just how useful is a sense classifier whose accuracy is 85% or less? Probably not very useful if it is part of a fully automated NLP application, but its performance might be adequate in an interactive application (e.g., machine-assisted translation, on-line thesaurus functions in word processing, interactive information retrieval). In fact, when recall does not have to be 100% (as when a human is in the loop) the precision of the classifier can be improved considerably. The classifier described above always selects the sense that has the highest probability. We have observed that when Classifier performance on three senses of the adjective hard. Percentage accounted for by most frequent sense = 80%. the difference between the probability of this sense and that of the second highest is relatively small, the classifier's choice is often incorrect. One way to improve the precision of the classifier, though at the price of reduced recall, is to identify these situations and allow it to respond do not know rather than forcing a decision. What is needed is a measure of the difference in the probabilities of the two senses. Following the approach of Dagan and Itai (1994), we use the log of the ratio of the probabilities In(pi/p2) for this purpose. Based on this value, a threshold e can be set to control when the classifier selects the most probable sense. For example, if e = 2, then ln(pi/p2) must be 2 or greater for a decision to be made. Dagan and Itai (1994) also describe a way to make the threshold dynamic so that it adjusts for the amount of evidence used to estimate pi and p2. The basic idea is to create a one-tailed confidence interval so that we can state with probability 1 — a that the true value of the difference measure is greater than O. When the amount of evidence is small, the value of the measure must be larger in order to insure that e is indeed exceeded. Table 2 shows precision and recall values for serve, hard, and line at eight different settings of 0 using a 60% confidence interval. TLC was first trained on 100 examples of each sense, and it was then tested on separate 100-example sets. In all cases, precision was positively correlated with the square root of 0 (all r values > .97), and recall was negatively correlated with the square root of 0 (r values < —.96). As cross-validation, the equations of the lines that fit the precision and recall results on the test sample were used to predict the precision and recall at the various values of 0 on a second test sample. They provided a good fit to the new data, accounting for an average of 93% of the variance. The standard errors of estimate for hard, serve, and line were .028, .030, and .029 for precision, and .053, .068, and .041 for recall. This demonstrates that it is possible to produce accurate predictions of precision and recall as a function of for new test sets. When the threshold is set to a large value, precision approaches 100%. The criterion thus provides a way to locate those cases that can be identified automatically with very high accuracy. When TLC uses a high criterion for assigning senses, it can be used to augment the training examples by automatically collecting new examples from the test corpus. In summary, the results obtained with TLC support the following preliminary conclusions: (a) improvement with training levels off after about 100 training examples for the least frequent sense; (b) the high predictive power of local context for the verb and adjective indicate that the local parameters effectively capture syntactically mediated relations, e.g., the subject and object or complement of verbs, or the noun that an adjective modifies; (c) nouns may be more &quot;topical&quot; than verbs and adjectives, and therefore benefit more from the combination of topical and local context; (d) the precision of TLC can be considerably improved at the price of recall, a trade-off that may be desirable in some interactive NLP applications. A final observation we can make is that when topical and local information is combined, what we have called &quot;nontopical senses&quot; can reduce overall accuracy. For example, the 'textual' sense of line is relatively topic-independent. The results of the line experiment were not affected too adversely because the nontopical sense of line accounted for only 10% of the training examples. The effects of nontopical senses will be more serious when most senses are nontopical, as in the case of many adjectives and verbs. The generality of these conclusions must, of course, be tested with additional words, which brings us to the problem of obtaining training and testing corpora. On one hand, it is surprising that a purely statistical classifier can &quot;learn&quot; how to identify a sense of a polysemous word with as few as 100 example contexts. On the other hand, anyone who has manually built such sets knows that even collecting 100 examples of each sense is a long and tedious process. The next section presents one way in which the lexical knowledge in WordNet can be used to extract training examples automatically. Corpus-based word sense identifiers are data hungry—it takes them mere seconds to digest all of the information contained in training materials that take months to prepare manually. So, although statistical classifiers are undeniably effective, they are not feasible until we can obtain reliable unsupervised training data. In the Gale, Church, and Yarowsky (1992a) study, training and testing materials were automatically acquired using an aligned French-English bilingual corpus by searching for English words that have two different French translations. For example, English tokens of sentence were translated as either peine or phrase. They collected contexts of sentence translated as peine to build a corpus for the judicial sense, and collected contexts of sentence translated as phrase to build a corpus for the grammatical sense. One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare, and aligned bilingual corpora are even rarer. Another is that since French and English are so closely related, different senses of polysemous English words often translate to the same French word. For example, line is equally polysemous in French and English—and most senses of line translate into French as ligne. Several artificial techniques have been used so that classifiers can be developed and tested without having to invest in manually tagging the data: Yarowsky (1993) and Schtitze (1995) have acquired training and testing materials by creating pseudowords from existing nonhomographic forms. For example, a pseudoword was created by combining abused/escorted. Examples containing the string escorted were collected to train on one sense of the pseudoword and examples containing the string abused were collected to train on the other sense. In addition, Yarowsky (1993) used homophones (e.g., cellar/seller) and Yarowsky (1994) created homographs by stripping accents from French and Spanish words. Although these latter techniques are useful in their own right (e.g., spoken language systems or corrupted transmissions), the resulting materials do not generalize to the acquisition of tagged training for real polysemous or even homographic words. The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993). Yarowsky (1992) used a thesaurus to collect training materials. He tested the unsupervised training materials on 12 nouns with almost perfect results on homonyms (95-99%), 72% accuracy for four senses of interest, and 77% on three senses of cone. The training was collected in the following manner. Take a Roget's category—his examples were TOOL and ANIMAL—and collect sentences from a corpus (in this case, Grolier's Encyclopedia) using the words in each category. Consider the noun crane, which appears in both the Roget's categories TOOL and ANIMAL. To represent the TOOL category, Yarowsky extracted contexts from Gro/ier's Encyclopedia. For example, contexts with the words adz, shovel, crane, sickle, and so on. Similarly he collected sentences with names of animals from the ANIMAL category. In these samples, crane and drill appeared under both categories. Yarowsky points out that the resulting noise will be a problem only when one of the spurious senses is salient, dominating the training set, and he uses frequency-based weights to minimize these effects. We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous. Schtitze (1995) developed a statistical topical approach to word sense identification that provides its own automatically extracted training examples. For each occurrence t of a polysemous word in a corpus, a context vector is constructed by summing all the vectors that represent the co-occurrence patterns of the open-class words in t's context (i.e., topical information is expressed as a kind of second-order co-occurrence). These context vectors are clustered, and the centroid of each cluster is used to represent a &quot;sense.&quot; When given a new occurrence of the word, a vector of the words in its context is constructed, and this vector is compared to the sense representations to find the closest match. Schulze has used the method to disambiguate pseudowords, homographs, and polysemous words. Performance varies depending, in part, on the number of clusters that are created to represent senses, and on the degree to which the distinctions correspond to different topics. This approach performs very well, especially with pseudowords and homographs. However, there is no automatic means to map the sense representations derived from the system onto the more conventional word senses found in dictionaries. Consequently, it does not provide disambiguated examples that can be used by other systems. Yarowsky (1995) has proposed automatically augmenting a small set of experimenter-supplied seed collocations (e.g., manufacturing plant and plant life for two different senses of the noun plant) into a much larger set of training materials. He resolved the problem of the sparseness of his collocations by iteratively bootstrapping acquisition of training materials from a few seed collocations for each sense of a homograph. He locates examples containing the seeds in the corpus and analyzes these to find new predictive patterns in these sentences and retrieves examples containing these patterns. He repeats this step iteratively. Results for the 12 pairs of homographs reported are almost perfect. In his paper, Yarowsky suggests WordNet as a source for the seed collocations—a suggestion that we pursue in the next section. WordNet is particularly well suited to the task of locating sense-relevant context because each word sense is represented as a node in a rich semantic lexical network with synonymy, hyponymy, and meronymy links to other words, some of them polysemous and others monosemous. These lexical &quot;relatives&quot; provide a key to finding relevant training sentences in a corpus. For example, the noun suit is polysemous, but one sense of it has business suit as a monosemous daughter and another has legal proceeding as a hypernym. By collecting sentences containing the unambiguous nouns business suit and legal proceeding we can build two corpora of contexts for the respective senses of the polysemous word. All the systems described in Section 2.1 could benefit from the additional training materials that monosemous relatives can provide. The WordNet on-line lexical database (Miller 1990, 1995) has been developed at Princeton University over the past 10 years.' Like a standard dictionary, WordNet contains the definitions of words. It differs from a standard dictionary in that, instead of being organized alphabetically, WordNet is organized conceptually. The basic unit in WordNet is a synonym set, or synset, which represents a lexicalized concept. For example, WordNet Version 1.5 distinguishes between two senses of the noun shot with the synsets {shot, snapshot} and {shot, injection}. In the context, &quot;The photographer took a shot of Mary,&quot; the word snapshot can be substituted for one sense of shot. In the context, &quot;The nurse gave Mary a flu shot,&quot; the word injection can be substituted for another sense of shot. Nouns, verbs, adjectives, and adverbs are each organized differently in WordNet. All are organized in synsets, but the semantic relations among the synsets differ depending on the grammatical category, as can be seen in Table 3. Nouns are organized in a hierarchical tree structure based on hypernymy/hyponymy. The hyponym of a noun is its subordinate, and the relation between a hyponym and its hypernym is an is a kind of relation. For example, maple is a hyponym of tree, which is to say that a maple is a kind of tree. Hypernymy (supername) and its inverse, hyponymy (subname), are transitive semantic relations between synsets. Meronymy (part-name), and its inverse holortymy (whole-name), are complex semantic relations that distinguish component parts, substantive parts, and member parts. The verbal hierarchy is based on troponymy, the is a manner of relation. For example, stroll is a troponym of walk, which is to say that strolling is a manner of walking. Entailment relations between verbs are also coded in WordNet. The organization of attributive adjectives is based on the antonymy relation. Where direct antonyms exist, adjective synsets point to antonym synsets. A head adjective is one that has a direct antonym (e.g., hot versus cold or long versus short). Many adjectives, like sultry, have no direct antonyms. When an adjective has no direct antonym, its synset points to a head that is semantically similar to it. Thus sultry and torrid are similar in meaning to hot, which has the direct antonym of cold. So, although sultry has no direct antonym, it has cold as its indirect antonym. Relational adjectives do not have antonyms; instead they point to nouns. Consider the difference between a nervous disorder and a nervous student. In the former, nervous pertains to a noun, as in nervous system, whereas the latter is defined by its relation to other adjectives—its synonyms (e.g., edgy) and antonyms (e.g., relaxed). Adverbs have synonymy and antonymy relations. When the adverb is morphologically related to an adjective (when an -ly suffix is added to an adjective) and semantically related to the adjective as well, the adverb points to the adjective. We have had some success in exploiting WordNet's semantic relations for word sense identification. Since the main problem with classifiers that use local context is the sparseness of the training data, Leacock and Chodorow (1998) used a proximity measure on the hypernym relation to replace the subject and complement of the verb serve in the testing examples with the subject and complement from training examples that were &quot;closest&quot; to them in the noun hierarchy. For example, one of the test sentences was &quot;Sauerbraten is usually served with dumplings,&quot; where neither sauerbraten nor dumpling appeared in any training sentence. The similarity measures on WordNet found that sauerbraten was most similar to dinner in the training, and dumpling to bacon. These nouns were substituted for the novel ones in the test sets. Thus the sentence &quot;Dinner is usually served with bacon&quot; was substituted for the original sentence. Augmentation of the local .context classifier with WordNet similarity measures showed a small but consistent improvement in the classifier's performance. The improvement was greater with the smaller training sets. Resnik (1992) uses an information-based measure, the most informative class, on the WordNet taxonomy. A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms). Based on verb/object pairs collected from a corpus, Resnik found, for example, that the objects for the verb open fall into two classes: receptacle and oral communication. Conversely, the class of a verb's object could be used to determine the appropriate sense of that verb. The experiments in the next section depend on a subset of the WordNet lexical relations, those involving monosemous relatives, so we were interested in determining just what proportion of word senses have such relatives. We examined 8,500 polysemous nouns that appeared in a moderate-size, 25-million-word corpus. In all, these 8,500 nouns have more than 24,000 WordNet senses. Restricting the relations to synonyms, immediate hyponyms (i.e., daughters), and immediate hypernyms (parents), we found that about 64% (15,400) have monosemous relatives attested in the corpus. With larger corpora (e.g., with text obtained by Web crawling) and more lexical relations (e.g., meronymy), this percentage can be expected to increase. The approach we have used is related to that of Yarowsky (1992) in that training materials are collected using a knowledge base, but it differs in other respects, notably in the selection of training and testing materials, the choice of a knowledge base, and use of both topical and local classifiers. Yarowsky collects his training and testing materials from a specialized corpus, Grolier's Encyclopedia. It remains to be seen whether a statistical classifier trained on a topically organized corpus such as an encyclopedia will perform in the same way when tested on general unrestricted text, such as newspapers, periodicals, and books. One of our goals is to determine whether automatic extraction of training examples is feasible using general corpora. In his experiment, Yarowsky uses an updated on-line version of Roget's Thesaurus that is not generally available to the research community. The only generally available version of Roget's is the 1912 edition, which contains many lexical gaps. We are using WordNet, which can be obtained via anonymous ftp. Yarowsky's classifier is purely topical, but we also examine local context. Finally, we hope to avoid inclusion of spurious senses by using monosemous relatives. In this experiment we collected monosemous relatives of senses of 14 nouns. Training sets are created in the following manner. A program called AutoTrain retrieves from WordNet all of the monosemous relatives of a polysemous word sense, samples and retrieves example sentences containing these monosemous relatives from a 30-million-word corpus of the San Jose Mercury News, and formats them for TLC. The sampling process retrieves the &quot;closest&quot; relatives first. For example, suppose that the system is asked to retrieve 100 examples for each sense of the noun court. The system first looks for the strongest or top-level relatives: for monosemous synonyms of the sense (e.g., tribunal) and for daughter collocations that contain the target word as the head (e.g., superior court) and tallies the number of examples in the corpus for each. If the corpus has 100 or more examples for these top-level relatives, it retrieves a sampling of them and formats them for TLC. If there are not enough top-level examples, the remainder of the target's monosemous relatives are inspected in the order: all other daughters; hyponym collocations that contain the target; all other hyponyms; hypernyms; and, finally, sisters. AutoTrain takes as broad a sampling as possible across the corpus and never takes more than one example from an article. The number of examples for each relative is based on the relative proportion of its occurrences in the corpus. Table 4 shows the monosemous relatives that were used to train five senses of the noun line—the monosemous relatives of the sixth sense in the original study, line as an abstract division, are not attested in the SIM corpus. The purpose of the experiment was to see how well TLC performed using unsupervised training and, when possible, to compare this with its performance when training on the manually tagged materials being produced at Princeton's Cognitive Science Laboratory.' When a sufficient number of examples for two or more senses were available, 100 examples of each sense were set aside to use in training. The remainder were used for testing. Only the topical and local open-class cues were used, since preliminary tests showed that performance declined when using local closed-class and part-of-speech cues obtained from the monosemous relatives. This is not surprising, as many of the relatives are collocations whose local syntax is quite different from that of the polysemous word in its typical usage. For example, the 'formation' sense of line is often followed by an of-phrase as in a line of children, but its relative, picket line, is not. Prior probabilities for the sense were taken from the manually tagged materials. Table 5 shows the results when TLC was trained on monosemous relatives and on manually tagged training materials. Baseline performance is when the classifier always chooses the most frequent sense. Eight additional words had a sufficient number of manually tagged examples for testing but not for training TLC. These are shown in Table 6. For four of the examples in Table 5, training with relatives produced results within 1% or 2% of manually tagged training. Line and work, however, showed a substantial decrease in performance. In the case of line, this might be due to overly specific training contexts. Almost half of the training examples for the 'formation' sense of line come from one relative, picket line. In fact, all of the monosemous relatives, except for rivet line and trap line, are human formations. This may have skewed training so that the classifier performs poorly on other uses of line as formation. In order to compare our results with those reported in Yarowsky (1992), we trained and tested on the same two senses of the noun duty that Yarowsky had tested ('obligation' and 'tax'). He reported that his thesaurus-based approach yielded 96% precision with 100% recall. TLC used training examples based on monosemous WordNet relatives and correctly identified the senses with 93.5% precision at 100% recall. Table 6 shows TLC's performance on the other eight words after training with monosemous relatives and testing on manually tagged examples. Performance is about the same as, or only slightly better than, the highest prior probability. In part, this is due to the rather high probability of the most frequent sense for this set. The values in the table are based on decisions made on all test examples. If a threshold is set for TLC (see Section 2.4), precision of the classifier can be increased substantially, at the expense of recall. Table 7 shows recall levels when TLC is trained on monosemous relatives and the value of e is set for 95% precision. Operating in this mode, the classifier can gather new training materials, automatically, and with high precision. This is a particularly good way to find clear cases of the most frequent sense. The results also show that not all words are well suited to this kind of operation. Little can be gained for a word like work, where the two senses, 'activity' and 'product,' are closely related and therefore difficult for the classifier to distinguish, due to a high degree of overlap in the training contexts. Problems of this sort can be detected even before testing, by computing correlations between the vectors of open-class words for the different senses. The cosine correlation between the 'activity' and 'product' senses of work is r = .49, indicating a high degree of overlap. The mean correlation between pairs of senses for the other words in Table 7 is r = .31. Our evidence indicates that local context is superior to topical context as an indicator of word sense when using a statistical classifier. The benefits of adding topical to local context alone depend on syntactic category as well as on the characteristics of the individual word. The three words studied yielded three different patterns; a substantial benefit for the noun line, slightly less for the verb serve, and none for the adjective hard. Some word senses are simply not limited to specific topics, and appear freely in many different domains of discourse. The existence of nontopical senses also limits the applicability of the &quot;one sense per discourse&quot; generalization of Gale, Church, and Yarowsky (1992b), who observed that, within a document, a repeated word is almost always used in the same sense. Future work should be directed toward developing methods for determining when a word has a nontopical sense. One approach to this problem is to look for a word that appears in many more topical domains than its total number of senses. Because the supply of manually tagged training data will always be limited, we propose a method to obtain training data automatically using commonly available materials: exploiting WordNet's lexical relations to harvest training examples from LDC corpora or even the World Wide Web. We found this method to be effective, although not as effective as using manually tagged training. We have presented the components of a system for acquiring unsupervised training materials that can be used with any statistical classifier. The components can be fit together in the following manner. For a polysemous word, locate the monosemous relatives for each of its senses in WordNet and extract examples containing these relatives from a large corpus. Senses whose contexts greatly overlap can be identified with a simple cosine correlation. Often, correlations are high between senses of a word that are systematically related, as we saw for the 'activity' and 'product' senses of work. In some cases, the contexts for the two closely related senses may be combined. Since the frequencies of the monosemous relatives do not correlate with the frequencies of the senses, prior probabilities must be estimated for classifiers that use them. In the experiments of Section 3.2, these were estimated from the testing materials. They can also be estimated from a small manually tagged sample, such as the parts of the Brown corpus that have been tagged with senses in WordNet. When the threshold is set to maximize precision, the results are highly reliable and can be used to support an interactive application, such as machine-assisted translation, with the goal of reducing the amount of interaction. Although we have looked at only a few examples, it is clear that, given WordNet and a large enough corpus, the methods outlined for training on monosemous relatives can be generalized to build training materials for thousands of polysemous words. We are indebted to the other members of the WordNet group who have provided advice and technical support: Christiane Fellbaum, Shari Landes, and Randee Tengi. We are also grateful to Paul Bagyenda, Ben Johnson-Laird and Joshua Schecter. We thank Scott Wayland, Tim Allison and Jill Hollifield for tagging the serve and hard corpora. Finally we are grateful to the three anonymous CL reviewers for their comments and advice. This material is based upon work supported in part by the National Science Foundation under NSF Award No. 1R19528983 and by the Defense Advanced Research Projects Agency, Grant No. N00014-91-1634.
Automatic Labeling Of Semantic Roles present a system for identifying the semantic relationships, or filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word frame, the system labels constituents with either abstract semantic roles, such as or more domain-specific semantic roles, such as and The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. Recent years have been exhilarating ones for natural language understanding. The excitement and rapid advances that had characterized other language-processing tasks such as speech recognition, part-of-speech tagging, and parsing have finally begun to appear in tasks in which understanding and semantics play a greater role. For example, there has been widespread commercial deployment of simple speech-based natural language understanding systems that answer questions about flight arrival times, give directions, report on bank balances, or perform simple financial transactions. More sophisticated research systems generate concise summaries of news articles, answer fact-based questions, and recognize complex semantic and dialogue structure. But the challenges that lie ahead are still similar to the challenge that the field has faced since Winograd (1972): moving away from carefully hand-crafted, domaindependent systems toward robustness and domain independence. This goal is not as far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms. Current information extraction and dialogue understanding systems, however, are still based on domain-specific frame-and-slot templates. Systems for booking airplane information use domain-specific frames with slots like ORIG CITY, DEST CITY, or DEPART TIME (Stallard 2000). Systems for studying mergers and acquisitions use slots like PRODUCTS, RELATIONSHIP, JOINT VENTURE COMPANY, and AMOUNT (Hobbs et al. 1997). For natural language understanding tasks to proceed beyond these specific domains, we need semantic frames and semantic understanding systems that do not require a new set of slots for each new application domain. In this article we describe a shallow semantic interpreter based on semantic roles that are less domain specific than TO AIRPORT or JOINT VENTURE COMPANY. These roles are defined at the level of semantic frames of the type introduced by Fillmore (1976), which describe abstract actions or relationships, along with their participants. For example, the JUDGEMENT frame contains roles like JUDGE, EVALUEE, and REASON, and the STATEMENT frame contains roles like SPEAKER, ADDRESSEE, and MESSAGE, as the following examples show: These shallow semantic roles could play an important role in information extraction. For example, a semantic role parse would allow a system to realize that the ruling that is the direct object of change in (3) plays the same THEME role as the ruling that is the subject of change in (4): The fact that semantic roles are defined at the frame level means, for example, that the verbs send and receive would share the semantic roles (SENDER, RECIPIENT, GOODS, etc.) defined with respect to a common TRANSFER frame. Such common frames might allow a question-answering system to take a question like (5) and discover that (6) is relevant in constructing an answer to the question: This shallow semantic level of interpretation has additional uses outside of generalizing information extraction, question answering, and semantic dialogue systems. One such application is in word sense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorporating semantic roles into probabilistic models of language may eventually yield more accurate parsers and better language models for speech recognition. This article describes an algorithm for identifying the semantic roles filled by constituents in a sentence. We apply statistical techniques that have been successful for the related problems of syntactic parsing, part-of-speech tagging, and word sense disambiguation, including probabilistic parsing and statistical classification. Our statistical algorithms are trained on a hand-labeled data set: the FrameNet database (Baker, Fillmore, and Lowe 1998; Johnson et al. 2001). The FrameNet database defines a tag set of semantic roles called frame elements and included, at the time of our experiments, roughly 50,000 sentences from the British National Corpus hand-labeled with these frame elements. This article presents our system in stages, beginning in Section 2 with a more detailed description of the data and the set of frame elements or semantic roles used. We then introduce (in Section 3) the statistical classification technique used and examine in turn the knowledge sources of which our system makes use. Section 4 describes the basic syntactic and lexical features used by our system, which are derived from a Penn Treebank–style parse of individual sentences to be analyzed. We break our task into two subproblems: finding the relevant sentence constituents (deferred until Section 5), and giving them the correct semantic labels (Sections 4.2 and 4.3). Section 6 adds higher-level semantic knowledge to the system, attempting to model the selectional restrictions on role fillers not directly captured by lexical statistics. We compare hand-built and automatically derived resources for providing this information. Section 7 examines techniques for adding knowledge about systematic alternations in verb argument structure with sentence-level features. We combine syntactic parsing and semantic role identification into a single probability model in Section 8. Section 9 addresses the question of generalizing statistics from one target predicate to another, beginning with a look at domain-independent thematic roles in Section 9.1. Finally we draw conclusions and discuss future directions in Section 10. Semantic roles are one of the oldest classes of constructs in linguistic theory, dating back thousands of years to Panini’s k¯araka theory (Misra 1966; Rocher 1964; Dahiya 1995). Longevity, in this case, begets variety, and the literature records scores of proposals for sets of semantic roles. These sets of roles range from the very specific to the very general, and many have been used in computational implementations of one type or another. At the specific end of the spectrum are domain-specific roles such as the FRoM AIRPoRT, To AIRPoRT, or DEPART TIME discussed above, or verb-specific roles such as EATER and EATEN for the verb eat. The opposite end of the spectrum consists of theories with only two “proto-roles” or “macroroles”: PRoTo-AGENT and PRoTo-PATIENT (Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10 roles, such as Fillmore’s (1971) list of nine: AGENT, EXPERIENCER, INSTRUMENT, OBJECT, SoURCE, GoAL, LoCATIoN, TIME, and PATH.1 Sample domains and frames from the FrameNet lexicon. Many of these sets of roles have been proposed by linguists as part of theories of linking, the part of grammatical theory that describes the relationship between semantic roles and their syntactic realization. Other sets have been used by computer scientists in implementing natural language understanding systems. As a rule, the more abstract roles have been proposed by linguists, who are more concerned with explaining generalizations across verbs in the syntactic realization of their arguments, whereas the more specific roles have more often been proposed by computer scientists, who are more concerned with the details of the realization of the arguments of specific verbs. The FrameNet project (Baker, Fillmore, and Lowe 1998) proposes roles that are neither as general as the 10 abstract thematic roles, nor as specific as the thousands of potential verb-specific roles. FrameNet roles are defined for each semantic frame. A frame is a schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore 1976). For example, the frame CONVERSATION, shown in Figure 1, is invoked by the semantically related verbs argue, banter, debate, converse, and gossip, as well as the nouns dispute, discussion, and tiff, and is defined as follows: The roles defined for this frame, and shared by all its lexical entries, include PROTAGONIST-1 and PROTAGONIST-2 or simply PROTAGONISTS for the participants in the conversation, as well as MEDIUM and TOPIC. Similarly, the JUDGMENT frame mentioned above has the roles JUDGE, EVALUEE, and REASON and is invoked by verbs such as blame, admire, and praise and nouns such as fault and admiration. We refer to the roles for a given frame as frame elements. A number of hand-annotated examples from the JUDGMENT frame are included below to give a flavor of the FrameNet database: Defining semantic roles at this intermediate frame level helps avoid some of the well-known difficulties of defining a unique small set of universal, abstract thematic roles while also allowing some generalization across the roles of different verbs, nouns, and adjectives, each of which adds semantics to the general frame or highlights a particular aspect of the frame. One way of thinking about traditional abstract thematic roles, such as AGENT and PATIENT, in the context of FrameNet is to conceive them as frame elements defined by abstract frames, such as action and motion, at the top of an inheritance hierarchy of semantic frames (Fillmore and Baker 2000). The examples above illustrate another difference between frame elements and thematic roles as commonly described in the literature. Whereas thematic roles tend to be arguments mainly of verbs, frame elements can be arguments of any predicate, and the FrameNet database thus includes nouns and adjectives as well as verbs. The examples above also illustrate a few of the phenomena that make it hard to identify frame elements automatically. Many of these are caused by the fact that there is not always a direct correspondence between syntax and semantics. Whereas the subject of blame is often the JUDGE, the direct object of blame can be an EVALUEE (e.g., the poor in “blaming the poor”) or a REASON (e.g., everything in “blame everything on coyotes”). The identity of the JUDGE can also be expressed in a genitive pronoun, (e.g., his in “his praise”) or even an adjective (e.g., critical in “critical praise”). The corpus used in this project is perhaps best described in terms of the methodology used by the FrameNet team. We outline the process here; for more detail see Johnson et al. (2001). As the first step, semantic frames were defined for the general domains chosen; the frame elements, or semantic roles for participants in a frame, were defined; and a list of target words, or lexical predicates whose meaning includes aspects of the frame, was compiled for each frame. Example sentences were chosen by searching the British National Corpus for instances of each target word. Separate searches were performed for various patterns over lexical items and part-of-speech sequences in the target words’ context, producing a set of subcorpora for each target word, designed to capture different argument structures and ensure that some examples of each possible syntactic usage of the target word would be included in the final database. Thus, the focus of the project was on completeness of examples for lexicographic needs, rather than on statistically representative data. Sentences from each subcorpus were then annotated by hand, marking boundaries of each frame element expressed in the sentence and assigning tags for the annotated constituent’s frame semantic role, syntactic category (e.g., noun phrase or prepositional phrase), and grammatical function in relation to the target word (e.g., object or complement of a verb). In the final phase of the process, the annotated sentences for each target word were checked for consistency. In addition to the tags just mentioned, the annotations include certain other information, which we do not make use of in this work, such as word sense tags for some target words and tags indicating metaphoric usages. Tests of interannotator agreement were performed for data from a small number of predicates before the final consistency check. Interannotator agreement at the sentence level, including all frame element judgments and boundaries for one predicate, varied from .66 to .82 depending on the predicate. The kappa statistic (Siegel and Castellan 1988) varied from .67 to .82. Because of the large number of possible categories when boundary judgments are considered, kappa is nearly identical to the interannotator agreement. The system described in this article (which gets .65/.61 precision/recall on individual frame elements; see Table 15) correctly identifies all frame elements in 38% of test sentences. Although this .38 is not directly comparable to the .66–.82 interannotator agreements, it’s clear that the performance of our system still falls significantly short of human performance on the task. The British National Corpus was chosen as the basis of the FrameNet project despite differences between British and American usage because, at 100 million words, it provides the largest corpus of English with a balanced mixture of text genres. The British National Corpus includes automatically assigned syntactic part-of-speech tags for each word but does not include full syntactic parses. The FrameNet annotators did not make use of, or produce, a complete syntactic parse of the annotated sentences, although some syntactic information is provided by the grammatical function and phrase type tags of the annotated frame elements. The preliminary version of the FrameNet corpus used for our experiments contained 67 frame types from 12 general semantic domains chosen for annotation. A complete list of the semantic domains represented in our data is shown in Table 1, along with representative frames and predicates. Within these frames, examples of a total of 1,462 distinct lexical predicates, or target words, were annotated: 927 verbs, 339 nouns, and 175 adjectives. There are a total of 49,013 annotated sentences and 99,232 annotated frame elements (which do not include the target words themselves). How important is the particular set of semantic roles that underlies our system? For example, could the optimal choice of semantic roles be very dependent on the application that needs to exploit their information? Although there may well be application-specific constraints on semantic roles, our semantic role classifiers seem in practice to be relatively independent of the exact set of semantic roles under consideration. Section 9.1 describes an experiment in which we collapsed the FrameNet roles into a set of 18 abstract thematic roles. We then retrained our classifier and achieved roughly comparable results; overall performance was 82.1% for abstract thematic roles, compared to 80.4% for frame-specific roles. Although this doesn’t show that the detailed set of semantic roles is irrelevant, it does suggest that our statistical classification algorithm, at least, is relatively robust to even quite large changes in role identities. Assignment of semantic roles is an important part of language understanding, and the problem of how to assign such roles has been attacked by many computational systems. Traditional parsing and understanding systems, including implementations of unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each way in which semantic roles may be realized syntactically. Writing such grammars is time consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by “shallow” systems that avoid complex feature structures and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta filled a semantic slot such as DESTINATION in a semantic frame for air travel. In a data-driven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to derive automatically entire “case frames” for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalizing beyond the relatively small number of frames considered in the tasks. More recently, a domain-independent system has been trained by Blaheta and Charniak (2000) on the function tags, such as MANNER and TEMPORAL, included in the Penn Treebank corpus. Some of these tags correspond to FrameNet semantic roles, but the Treebank tags do not include all the arguments of most predicates. In this article, we aim to develop a statistical system for automatically learning to identify all semantic roles for a wide variety of predicates in unrestricted text. In this section we describe the first, basic version of our statistically trained system for automatically identifying frame elements in text. The system will be extended in later sections. We first describe in detail the sentence- and constituent-level features on which our system is based and then use these features to calculate probabilities for predicting frame element labels in Section 4.2. In this section we give results for a system that labels roles using the human-annotated boundaries for the frame elements within the sentence; we return to the question of automatically identifying the boundaries in Section 5. Our system is a statistical one, based on training a classifier on a labeled training set and testing on a held-out portion of the data. The system is trained by first using an automatic syntactic parser to analyze the 36,995 training sentences, matching annotated frame elements to parse constituents and extracting various features from the string of words and the parse tree. During testing, the parser is run on the test sentences and the same features are extracted. Probabilities for each possible semantic role r are then computed from the features. The probability computation is described in the next section; here we discuss the features used. The features used represent various aspects of the syntactic structure of the sentence as well as lexical information. The relationship between such surface manifestations and semantic roles is the subject of linking theory (see Levin and Rappaport Hovav [1996] for a synthesis of work in this area). In general, linking theory argues that the syntactic realization of arguments of a predicate is predictable from semantics; exactly how this relationship works, however, is the subject of much debate. Regardless of the underlying mechanisms used to generate syntax from semantics, the relationship between the two suggests that it may be possible to learn to recognize semantic relationships from syntactic cues, given examples with both types of information. 4.1.1 Phrase Type. Different semantic roles tend to be realized by different syntactic categories. For example, in communication frames, the SpEAKER is likely to appear as a noun phrase, Topic as a prepositional phrase or noun phrase, and MEDiUM as a prepositional phrase, as in: “[Speaker We ] talked [Topic about the proposal ] [Medium over the phone ] .” The phrase type feature we used indicates the syntactic category of the phrase expressing the semantic roles, using the set of syntactic categories of the Penn Treebank project, as described in Marcus, Santorini, and Marcinkiewicz (1993). In our data, frame elements are most commonly expressed as noun phrases (NPs, 47% of frame elements in the training set), and prepositional phrases (PPs, 22%). The next most common categories are adverbial phrases (ADVPs, 4%), particles (e.g. “make something up”; PRTs, 2%) and clauses (SBARs, 2%, and Ss, 2%). (Tables 22 and 23 in the Appendix provides a listing of Penn Treebank’s part-of-speech tags and constituent labels.) We used Collins’ (1997) statistical parser trained on examples from the Penn Treebank to generate parses of the same format for the sentences in our data. Phrase types were derived automatically from parse trees generated by the parser, as shown in Figure 2. Given the automatically generated parse tree, the constituent spanning the same set of words as each annotated frame element was found, and the constituent’s nonterminal label was taken as the phrase type. In cases in which more than one constituent matches because of a unary production in the parse tree, the higher constituent was chosen. A sample sentence with parser output (above) and FrameNet annotation (below). Parse constituents corresponding to frame elements are highlighted. The matching was performed by calculating the starting and ending word positions for each constituent in the parse tree, as well as for each annotated frame element, and matching each frame element with the parse constituent with the same beginning and ending points. Punctuation was ignored in this computation. Because of parsing errors, or, less frequently, mismatches between the parse tree formalism and the FrameNet annotation standards, for 13% of the frame elements in the training set, there was no parse constituent matching an annotated frame element. The one case of systematic mismatch between the parse tree formalism and the FrameNet annotation standards is the FrameNet convention of including both a relative pronoun and its antecedent in frame elements, as in the first frame element in the following sentence: Mismatch caused by the treatment of relative pronouns accounts for 1% of the frame elements in the training set. During testing, the largest constituent beginning at the frame element’s left boundary and lying entirely within the element was used to calculate the frame element’s features. We did not use this technique on the training set, as we expected that it would add noise to the data, but instead discarded examples with no matching parse constituent. Our technique for finding a near match handles common parse errors such as a prepositional phrase being incorrectly attached to a noun phrase at the right-hand edge, and it guarantees that some syntactic category will be returned: the part-of-speech tag of the frame element’s first word in the limiting case. alization as subject or direct object is one of the primary facts that linking theory attempts to explain. It was a motivation for the case hierarchy of Fillmore (1968), which allowed such rules as “If there is an underlying AGENT, it becomes the syntactic subject.” Similarly, in his theory of macroroles, Van Valin (1993) describes the ACTOR as being preferred in English for the subject. Functional grammarians consider syntactic subjects historically to have been grammaticalized agent markers. As an example of how such a feature can be useful, in the sentence “He drove the car over the cliff,” the subject NP is more likely to fill the AGENT role than the other two NPs. We will discuss various grammatical-function features that attempt to indicate a constituent’s syntactic relation to the rest of the sentence, for example, as a subject or object of a verb. The first such feature, which we call “governing category,” or gov, has only two values, S and VP, corresponding to subjects and objects of verbs, respectively. This feature is restricted to apply only to NPs, as it was found to have little effect on other phrase types. As with phrase type, the feature was read from parse trees returned by the parser. We follow links from child to parent up the parse tree from the constituent corresponding to a frame element until either an S or VP node is found and assign the value of the feature according to whether this node is an S or a VP. NP nodes found under S nodes are generally grammatical subjects, and NP nodes under VP nodes are generally objects. In most cases the S or VP node determining the value of this feature immediately dominates the NP node, but attachment errors by the parser or constructions such as conjunction of two NPs can cause intermediate nodes to be introduced. Searching for higher ancestor nodes makes the feature robust to such cases. Even given good parses, this feature is not perfect in discriminating grammatical functions, and in particular it confuses direct objects with adjunct NPs such as temporal phrases. For example, town in the sentence “He left town” and yesterday in the sentence “He left yesterday” will both be assigned a governing category of VP. Direct and indirect objects both appear directly under the VP node. For example, in the sentence “He gave me a new hose,” me and a new hose are both assigned a governing category of VP. More sophisticated handling of such cases could improve our system. 4.1.3 Parse Tree Path. Like the governing-category feature described above, the parse tree path feature (path) is designed to capture the syntactic relation of a constituent to the rest of the sentence. The path feature, however, describes the syntactic relation between the target word (that is, the predicate invoking the semantic frame) and the constituent in question, whereas the gov feature is independent of where the target word appears in the sentence; that is, it identifies all subjects whether they are the subject of the target word or not. The path feature is defined as the path from the target word through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 3. Although the path is composed as a string of symbols, our system treats the string as an atomic value. The path includes, as the first element of the string, the part of speech of the target word and, as the last element, the phrase type or syntactic category of the sentence constituent marked as a frame element. After some experimentation, we settled on a version of the path feature that collapses the various part-of-speech tags for verbs, including past-tense verb (VBD), third-person singular present-tense verb (VBZ), other present-tense verb (VBP), and past participle (VBN), into a single verb tag denoted “VB.” Our path feature is dependent on the syntactic representation used, which in our case is the Treebank-2 annotation style (Marcus et al. 1994), as our parser is trained on this later version of the Treebank data. Figure 4 shows the annotation for the sentence “They expect him to cut costs throughout the organization,” which exhibits In this example, the path from the target word ate to the frame element He can be represented as VBTVPTS↓NP, with T indicating upward movement in the parse tree and ↓ downward movement. The NP corresponding to He is found as described in Section 4.1.1. Treebank annotation of raising constructions. the syntactic phenomenon known as subject-to-object raising, in which the main verb’s object is interpreted as the embedded verb’s subject. The Treebank-2 style tends to be generous in its usage of S nodes to indicate clauses, a decision intended to make possible a relatively straightforward mapping from S nodes to predications. In this example, the path from cut to the frame element him would be VBTVPTVPTStNP, which typically indicates a verb’s subject, despite the accusative case of the pronoun him. For the target word of expect in the sentence of Figure 4, the path to him would be VBTVPtStNP, rather than the typical direct-object path of VBTVPtNP. An example of Treebank-2 annotation of an “equi” construction, in which a noun phrase serves as an argument of both the main and subordinate verbs, is shown in Figure 5. Here, an empty category is used in the subject position of the subordinate clause and is co-indexed with the NP Congress in the direct-object position of the main clause. The empty category, however, is not used in the statistical model of the parser or shown in its output and is also not used by the FrameNet annotation, which would mark the NP Congress as a frame element of raise in this example. Thus, the value of our path feature from the target word raise to the frame element Congress would be VBTVPTVPTSTVPtNP, and from the target word of persuaded the path to Congress would be the standard direct-object path VBTVPtNP. Other changes in annotation style from the original Treebank style were specifically intended to make predicate argument structure easy to read from the parse trees and include new empty (or null) constituents, co-indexing relations between nodes, and secondary functional tags such as subject and temporal. Our parser output, however, does not include this additional information, but rather simply gives trees of phrase type categories. The sentence in Figure 4 is one example of how the change in annotation style of Treebank-2 can affect this level of representation; the earlier style assigned the word him an NP node directly under the VP of expect. The most common values of the path feature, along with interpretations, are shown in Table 2. For the purposes of choosing a frame element label for a constituent, the path feature is similar to the gov feature defined above. Because the path captures more information than the governing category, it may be more susceptible to parser errors and data sparseness. As an indication of this, our path feature takes on a total of 2,978 possible values in the training data when frame elements with no matching Example of target word renting in a small clause. parse constituent are not counted and 4,086 possible values when paths are found to the best-matching constituent in these cases. The governing-category feature, on the other hand, which is defined only for NPs, has only two values (S, corresponding to subjects, and VP, corresponding to objects). In cases in which the path feature includes an S or VP ancestor of an NP node as part of the path to the target word, the gov feature is a function of the path feature. This is the case most of the time, including for our prototypical subject (VBTVPTStNP) and object (VBTVPtNP) paths. Of the 35,138 frame elements identified as NPs by the parser, only 4% have a path feature that does not include a VP or S ancestor. One such example is shown in Figure 6, where the small clause “the remainder renting ...” has no S node, giving a path feature from renting to the remainder of VBTVPTNPtNP. The value of the gov feature here is VP, as the algorithm finds the VP of the sentence’s main clause as it follows parent links up the tree. The feature is spurious in this case, because the main VP is not headed by, or relevant to, the target word renting. Systems based on the path and gov features are compared in Section 4.3. The differences between the two are relatively small for the purpose of identifying semantic roles when frame element boundaries are known. The path feature will, however, be important in identifying which constituents are frame elements for a given target word, as it gives us a way of navigating through the parse tree to find the frame elements in the sentence. 4.1.4 Position. To overcome errors due to incorrect parses, as well as to see how much can be done without parse trees, we introduced position as a feature. The position feature simply indicates whether the constituent to be labeled occurs before or after the predicate defining the semantic frame. We expected this feature to be highly correlated with grammatical function, since subjects will generally appear before a verb and objects after. Although we do not have hand-checked parses against which to measure the performance of the automatic parser on our corpus, the result that 13% of frame elements have no matching parse constituent gives a rough idea of the parser’s accuracy. Almost all of these cases in which no matching parse constituent was found are due to parser error. Other parser errors include cases in which a constituent is found, but with the incorrect label or internal structure. This result also considers only the individual constituent representing the frame element: the parse for the rest of the sentence may be incorrect, resulting in an incorrect value for the grammatical function features described in the previous two sections. Collins (1997) reports 88% labeled precision and recall on individual parse constituents on data from the Penn Treebank, roughly consistent with our finding of at least 13% error. 4.1.5 Voice. The distinction between active and passive verbs plays an important role in the connection between semantic role and grammatical function, since direct objects of active verbs often correspond in semantic role to subjects of passive verbs. From the parser output, verbs were classified as active or passive by building a set of 10 passive-identifying patterns. Each of the patterns requires both a passive auxiliary (some form of to be or to get) and a past participle. Roughly 5% of the examples were identified as passive uses. 4.1.6 Head Word. As previously noted, we expected lexical dependencies to be extremely important in labeling semantic roles, as indicated by their importance in related tasks such as parsing. Head words of noun phrases can be used to express selectional restrictions on the semantic types of role fillers. For example, in a communication frame, noun phrases headed by Bill, brother, or he are more likely to be the SpEAKER, whereas those headed by proposal, story, or question are more likely to be the Topic. (We did not attempt to resolve pronoun references.) Since the parser we used assigns each constituent a head word as an integral part of the parsing model, we were able to read the head words of the constituents from the parser output, employing the same set of rules for identifying the head child of each constituent in the parse tree. The rules for assigning a head word are listed in Collins (1999). Prepositions are considered to be the head words of prepositional phrases. The rules for assigning head words do not attempt to distinguish between cases in which the preposition expresses the semantic content of a role filler, such as PATH frame elements expressed by prepositional phrases headed by along, through, or in, and cases in which the preposition might be considered to be purely a case marker, as in most uses of of, where the semantic content of the role filler is expressed by the preposition’s object. Complementizers are considered to be heads, meaning that infinitive verb phrases are always headed by to and subordinate clauses such as in the sentence “I’m sure that he came” are headed by that. For our experiments, we divided the FrameNet corpus as follows: one-tenth of the annotated sentences for each target word were reserved as a test set, and another onetenth were set aside as a tuning set for developing our system. A few target words where fewer than 10 examples had been chosen for annotation were removed from the corpus. (Section 9 will discuss generalization to unseen predicates.) In our corpus, the average number of sentences per target word is only 34, and the number of sentences per frame is 732, both relatively small amounts of data on which to train frame element classifiers. To label the semantic role of a constituent automatically, we wish to estimate a probability distribution indicating how likely the constituent is to fill each possible Distributions calculated for semantic role identification: r indicates semantic role, pt phrase type, gov grammatical function, h head word, and t target word, or predicate. Distribution role, given the features described above and the predicate, or target word, t: P(r I h, pt, gov, position, voice, t) where r indicates semantic role, h head word, and pt phrase type. It would be possible to calculate this distribution directly from the training data by counting the number of times each role appears with a combination of features and dividing by the total number of times the combination of features appears: #(r, h, pt,gov, position, voice, t) P(r  |h, pt, gov, position, voice, t) = #(h, pt,gov, position, voice, t) In many cases, however, we will never have seen a particular combination of features in the training data, and in others we will have seen the combination only a small number of times, providing a poor estimate of the probability. The small number of training sentences for each target word and the large number of values that the head word feature in particular can take (any word in the language) contribute to the sparsity of the data. Although we expect our features to interact in various ways, we cannot train directly on the full feature set. For this reason, we built our classifier by combining probabilities from distributions conditioned on a variety of subsets of the features. Table 3 shows the probability distributions used in the final version of the system. Coverage indicates the percentage of the test data for which the conditioning event had been seen in training data. Accuracy is the proportion of covered test data for which the correct role is given the highest probability, and Performance, which is the product of coverage and accuracy, is the overall percentage of test data for which the correct role is predicted.3 Accuracy is somewhat similar to the familiar metric of precision in that it is calculated over cases for which a decision is made, and performance is similar to recall in that it is calculated over all true frame elements. Unlike in a traditional precision/recall trade-off, however, these results have no threshold to adjust, and the task is a multiway classification rather than a binary decision. The distributions calculated were simply the empirical distributions from the training data. That is, occurrences of each role and each set of conditioning events were counted in a table, and probabilities calculated by dividing the counts for each role by the total number Sample probabilities for P(r  |pt, gov, t) calculated from training data for the verb abduct. The variable gov is defined only for noun phrases. The roles defined for the removing frame in the motion domain are AGENT (AGT), THEME (THM), COTHEME (COTHM) (“... had been abducted with him”), and MANNER (MANR). of observations for each conditioning event. For example, the distribution P(r  |pt, t) was calculated as follows: Some sample probabilities calculated from the training are shown in Table 4. As can be seen from Table 3, there is a trade-off between more-specific distributions, which have high accuracy but low coverage, and less-specific distributions, which have low accuracy but high coverage. The lexical head word statistics, in particular, are valuable when data are available but are particularly sparse because of the large number of possible head words. To combine the strengths of the various distributions, we merged them in various ways to obtain an estimate of the full distribution P(r  |h, pt,gov, position, voice, t). The first combination method is linear interpolation, which simply averages the probabilities given by each of the distributions: where Ei λi = 1. The geometric mean, when expressed in the log domain, is similar: where Z is a normalizing constant ensuring that Er P(r  |constituent) = 1. Results for systems based on linear interpolation are shown in the first row of Table 5. These results were obtained using equal values of λ for each distribution defined for the relevant conditioning event (but excluding distributions for which the conditioning event was not seen in the training data). As a more sophisticated method of choosing interpolation weights, the expectation maximization (EM) algorithm was used to estimate the likelihood of the observed role’s being produced by each of the distributions in the general techniques of Jelinek and Mercer (1980). Because a number of the distributions used may have no training data for a given set of variables, the data were divided according to the set of distributions available, and a separate set of interpolation weights was trained for each set of distributions. This technique (line 2 of Table 5) did not outperform equal weights even on the data used to determine the weights. Although the EM algorithm is guaranteed to increase the likelihood of the training data, that likelihood does not always correspond to our scoring, which is based only on whether the correct outcome is assigned the highest probability. Results of the EM interpolation on held-out test data are shown in Table 6. Experimentation has shown that the weights used have relatively little impact in our interpolation scheme, no doubt because the evaluation metric depends only on the ranking of the probabilities and not on their exact values. Changing the interpolation weights rarely changes the probabilities of the roles enough to change their ranking. What matters most is whether a combination of variables has been seen in the training data or not. Results for the geometric mean are shown in row 3 of Table 5. As with linear interpolation, the exact weights were found to have little effect, and the results shown reflect equal weights. An area we have not explored is the use of the maximum-entropy techniques of, for example, Pietra, Pietra, and Lafferty (1997), to set weights for the log-linear model, either at the level of combining our probability distributions or at the level of calculating weights for individual values of the features. In the “backoff” combination method, a lattice was constructed over the distributions in Table 3 from more-specific conditioning events to less-specific, as shown in Figure 7. The lattice is used to select a subset of the available distributions to combine. The less-specific distributions were used only when no data were present for any more-specific distribution. Thus, the distributions selected are arranged in a cut across the lattice representing the most-specific distributions for which data are available. The selected probabilities were combined with both linear interpolation and a geometric mean, with results shown in Table 5. The final row of the table represents the baseline Lattice organization of the distributions from Table 3, with more-specific distributions toward the top. of always selecting the most common role of the target word for all its constituents, that is, using only P(r I t). Although this lattice is reminiscent of techniques of backing off to less specific distributions commonly used in n-gram language modeling, it differs in that we use the lattice only to select distributions for which the conditioning event has been seen in the training data. Discounting and deleted interpolation methods in language modeling typically are used to assign small, nonzero probability to a predicted variable unseen in the training data even when a specific conditioning event has been seen. In our case, we are perfectly willing to assign zero probability to a specific role (the predicted variable). We are interested only in finding the role with the highest probability, and a role given a small, nonzero probability by smoothing techniques will still not be chosen as the classifier’s output. The lattice presented in Figure 7 represents just one way of choosing subsets of features for our system. Designing a feature lattice can be thought of as choosing a set of feature subsets: once the probability distributions of the lattice have been chosen, the graph structure of the lattice is determined by the subsumption relations among the sets of conditioning variables. Given a set of N conditioning variables, there are 2N possible subsets, and 22N possible sets of subsets, giving us a doubly exponential number of possible lattices. The particular lattice of Figure 7 was chosen to represent some expected interaction between features. For example, we expect position and voice to interact, and they are always used together. We expect the head word h and the phrase type pt to be relatively independent predictors of the semantic role and therefore include them separately as roots of the backoff structure. Although we will not explore all the possibilities for our lattice, some of the feature interactions are examined more closely in Section 4.3. The final system performed at 80.4% accuracy, which can be compared to the 40.9% achieved by always choosing the most probable role for each target word, essentially chance performance on this task. Results for this system on test data, held out during development of the system, are shown in Table 6. Surprisingly, the EM-based interpolation performed better than the lattice-based system on the held-out test set, but not on the data used to set the weights in the EM-based system. We return to an analysis of which roles are hardest to classify in Section 9.1. Three of our features, position, gov, and path, attempt to capture the syntactic relation between the target word and the constituent to be labeled, and in particular to differentiate the subjects from objects of verbs. To compare these three features directly, experiments were performed using each feature alone in an otherwise identical sysLattice structures for comparing grammatical-function features. tem. Results are shown in Table 7. For the first set of experiments, corresponding to the first column of Table 7, no voice information was used, with the result that the remaining distributions formed the lattice of Figure 8a. (“GF” (grammatical function) in the figure represents one of the features position, gov, and path.) Adding voice information back into the system independently of the grammatical-function feature results Minimal lattice. in the lattice of Figure 8b, corresponding to the second column of Table 7. Choosing distributions such that the grammatical function and voice features are always used together results in Figure 8c, corresponding to the third column of Table 7. In each case, as in previous results, the grammatical function feature was used only when the candidate constituent was an NP. The last row of Table 7 shows results using no grammatical-function feature: the distributions making use of GF are removed from the lattices of Figure 8. As a guideline for interpreting these results, with 8,167 observations, the threshold for statistical significance with p < .05 is a 1.0% absolute difference in performance. It is interesting to note that looking at a constituent’s position relative to the target word performed as well as either of our features that read grammatical function off the parse tree, both with and without passive information. The gov and path features seem roughly equivalent in performance. Using head word, phrase type, and target word without either position or grammatical function yielded only 76.3% accuracy, indicating that although the two features accomplish a similar goal, it is important to include some measure of the constituent’s relationship to the target word, whether relative position or either of the syntactic features. Use of the active/passive voice feature seems to be beneficial only when the feature is tied to grammatical function: the second column in Table 7 shows no improvement over the first, while the right-hand column, where grammatical function and voice are tied, shows gains (although only trends) of at least 0.5% in all cases. As before, our three indicators of grammatical function seem roughly equivalent, with the best result in this case being the gov feature. The lattice of Figure 8c performs as well as our system of Figure 7, indicating that including both position and either of the syntactic relations is redundant. As an experiment to see how much can be accomplished with as simple a system as possible, we constructed the minimal lattice of Figure 9, which includes just two distributions, along with a prior for the target word to be used as a last resort when no data are available. This structure assumes that head word and grammatical function are independent. It further makes no use of the voice feature. We chose the path feature as the representation of grammatical function in this case. This system classified 76.3% of frame elements correctly, indicating that one can obtain roughly nine-tenths the performance of the full system with a simple approach. (We will return to a similar system for the purposes of cross-domain experiments in Section 9.) In this section we examine the system’s performance on the task of locating the frame elements in a sentence. Although our probability model considers the question of finding the boundaries of frame elements separately from the question of finding the correct label for a particular frame element, similar features are used to calculate both probabilities. In the experiments below, the system is no longer given frame element boundaries but is still given as inputs the human-annotated target word and the frame to which it belongs. We do not address the task of identifying which frames come into play in a sentence but envision that existing word sense disambiguation techniques could be applied to the task. As before, features are extracted from the sentence and its parse and are used to calculate probability tables, with the predicted variable in this case being fe, a binary indicator of whether a given constituent in the parse tree is or is not a frame element. The features used were the path feature of Section 4.1.3, the identity of the target word, and the identity of the constituent’s head word. The probability distributions calculated from the training data were P(fe  |path), P(fe  |path, t), and P(fe  |h, t), where fe indicates an event where the parse constituent in question is a frame element, path the path through the parse tree from the target word to the parse constituent, t the identity of the target word, and h the head word of the parse constituent. Some sample values from these distributions are shown in Table 8. For example, the path VBTVPtNP, which corresponds to the direct object of a verbal target word, had a high probability of being a frame element. The table also illustrates cases of sparse data for various feature combinations. By varying the probability threshold at which a decision is made, one can plot a precision/recall curve as shown in Figure 10. P(fe  |path, t) performs relatively poorly because of fragmentation of the training data (recall that only about 30 sentences are available for each target word). Although the lexical statistic P(fe  |h, t) alone is not useful as a classifier, using it in linear interpolation with the path statistics improves results. The curve labeled “interpolation” in Figure 10 reflects a linear interpolation of the form Note that this method can identify only those frame elements that have a corresponding constituent in the automatically generated parse tree. For this reason, it is interesting to calculate how many true frame elements overlap with the results of the system, relaxing the criterion that the boundaries must match exactly. Results for partial matching are shown in Table 9. Three types of overlap are possible: the identified constituent entirely within the true frame element, the true frame element entirely within the identified constituent, and each sequence partially contained by the other. An example of the first case is shown in Figure 11, where the true MESSAGE frame element is Mandarin by a head, but because of an error in the parser output, no constituent exactly matches the frame element’s boundaries. In this case, the system identifies Plot of precision/recall curve for various methods of identifying frame elements. Recall is calculated over only frame elements with matching parse constituents. Exactly matching boundaries 66% 5,421 Identified constituent entirely within true frame element 8 663 True frame element entirely within identified constituent 7 599 Both partially within the other 0 26 No overlap with any true frame element 13 972 two frame elements, indicated by shading, which together span the true frame element. When the automatically identified constituents were fed through the role-labeling system described above, 79.6% of the constituents that had been correctly identified in the first stage were assigned the correct role in the second, roughly equivalent to the performance when roles were assigned to constituents identified by hand. A more sophisticated integrated system for identifying and labeling frame elements is described in Section 7.1. As can be seen from Table 3, information about the head word of a constituent is valuable in predicting the constituent’s role. Of all the distributions presented, An example of overlap between identified frame elements and the true boundaries, caused by parser error. In this case two frame elements identified by the classifier (shaded subtrees) are entirely within the human annotation (indicated below the sentence), contributing two instances to row 2 of Table 9. P(r I h, pt, t) predicts the correct role most often (87.4% of the time) when training data for a particular head word have been seen. Because of the large vocabulary of possible head words, however, it also has the smallest coverage, meaning that it is likely that, for a given case in the test data, no frame element with the same head word will have been seen in the set of training sentences for the target word in question. To capitalize on the information provided by the head word, we wish to find a way to generalize from head words seen in the training data to other head words. In this section we compare three different approaches to the task of generalizing over head words: automatic clustering of a large vocabulary of head words to identify words with similar semantics; use of a hand-built ontological resource, WordNet, to organize head words in a semantic hierarchy; and bootstrapping to make use of unlabeled data in training the system. We will focus on frame elements filled by noun phrases, which constitute roughly half the total. To find groups of head words that are likely to fill the same semantic roles, an automatic clustering of nouns was performed using word co-occurrence data from a large corpus. This technique is based on the expectation that words with similar semantics will tend to co-occur with the same other sets of words. For example, nouns describing foods will tend to occur as direct objects of verbs such as eat devour, and savor. The clustering algorithm attempts to find such patterns of co-occurrence from the counts of grammatical relations between pairs of specific words in the corpus, without the use of any external knowledge or semantic representation. We extracted verb–direct object relations from an automatically parsed version of the British National Corpus, using the parser of Carroll and Rooth (1998).4 Clustering was performed using the probabilistic model of co-occurrence described in detail by Hofmann and Puzicha (1998). (For other natural language processing [NLP] applications of the probabilistic clustering algorithm, see, e.g., Rooth [1995], Rooth et al. [1999]; for application to language modeling, see Gildea and Hofmann [1999]. According to this model, the two observed variables, in this case the verb and the head noun of its object, can be considered independent given the value of a hidden cluster variable, c: One begins by setting a priori the number of values that c can take and using the EM algorithm to estimate the distributions P(c), P(n  |c), and P(v  |c). Deterministic annealing was used to prevent overfitting of the training data. We are interested only in the clusters of nouns given by the distribution P(n  |c): the verbs and the distribution P(v  |c) are thrown away once training is complete. Other grammatical relations besides direct object could be used, as could a set of relations. We used the direct object (following other clustering work such as Pereira, Tishby, and Lee [1993]) because it is particularly likely to exhibit semantically significant selectional restrictions. A total of 2,610,946 verb-object pairs were used as training data for the clustering, with a further 290,105 pairs used as a cross-validation set to control the parameters of the clustering algorithm. Direct objects were identified as noun phrases directly under a verb phrase node—not a perfect technique, since it also finds nominal adjuncts such as “I start today.” Forms of the verb to be were excluded from the data, as its cooccurrence patterns are not semantically informative. The number of values possible for the latent cluster variable was set to 256. (Comparable results were found with 64 clusters; the use of deterministic annealing prevents large numbers of clusters from resulting in overfitting.) The soft clustering of nouns thus generated is used as follows: for each example in the frame element–annotated training data, probabilities for values of the hidden cluster variable were calculated using Bayes’ rule: The clustering was applied only to noun phrase constituents; the distribution P(n  |c) from the clustering is used as a distribution P(h  |c) over noun head words. Using the cluster probabilities, a new estimate of P(r  |c, pt, t) is calculated for cases where pt, the phrase type or syntactic category of the constituent, is NP: � where j is an index ranging over the frame elements in the training set and their associated features pt, t, h and their semantic roles r. During testing, a smoothed estimate of the head word–based role probability is calculated by marginalizing over cluster values: As with the other methods of generalization described in this section, automatic clustering was applied only to noun phrases, which represent 50% of the constituents in the test data. We would not expect head word to be as valuable for other phrase types. The second most common category is prepositional phrases. The head of a prepositional phrase (PP) is considered to be the preposition, according to the rules we use, and because the set of prepositions is small, coverage is not as great a problem. Furthermore, the preposition is often a direct indicator of the semantic role. (A more complete model might distinguish between cases in which the preposition serves as a case or role marker and others in which it is semantically informative, with clustering performed on the preposition’s object in the former case. We did not attempt to make this distinction.) Phrase types other than NP and PP make up only a small proportion of the data. Table 10 shows results for the use of automatic clustering on constituents identified by the parser as noun phrases. As can be seen in the table, the vocabulary used for clustering includes almost all (97.9%) of the test data, and the decrease in accuracy from direct lexical statistics to clustered statistics is relatively small (from 87.0% to 79.7%). When combined with the full system described above, clustered statistics increase performance on NP constituents from 83.4% to 85.0% (statistically significant at p < .05). Over the entire test set, this translates into an improvement from 80.4% to 81.2%. The automatic clustering described above can be seen as an imperfect method of deriving semantic classes from the vocabulary, and we might expect a hand-developed set of classes to do better. We tested this hypothesis using WordNet (Fellbaum 1998), a freely available semantic hierarchy. The basic technique, when presented with a head word for which no training examples had been seen, was to ascend the type hierarchy until reaching a level for which training data are available. To do this, counts of training data were percolated up the semantic hierarchy in a technique similar to that of, for example, McCarthy (2000). For each training example, the count #(r, s, pt, t) was incremented in a table indexed by the semantic role r, WordNet sense s, phrase type pt, and target word t, for each WordNet sense s above the head word h in the hypernym hierarchy. In fact, the WordNet hierarchy is not a tree, but rather includes multiple inheritance. For example, person has as hypernyms both life form and causal agent. In such cases, we simply took the first hypernym listed, effectively converting the structure into a tree. A further complication is that several WordNet senses are possible for a given head word. We simply used the first sense listed for each word; a word sense disambiguation module capable of distinguishing WordNet senses might improve our results. As with the clustering experiments reported above, the WordNet hierarchy was used only for noun phrases. The WordNet hierarchy does not include pronouns; to increase coverage, the personal pronouns I, me, you, he, she, him, her, we, and us were added as hyponyms of person. Pronouns that refer to inanimate, or both animate and inanimate, objects were not included. In addition, the CELEX English lexical database (Baayen, Piepenbrock, and Gulikers 1995) was used to convert plural nouns to their singular forms. As shown in Table 11, accuracy for the WordNet technique is roughly the same as that in the automatic clustering results in Table 10: 84.3% on NPs, as opposed to 85.0% with automatic clustering. This indicates that the error introduced by the unsupervised clustering is roughly equivalent to the error caused by our arbitrary choice of the first WordNet sense for each word and the first hypernym for each WordNet sense. Coverage for the WordNet technique is lower, however, largely because of the absence of proper nouns from WordNet, as well as the absence of nonanimate pronouns (both personal pronouns such as it and they and indefinite pronouns such as something and anyone). A dictionary of proper nouns would likely help improve coverage, and a module for anaphora resolution might help cases with pronouns, with or without the use of WordNet. The conversion of plural forms to singular base forms was an important part of the success of the WordNet system, increasing coverage from 71.0% to 80.8%. Of the remaining 19.2% of all noun phrases not covered by the combination of lexical and WordNet sense statistics, 22% consisted of head words defined in WordNet, but for which no training data were available for any hypernym, and 78% consisted of head words not defined in WordNet. A third way of attempting to improve coverage of the lexical statistics is to “bootstrap,” or label unannotated data with the automatic system described in Sections 4 and 5 and use the (imperfect) result as further training data. This can be considered a variant of the EM algorithm, although we use the single most likely hypothesis for the unannotated data, rather than calculating the expectation over all hypotheses. Only one iteration of training on the unannotated data was performed. The unannotated data used consisted of 156,590 sentences containing the target words of our corpus, increasing the total amount of data available to roughly six times the 36,995 annotated training sentences. Table 12 shows results on noun phrases for the bootstrapping method. The accuracy of a system trained only on data from the automatic labeling (Panto) is 81.0%, reasonably close to the 87.0% for the system trained only on annotated data (Ptrai,,,). Combining the annotated and automatically labeled data increases coverage from 41.6% to 54.7% and performance to 44.5%. Because the automatically labeled data are not as accurate as the annotated data, we can do slightly better by using the automatic data only in cases where no training data are available, backing off to the distribution Panto from Ptrai,,,. The fourth row of Table 12 shows results with Panto incorporated into the backoff lattice of all the features of Figure 7, which actually resulted in a slight decrease in performance from the system without the bootstrapped data, shown in the third row. This is presumably because, although the system trained on automatically labeled data performed with reasonable accuracy, many of the cases it classifies correctly overlap with the training data. In fact our backing-off estimate of P(r h, pt, t) classifies correctly only 66% of the additional cases that it covers over Ptrain(r h, pt, t). The three methods of generalizing lexical statistics each had roughly equivalent accuracy on cases for which they were able to derive an estimate of the role probabilities for unseen head words. The differences between the three were primarily due to how much they could improve the coverage of the estimator, that is, how many new noun heads they were able to handle. The automatic-clustering method performed by far the best on this metric; only 2.1% of test cases were unseen in the data used for the automatic clustering. This indicates how much can be achieved with unsupervised methods given very large training corpora. The bootstrapping technique described here, although it has a similar unsupervised flavor, made use of much less data than the corpus used for noun clustering. Unlike probabilistic clustering, the bootstrapping technique can make use of only those sentences containing the target words in question. The WordNet experiment, on the other hand, indicates both the usefulness of hand-built resources when they apply and the difficulty of attaining broad coverage with such resources. Combining the three systems described would indicate whether their gains are complementary or overlapping. One of the primary difficulties in labeling semantic roles is that one predicate may be used with different argument structures: for example, in the sentences “He opened the door” and “The door opened,” the verb open assigns different semantic roles to its syntactic subject. In this section we compare two strategies for handling this type of alternation in our system: a sentence-level feature for frame element groups and a subcategorization feature for the syntactic uses of verbs. Then a simple system using the predicate’s argument structure, or syntactic signature, as the primary feature will be contrasted with previous systems based on local, independent features. The system described in previous sections for classifying frame elements makes an important simplifying assumption: it classifies each frame element independent of the decisions made for the other frame elements in the sentence. In this section we remove this assumption and present a system that can make use of the information that, for example, a given target word requires that one role always be present or that having two instances of the same role is extremely unlikely. To capture this information, we introduce the notion of a frame element group, which is the set of frame element roles present in a particular sentence (technically a multiset, as duplicates are possible, though quite rare). Frame element groups (FEGs) are unordered: examples are shown in Table 13. Sample probabilities from the training data for the frame element groups of the target word blame are shown in Table 14. The FrameNet corpus recognizes three types of “null-instantiated” frame elements (Fillmore 1986), which are implied but do not appear in the sentence. An example of null instantiation is the sentence “Have you eaten?” where food is understood. We did not attempt to identify such null elements, and any null-instantiated roles are not included in the sentence’s FEG. This increases the variability of observed FEGs, as a predicate may require a certain role but allow it to be null instantiated. Our system for choosing the most likely overall assignment of roles for all the frame elements of a sentence uses an approximation that we derive beginning with the true probability of the optimal role assignment r*: where P(r1...n I t,f1...n) represents the probability of an overall assignment of roles ri to each of the n constituents of a sentence, given the target word t and the various features fi of each of the constituents. In the first step we apply Bayes’ rule to this and in the second we make the assumption that the features of the various constituents of a sentence are independent given the target word and each constituent’s role and discard the term P(f1...n  |t), which is constant with respect to r: We estimate the prior over frame element assignments as the probability of the frame element groups, represented with the set operator {}: and finally discard the feature prior P(fi  |t) as being constant over the argmax expression: This leaves us with an expression in terms of the prior for frame element groups of a particular target word P({r1...n}  |t), the local probability of a frame element given a constituent’s features P(ri  |fi, t) on which our previous system was based, and the individual priors for the frame elements chosen P(ri  |t). This formulation can be used to assign roles either when the frame element boundaries are known or when they are not, as we will discuss later in this section. Calculating empirical FEG priors from the training data is relatively straightforward, but the sparseness of the data presents a problem. In fact, 15% of the test sentences had an FEG not seen in the training data for the target word in question. Using the empirical value for the FEG prior, these sentences could never be correctly classified. For this reason, we introduce a smoothed estimate of the FEG prior consisting of a linear interpolation of the empirical FEG prior and the product, for each possible frame element, of the probability of being present or not present in a sentence given the target word: The value of A was empirically set to maximize performance on the development set; a value of 0.6 yielded performance of 81.6%, a significant improvement over the 80.4% of the baseline system. Results were relatively insensitive to the exact value of A. Up to this point, we have considered separately the problems of labeling roles given that we know where the boundaries of the frame elements lie (Section 4, as well as Section 6) and finding the constituents to label in the sentence (Section 5). We now turn to combining the two systems described above into a complete role labeling system. We use equation (16), repeated below, to estimate the probability that a constituent is a frame element: where p is the path through the parse tree from the target word to the constituent, t is the target word, and h is the constituent’s head word. The first two rows of Table 15 show the results when constituents are determined to be frame elements by setting the threshold on the probability P(fe  |p, h, t) to 0.5 and then running the labeling system of Section 4 on the resulting set of constituents. The first two columns of results show precision and recall for the task of identifying frame element boundaries correctly. The second pair of columns gives precision and recall for the combined task of boundary identification and role labeling; to be counted as correct, the frame element must both have the correct boundary and be labeled with the correct role. Contrary to our results using human-annotated boundaries, incorporating FEG priors into the system based on automatically identified boundaries had a negative effect on labeled precision and recall. No doubt this is due to introducing a dependency on other frame element decisions that may be incorrect: the use of FEG priors causes errors in boundary identification to be compounded. One way around this problem is to integrate boundary identification with role labeling, allowing the FEG priors and the role-labeling decisions to affect which constituents are frame elements. This was accomplished by extending the formulation where fei is a binary variable indicating that a constituent is a frame element and P(fei  |fi) is calculated as above. When fei is true, role probabilities are calculated as before; when fei is false, ri assumes an empty role with probability one and is not included in the FEG represented by fr1...nj. One caveat in using this integrated approach is its exponential complexity: each combination of role assignments to constituents is considered, and the number of combinations is exponential in the number of constituents. Although this did not pose a problem when only the annotated frame elements were under consideration, now we Two subcategorizations for the target word open. The relevant production in the parse tree is highlighted. On the left, the value of the feature is “VP → VB NP”; on the right it is “VP → VB.” must include every parse constituent with a nonzero probability for P(fei  |fi). To make the computation tractable, we implement a pruning scheme: hypotheses are extended by choosing assignments for one constituent at a time, and only the top m hypotheses are retained for extension by assignments to the next constituent. Here we set m = 10 after experimentation showed that increasing m yielded no significant improvement. Results for the integrated approach are shown in the last row of Table 15. Allowing role assignments to influence boundary identification improves results both on the unlabeled boundary identification task and on the combined identification and labeling task. The integrated approach puts us in a different portion of the precision/recall curve from the results in the first two rows, as it returns a higher number of frame elements (7,736 vs. 5,719). A more direct comparison can be made by lowering the probability threshold for frame element identification from 0.5 to 0.35 to force the nonintegrated system to return the same number of frame elements as the integrated system. This yields a frame element identification precision of 71.3% and recall of 67.6% and a labeled precision of 60.8% and recall of 57.6%, which is dominated by the result for the integrated system. The integrated system does not have a probability threshold to set; nonetheless it comes closer to identifying the correct number of frame elements (8,167) than does the independent boundary identifier when the theoretically optimal threshold of 0.5 is used with the latter. Recall that use of the FEG prior was motivated by the tendency of verbs to assign differing roles to the same syntactic position. For example, the verb open assigns different roles to the syntactic subject in He opened the door and The door opened. In this section we consider a different feature motivated by these problems: the syntactic subcategorization of the verb. For example, the verb open seems to be more likely to assign the role PATIENT to its subject in an intransitive context and AGENT to its subject in a transitive context. Our use of a subcategorization feature was intended to differentiate between transitive and intransitive uses of a verb. The feature used was the identity of the phrase structure rule expanding the target word’s parent node in the parse tree, as shown in Figure 12. For example, for He closed the door, with close as the target word, the subcategorization feature would be “VP → VB NP.” The subcategorization feature was used only when the target word was a verb. The various part-of-speech tags for verb forms (VBD for past-tense verb forms, VBZ for third-person singular present tense, VBP for other present tense, VBG for present participles, and VBN for past participles) were collapsed into a single tag VB. It is important to note that we are not able to distinguish complements from adjuncts, and our subcategorization feature could be sabotaged by cases such as The door closed yesterday. In the Penn Treebank style, yesterday is considered an NP with tree structure equivalent to that of a direct object. Our subcategorization feature is fairly specific: for example, the addition of an ADVP to a verb phrase will result in a different value. We tested variations of the feature that counted the number of NPs in a VP or the total number of children of the VP, with no significant change in results. The subcategorization feature was used in conjunction with the path feature, which represents the sequence of nonterminals along the path through the parse tree from the target word to the constituent representing a frame element. Making use of the new subcategorization (subcat) feature by adding the distribution P(r  |subcat, path, t) to the lattice of distributions in the baseline system resulted in a slight improvement to 80.8% performance from 80.4%. As with the gov feature in the baseline system, it was found beneficial to use the subcat feature only for NP constituents. Combining the FEG priors and subcategorization feature into a single system resulted in performance of 81.6%, no improvement over using FEG priors without subcategorization. We suspect that the two seemingly different approaches in fact provide similar information. For example, in our hypothetical example of the sentence He opened the door vs. the sentence The door opened, the verb open would have high priors for the FEGs {AGENT, THEME} and {THEME}, but a low prior for {AGENT}. In sentences with only one candidate frame element (the subject in The door closed), the use of the FEG prior will cause it to be labeled THEME, even when the feature probabilities prefer labeling a subject as AGENT. Thus the FEG prior, by representing the set of arguments the predicate is likely to take, essentially already performs the function of the subcategorization feature. The FEG prior allows us to introduce a dependency between the classifications of the sentence’s various constituents with a single parameter. Thus, it can handle the alternation of our example without, for example, introducing the role chosen for one constituent as an additional feature in the probability distribution for the next constituent’s role. It appears that because introducing additional features can further fragment our already sparse data, it is preferable to have a single parameter for the FEG prior. An interesting result reinforcing this conclusion is that some of the argumentstructure features that aided the system when individual frame elements were considered independently are unnecessary when using FEG priors. Removing the features passive and position from the system and using a smaller lattice of only the distributions not employing these features yields an improved performance of 82.8% on the role-labeling task using hand-annotated boundaries. We believe that, because these features pertain to syntactic alternations in how arguments are realized, they overlap with the function of the FEG prior. Adding unnecessary features to the system can reduce performance by fragmenting the training data. In the experiments reported in previous sections, we have used the parse tree returned by a statistical parser as input to the role-labeling system. In this section, we explore the interaction between semantic roles and syntactic parsing by integrating the parser with the semantic-role probability model. This allows the semantic-role assignment to affect the syntactic attachment decisions made by the parser, with the hope of improving the accuracy of the complete system. Although most statistical parsing work measures performance in terms of syntactic trees without semantic information, an assignment of role fillers has been incorporated into a statistical parsing model by Miller et al. (2000) for the domain-specific templates of the Message Understanding Conference (Defense Advanced Research Projects Agency 1998) task. A key finding of Miller et al.’s work was that a system developed by annotating role fillers in text and training a statistical system performed at the same level as one based on writing a large system of rules, which requires much more highly skilled labor to design. We use as the baseline of all our parsing experiments the model described in Collins (1999). The algorithm is a form of chart parsing, which uses dynamic programming to search through the exponential number of possible parses by considering subtrees for each subsequence of the sentence independently. To apply chart parsing to a probabilistic grammar, independence relations must be assumed to hold between the probabilities of a parse tree and the internal structure of its subtrees. In the case of stochastic context-free grammar, the probability of a tree is independent of the internal structure of its subtrees, given the topmost nonterminal of the subtree. The chart-parsing algorithm can simply find the highest-probability parse for each nonterminal for each substring of the input sentence. No lower-probability subtrees will ever be used in a complete parse, and they can be thrown away. Recent lexicalized stochastic parsers such as Collins (1999), Charniak (1997), and others add additional features to each constituent, the most important being the head word of the parse constituent. The statistical system for assigning semantic roles described in the previous sections does not fit easily into the chart-parsing framework, as it relies on long-distance dependencies between the target word and its frame elements. In particular, the path feature, which is used to “navigate” through the sentence from the target word to its likely frame elements, may be an arbitrarily long sequence of syntactic constituents. A path feature looking for frame elements for a target word in another part of the sentence may examine the internal structure of a constituent, violating the independence assumptions of the chart parser. The use of priors over FEGs further complicates matters by introducing sentence-level features dependent on the entire parse. For these reasons, we use the syntactic parsing model without frame element probabilities to generate a number of candidate parses, compute the best frame element assignment for each, and then choose the analysis with the highest overall probability. The frame element assignments are computed as in Section 7.1, with frame element probabilities being applied to every constituent in the parse. To return a large number of candidate parses, the parser was modified to include constituents in the chart even when they were equivalent, according to the parsing model, to a higher-probability constituent. Rather than choosing a fixed n and keeping the n best constituents for each entry in the chart, we chose a probability threshold and kept all constituents within a margin of the highest-probability constituent. Thus the mechanism is similar to the beam search used to prune nonequivalent edges, but a lower threshold was used for equivalent edges (1e vs. 1 100). Using these pruning parameters, an average of 14.9 parses per sentence were obtained. After rescoring with frame element probabilities, 18% of the sentences were assigned a parse different from the original best parse. Nevertheless, the impact on identification of frame elements was small; results are shown in Table 16. The results show a slight, but not statistically significant, increase in recall of frame elements. One possible reason that the improvement is not greater is the relatively small number of parses per sentence available for rescoring. Unfortunately, the parsing algorithm used to generate n-best parses is inefficient, and generating large numbers of parses seems to be computationally intractable. In theory, the complexity of n-best variations of the Viterbi chart-parsing algorithm is quadratic in n. One can simply expand the dynamic programming chart to have n slots for the best solutions to each subproblem, rather than one. As our grammar forms new constituents from pairs of smaller constituents (that is, it internally uses a binarized grammar), for each pair of constituents considered in a single-best parser, up to n2 pairs would be present in the n-best variant. The beam search used by modern parsers, however, makes the analysis more complex. Lexicalization of parse constituents dramatically increases the number of categories that must be stored in the chart, and efficient parsing requires that constituents below a particular probability threshold be dropped from further consideration. In practice, returning a larger number of parses with our algorithm seems to require increasing the pruning beam size to a degree that makes run times prohibitive. In addition to the robustness of even relatively simple parsing models, one explanation for the modest improvement may be the fact that even our integrated system includes semantic information for only one word in the sentence. As the coverage of our frame descriptions increases, it may be possible to do better and to model the interactions between the frames invoked by a text. Most of the statistics used in the system as described above are conditioned on the target word, or predicate, for which semantic roles are being identified. This limits the applicability of the system to words for which training data are available. In Section 6, we attempted to generalize across fillers for the roles of a single predicate. In this section, we turn to the related but somewhat more difficult question of generalizing from seen to unseen predicates. Many ways of attempting this generalization are possible, but the simplest is provided by the frame-semantic information of the FrameNet database. We can use data from target words in the same frame to predict behavior for an unseen word, or, if no data are available for the frame in question, we can use data from the same broad semantic domain into which the frames are grouped. To investigate the degree to which our system is dependent on the set of semantic roles used, we performed experiments using abstract, general semantic roles such as AGENT, PATIENT, and GOAL. Such roles were proposed in theories of linking such as Fillmore (1968) and Jackendoff (1972) to explain the syntactic realization of semantic arguments. This level of roles, often called thematic roles, was seen as useful for expressing generalizations such as “If a sentence has an AGENT, the AGENT will occupy the subject position.” Such correlations might enable a statistical system to generalize from one semantic domain to another. Recent work on linguistic theories of linking has attempted to explain syntactic realization in terms of the fundamentals of verbs’ meaning (see Levin and Rappaport Hovav [1996] for a survey of a number of theories). Although such an explanation is desirable, our goal is more modest: an automatic procedure for identifying semantic roles in text. We aim to use abstract roles as a means of generalizing from limited training data in various semantic domains. We see this effort as consistent with various theoretical accounts of the underlying mechanisms of argument linking, since the various theories all postulate some sort of generalization between the roles of specific predicates. To this end, we developed a correspondence from frame-specific roles to a set of abstract thematic roles. For each frame, an abstract thematic role was assigned to each frame element in the frame’s definition. Since there is no canonical set of abstract semantic roles, we decided upon the list shown in Table 17. We are interested in adjuncts as well as arguments, leading to roles such as DEGREE not found in many theories of verb-argument linking. The difficulty of fitting many relations into standard categories such as AGENT and PATIENT led us to include other roles such as TOPIC. In all, we used 18 roles, a somewhat richer set than is often used, but still much more restricted than the frame-specific roles. Even with this enriched set, not all framespecific roles fit neatly into one category. An experiment was performed replacing each role tag in the training and test data with the corresponding thematic role and training the system as described above on the new dataset. Results were roughly comparable for the two types of semantic roles: overall performance was 82.1% for thematic roles, compared to 80.4% for framespecific roles. This reflects the fact that most frames had a one-to-one mapping from frame-specific to abstract roles, so the tasks were largely equivalent. We expect abstract roles to be most useful when one is generalizing to predicates and frames not found in the training data, the topic of the following sections. One interesting consequence of using abstract roles is that they allow us to compare more easily the system’s performance on different roles because of the smaller number of categories. This breakdown is shown in Table 18. Results are given for two systems: the first assumes that the frame element boundaries are known and the second finds them automatically. The second system, which is described in Section 7.1, corresponds to the rightmost two columns in Table 18. The “Labeled Recall” column shows how often the frame element is correctly identified, whereas the “Unlabeled Recall” column shows how often a constituent with the given role is correctly identified as being a frame element, even if it is labeled with the wrong role. EXPERIENCER and AGENT, two similar roles generally found as the subject for complementary sets of verbs, are the roles that are correctly identified the most often. The “Unlabeled Recall” column shows that these roles are easy to find in the sentence, as a predicate’s subject is almost always a frame element, and the “Known Boundaries” column shows that they are also not often confused with other roles when it is known that they are frame elements. The two most difficult roles in terms of unlabeled recall, MANNER and DEGREE, are typically realized by adverbs or prepositional phrases and considered adjuncts. It is interesting to note that these are considered in FrameNet to be general frame elements that can be used in any frame. STATE Rex spied out Sam Maggott hollering at all and sundry and making good use of his over-sized red gingham handkerchief. Topic He said, “We would urge people to be aware and be alert with fireworks because your fun might be someone else’s tragedy.” This section has shown that our system can use roles defined at a more abstract level than the corpus’s frame-level roles and in fact that when we are looking at a single predicate, the choice has little effect. In the following sections, we attempt to use the abstract roles to generalize the behavior of semantically related predicates. We will present results at different, successively broader levels of generalization, making use of the categorization of FrameNet predicates into frames and more general semantic domains. We first turn to using data from the appropriate frame when no data for the target word are available. Table 19 shows results for various probability distributions using a division of training and test data constructed such that no target words are in common. Every tenth target word was included in the test set. The amount of training data available for each frame varied, from just one target word in some cases to 167 target words in the “perception/noise” frame. The training set contained a total of 75,919 frame elements and the test set 7,801 frame elements. Performance broken down by abstract role. The third column represents accuracy when frame element boundaries are given to the system, and the fourth and fifth columns reflect finding the boundaries automatically. Unlabeled recall includes cases that were identified as a frame element but given the wrong role. The results show a familiar trade-off between coverage and accuracy. Conditioning both the head word and path features on the frame reduces coverage but improves accuracy. A linear interpolation, λ1P(r I path, f ) + λ2P(r I h,f) + λ3P(r I pt, position, voice,f) achieved 79.4% performance on the test set, significantly better than any of the individual distributions and approaching the result of 82.1% for the original system, using target-specific statistics and thematic roles. This result indicates that predicates in the same frame behave similarly in terms of their argument structure, a finding generally consistent with theories of linking that claim that the syntactic realization of verb arguments can be predicted from their semantics. We would expect verbs in the same frame to be semantically similar and to have the same patterns of argument structure. The relatively high performance of frame-level statistics indicates that the Minimal lattice for cross-frame generalization. frames defined by FrameNet are fine-grained enough to capture the relevant semantic similarities. This result is encouraging in that it indicates that a relatively small amount of data can be annotated for a few words in a semantic frame and used to train a system that can then bootstrap to a larger number of predicates. More difficult than the question of unseen predicates in a known frame are frames for which no training data are present. The 67 frames in the current data set cover only a fraction of the English language, and the high cost of annotation makes it difficult to expand the data set to cover all semantic domains. The FrameNet project is defining additional frames and annotating data to expand the scope of the database. The question of how many frames exist, however, remains unanswered for the time being; a full account of frame semantics is expected to include multiple frames being invoked by many words, as well as an inheritance hierarchy of frames and a more detailed representation of each frame’s meaning. In this section, we examine the FrameNet data by holding out an entire frame for testing and using other frames from the same general semantic domain for training. Recall from Figure 1 that domains like COMMUNICATION include frames like CONVERSATION, QUESTIONING, and STATEMENT. Because of the variation in difficulty between different frames and the dependence of the results on which frames are held out for testing, we used a jackknifing methodology. Each frame was used in turn as test data, with all other frames used as training data. The results in Table 20 show average results over the entire data set. Combining the distributions gives a system based on the (very restricted) backoff lattice of Figure 13. This system achieves performance of 51.0%, compared to 82.1% for the original system and 79.4% for the within-frame generalization task. The results show that generalizing across frames, even within a domain, is more difficult than generalizing across target words within a frame. There are several factors that may account for this: the FrameNet domains were intended primarily as a way of organizing the project, and their semantics have not been formalized. Thus, it may not be surprising that they do not correspond to significant generalizations about argument structure. The domains are fairly broad, as indicated by the fact that always choosing the most common role for a given domain (the baseline for cross-frame, within-domain generalization, given as P(r  |d) in Table 20, classifies 28.4% of frame elements correctly) does not do better than the cross-domain baseline of always choosing the most common role from the entire database regardless of domain (P(r) in Table 20, which yields 28.7% correct). This contrasts with a 40.9% baseline for P(r  |t), that is, always choosing the most common role for a particular target word (Table 5, last line). Domain information does not seem to help a great deal, given no information about the frame. Furthermore, the cross-frame experiments here are dependent on the mapping of frame-level roles to abstract thematic roles. This mapping was done at the frame level; that is, FrameNet roles with the same label in two different frames may be translated into two different thematic roles, but all target words in the same frame make use of the same mapping. The mapping of roles within a frame is generally one to one, and therefore the choice of mapping has little effect when using statistics conditioned on the target word and on the frame, as in the previous section. When we are attempting to generalize between frames, the mapping determines which roles from the training frame are used to calculate probabilities for the roles in the test frames, and the choice of mapping is much more significant. The mapping used is necessarily somewhat arbitrary. It is interesting to note that the path feature performs better when not conditioned on the domain. The head word, however, seems to be more domain-specific: although coverage declines when the context is restricted to the semantic domain, accuracy improves. This seems to indicate that the identity of certain role fillers is domainspecific, but that the syntax/semantics correspondence captured by the path feature is more general, as predicted by theories of syntactic linking. As general as they are, the semantic domains of the current FrameNet database cover only a small portion of the language. The domains are defined at the level of, for example, COMMUNICATION and EMOTION; a list of the 12 domains in our corpus is given in Table 1. Whether generalization is possible across domains is an important question for a general language-understanding system. For these experiments, a jackknifing protocol similar to that of the previous section was used, this time holding out one entire domain at a time and using all the others as training material. Results for the path and head word feature are shown in Table 21. The distributions P(r  |path), P(r  |h), and P(r) of Table 21 also appeared in Table 20; the difference between the experiments is only in the division of training and test sets. A linear interpolation, λ1P(r  |path) + λ2P(r  |h), classifies 39.8% of frame elements correctly. This is no better than our result of 40.9% (Table 3) for always choosing a predicate’s most frequent role; however, the cross-domain system does not have role frequencies for the test predicates. As one might expect, as we make successively broader generalizations to semantically more distant predicates, performance degrades. Our results indicate that frame semantics give us a level at which generalizations relevant to argument linking can be made. Our results for unseen predicates within the same frame are encouraging, indicating that the predicates are semantically similar in ways that result in similar argument structure, as the semantically based theories of linking advocated by Levin (1993) and Levin and Rappaport Hovav (1996) would predict. We hope that corpus-based systems such as ours can provide a way of testing and elaborating such theories in the future. We believe that some level of skeletal representation of the relevant aspects of a word’s meaning, along the lines of Kipper et al. (2000) and of the frame hierarchy being developed by the FrameNet project, could be used in the future to help a statistical system generalize from similar words for which training data are available. Our system is able to label semantic roles automatically with fairly high accuracy, indicating promise for applications in various natural language tasks. Semantic roles do not seem to be simple functions of a sentence’s syntactic tree structure, and lexical statistics were found to be extremely valuable, as has been the case in other natural language processing applications. Although lexical statistics are quite accurate on the data covered by observations in the training set, the sparsity of their coverage led us to introduce semantically motivated knowledge sources, which in turn allowed us to compare automatically derived and hand-built semantic resources. Various methods of extending the coverage of lexical statistics indicated that the broader coverage of automatic clustering outweighed its imprecision. Carefully choosing sentence-level features for representing alternations in verb argument structure allowed us to introduce dependencies between frame element decisions within a sentence without adding too much complexity to the system. Integrating semantic interpretation and syntactic parsing yielded only the slightest gain, showing that although probabilistic models allow easy integration of modules, the gain over an unintegrated system may not be large because of the robustness of even simple probabilistic systems. Many aspects of our system are still quite preliminary. For example, our system currently assumes knowledge of the correct frame type for the target word to determine the semantic roles of its arguments. A more complete semantic analysis system would thus require a module for frame disambiguation. It is not clear how difficult this problem is and how much it overlaps with the general problem of word-sense disambiguation. Much else remains to be done to apply the system described here to the interpretation of general text. One technique for dealing with the sparseness of lexical statistics would be the combination of FrameNet data with named-entity systems for recognizing times, dates, and locations, the effort that has gone into recognizing these items, typically used as adjuncts, should complement the FrameNet data, which is more focused on arguments. Generalization to predicates for which no annotated data are available may be possible using other lexical resources or automatic clustering of predicates. Automatically learning generalizations about the semantics and syntactic behavior of predicates is an exciting problem for the years to come. Penn Treebank constituent (or nonterminal) labels. We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus. This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.
Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar. Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies. The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations. CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames. As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition. This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)). Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers. Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels. According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie. leftor right, which is implicit in CCG categories) de fines a hP;H;Si dependency between the head word of S, wS, and the head word of H , wH. This measureis neutral with respect to the branching factor. Fur thermore, as noted by Hockenmaier (2001), it doesnot penalize equivalent analyses of multiple modi Computational Linguistics (ACL), Philadelphia, July 2002, pp. 335-342. Proceedings of the 40th Annual Meeting of the Association for Pierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29 N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N > > > > > N N N N (SnNP)n(SnNP) > NP NP NP NP < > > NP S[adj]nNP (S[b]nNP)=PP PP > NPnNP S[b]nNP < < NP S[b]nNP > NP S[dcl]nNP < S[dcl] Figure 1: A CCG derivation in our corpus fiers. In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen dency), scores can be compared across grammars with different sets of labels and different kinds of trees. In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser. For further discussion we refer the reader to Clark and Hockenmaier (2002) . CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?) arecovered by the translation procedure, which pro cesses 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23). The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992). Figure1 shows a derivation taken from CCGbank. Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames. The complement-adjunct distinction is made explicit; for instance as a nonexecutive director is marked up as PP-CLR in the Tree bank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier. The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank.The verbal categories in CCGbank carry features distinguishing declarative verbs (and auxiliaries) from past participles in past tense, past par ticiples for passive, bare infinitives and ing-forms. There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement. The derivations in CCGbank are ?normal-form? in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary. Expansion HeadCat NonHeadCat P(exp j : : : ) P(H j : : : ) P(S j : : : ) Baseline P P;exp P;exp;H + Conj P;con jP P;exp;con jP P;exp;H ;con jP + Grandparent P;GP P;GP;exp P;GP;exp;H + ? P#?L;RP P;exp#?L;RP P;exp;H#?L;RP Table 1: The unlexicalized models The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process. This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1. Given a (parent) node with category P, choose the expansion exp of P, where exp can beleaf (for lexical categories), unary (for unary ex pansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right). If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability). All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus. We replace all rare words in the training data with their POS-tag. For all experiments reported here and in section 5, the frequency threshold was set to 5. LikeCollins (1999), we assume that the test data is POS tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap propriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words. The performance of the baseline model is shown in the top row of table 3. For six out of the 2379 sentences in our test corpus we do not get a parse.1The reason is that a lexicon consisting of the word category pairs observed in the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but imperfect, solution to this problem in section 7. State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes. We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i
Corpus Statistics Meet The Noun Compound: Some Empirical Results Tagged Dependency -.— Tagged Adjacency -e— L. Pattern 3 5 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results. Three training schemes have been used and the tuned analysis procedures applied to the test set. Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines. If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy. However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%. 4 Conclusion The experiments above demonstrate a number of important points. The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns. At the very least, this information can be applied in broad coverage parsing to assist in the control of search. I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern. While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed. In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature. This result is in accordance with the informal reasoning given in section 1.3. The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets. In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching. Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing. 5 Acknowledgements This work has received valuable input from people too numerous to mention. The most significant contributions have been made by Richard Buckland, Robert Dale and Mark Dras. I am also indebted to Vance Gledhill, Mike Johnson, Philip Resnik, Richard Sproat, Wilco ter Stal, Lucy Vanderwende and Wobcke. Financial support is gratefully ack- 53 nowledged from the Microsoft Institute and the Australian Government. If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparck Jones, 1983). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984). While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens et al, 1987; Vanderwende, 1993 and Sproat, 1994). The task is illustrated in example 1: The parses assigned to these two compounds differ, even though the sequence of parts of speech are identical. The problem is analogous to the prepositional phrase attachment task explored in Hindle and Rooth (1993). The approach they propose involves computing lexical associations from a corpus and using these to select the correct parse. A similar architecture may be applied to noun compounds. In the experiments below the accuracy of such a system is measured. Comparisons are made across five dimensions: While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds. Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. While such patterns produce false training examples, the resulting noise often only introduces minor distortions. A more I.beral alternative is the use of a cooccurrence window. Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation. Similarly, Smadja (1993) uses a six content word window to extract significant collocations. A range of windowed training schemes are employed below. Importantly, the use of a window provides a natural means of trading off the amount of data against its quality. When data sparseness undermines the system accuracy, a wider window may admit a sufficient volume of extra accurate data to outweigh the additional noise. There are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds. Only two of these have been subjected to evaluation, and in each case, no comparison to any of the other three was performed. In fact all authors appear unaware of the other three proposals. I will therefore briefly describe these algorithms. Three of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253). Therein, the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable. It is reproduced here for reference: Given three nouns n1, n2 and n3: Only more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model. The simplest of these is reported in Pustejovsky et al (1993). Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents. Whichever is found is then chosen as the more closely bracketed pair. For example, when backup compiler disk is encountered, the analysis will be: The proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound. Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest. Again, there is no evaluation of the method other than a demonstration that four examples work correctly. The third proposal based on the adjacency model appears in Resnik (1993) and is rather more complex again. The SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate. The association between each pair of words in the compound is then computed by taking the maximum selectional association from all possible ways of regarding the pair as predicate and argument. Whilst this association metric is complicated, the decision procedure still follows the outline devised by Marcus (1980) above. Resnik (1993) used unambiguous noun compounds from the parsed Wall Street Journal (WSJ) corpus to estimate the association N alues and analysed a test set of around 160 compounds. After some tuning, the accuracy was about 73%, as compared with a baseline of 64% achieved by always bracketing the first two nouns together. The fourth algorithm, first described in Lauer (1994), differs in one striking manner from the other three. It uses what I will call the DEPENDENCY MODEL. This model utilises the following procedure when given three nouns ni, n2 and n3: Figure 1 shows a graphical comparison of the two analysis models. In Lauer (1994), the degree of acceptability is again provided by statistical measures over a corpus. The metric used is a mutual information-like measure based on probabilities of modification relationships. This is derived from the idea that parse trees capture the structure of semantic relationships within a noun compound.' The dependency model attempts to choose a parse which makes the resulting relationships as acceptable as possible. For example, when backup compiler disk is encountered, the analysis will be: I claim that the dependency model makes more intuitive sense for the following reason. Consider the compound calcium ion exchange, which is typically left-branching (that is, the first two words are bracketed together). There does not seem to be any reason why calcium ion should be any more frequent than ion exchange. Both are plausible compounds and regardless of the bracketing, ions are the object of an exchange. Instead, the correct parse depends on whether calcium characterises the ions or mediates the exchange. Another significant difference between the models is the predictions they make about the proportion 'Lauer and Dras (1994) give a formal construction motivating the algorithm given in Lauer (1994). of left and right-branching compounds. Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time). In the test set used here and in that of Resnik (1993), the proportion of leftbranching compounds is 67% and 64% respectively. In contrast, the adjacency model appears to predict a proportion of 50%. The dependency model has also been proposed by Kobayasi et al (1994) for analysing Japanese noun compounds, apparently independently. Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words). A simple calculation shows that using their own preprocessing hueristics to guess a bracketing provides a higher accuracy on their test set than their statistical model does. This renders their experiment inconclusive. A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier's encyclopedia corpus in the following way.2 Because the corpus is not tagged or parsed, a somewhat conservative strategy of looking for unambiguous sequences of nouns was used. To distinguish nouns from other words, the University of Pennsylvania morphological analyser (described in Karp et al, 1992) was used to generate the set of words that can only be used as nouns (I shall henceforth call this set A). All consecutive sequences of these words were extracted, and the three word sequences used to form the test set. For reasons made clear below, only sequences consisting entirely of words from Roget's thesaurus were retained, giving a total of 308 test triples.3 These triples were manually analysed using as context the entire article in which they appeared. In some cases, the sequence was not a noun compound (nouns can appear adjacent to one another across various constituent boundaries) and was marked as an error. Other compounds exhibited what Hindle and Rooth (1993) have termed SEMANTIC INDETERMINACY where the two possible bracketings cannot be distinguished in the context. The remaining compounds were assigned either a left-branching or right-branching analysis. Table 1 shows the number of each kind and an example of each. Accuracy figures in all the results reported below were computed using only those 244 compounds which received a parse. One problem with applying lexical association to noun compounds is the enormous number of parameters required, one for every possible pair of nouns. Not only does this require a vast amount of memory space, it creates a severe data sparseness problem since we require at least some data about each parameter. Resnik and Hearst (1993) coined the term CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words. By assuming that all words within a group behave similarly, the parameter space can be built in terms of the groups rather than in terms of the words. In this study, conceptual association is used with groups consisting of all categories from the 1911 version of Roget's thesaurus.4 Given two thesaurus categories t1 and t2, there is a parameter which represents the degree of acceptability of the structure [n1n2j where ni is a noun appearing in t1 and n2 appears in t2. By the assumption that words within a group behave similarly, this is constant given the two categories. Following Lauer and Dras (1994) we can formally write this parameter as Pr(ti —.12) where the event ti t2 denotes the modification of a noun in t2 by a noun in t1. To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus. Two types of training scheme are explored in this study, both unsupervised. The first employs a pattern that follows Pustejovsky (1993) in counting the occurrences of subcomponents. A training instance is any sequence of four words w1w2w3w4 where tv1,1v4 Ar and w2, w3 E Ar. Let countp(ni, n2) be the number of times a sequence w1n1n2w4 occurs in the training corpus with wi, tv4 H. The second type uses a window to collect training instances by observing how often a pair of nouns cooccur within some fixed number of words. In this study, a variety of window sizes are used. For n > 2, let countn(ni , n2) be the number of times a sequence niwi win2 occurs in the training corpus where i < n — 2. Note that windowed counts are asymmetric. In the case of a window two words wide, this yields the mutual information metric proposed by Liberman and Sproat (1992). Using each of these different training schemes to arrive at appropriate counts it is then possible to estimate the parameters. Since these are expressed in terms of categories rather than words, it is necessary to combine the counts of words to arrive at estimates. In all cases the estimates used are: Here ambig(w) is the number of categories in which w appears. It has the effect of dividing the evidence from a training instance across all possible categories for the words. The normaliser ensures that all parameters for a head noun sum to unity. Given the high level descriptions in section 1.3 it remains only to formalise the decision process used to analyse a noun compound. Each test compound presents a set of possible analyses and the goal is to choose which analysis is most likely. For three word compounds it suffices to compute the ratio of two probabilities, that of a left-branching analysis and that of a right-branching one. If this ratio is greater than unity, then the left-branching analysis is chosen. When it is less than unity, a right-branching analysis is chosen.5 If the ratio is exactly unity, the analyser guesses left-branching, although this is fairly rare for conceptual association as shown by the experimental results below. For the adjacency model, when the given compound is w1w2w3, we can estimate this ratio as: In both cases, we sum over all possible categories for the words in the compound. Because the dependency model equations have two factors, they are affected more severely by data sparseness. If the probability estimate for Pr(t2 t9 is zero for all possible categories 12 and 13 then both the numerator and the denominator will be zero. This will conceal any preference given by the parameters involving 11. In such cases, we observe that the test instance itself provides the information that the event 12 t3 can occur and we recalculate the ratio using Pr(12 19 = Jr for all possible categories 12,13 where k is any non-zero constant. However, no correction is made to the probability estimates for Pr(ti t2) and Pr(ti 13) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above. The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994) in one way. There, an additional weighting factor (of 2.0) is used to favour a left-branching analysis. This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often. Also, the work reported in Lauer and Dras (1994) uses simplistic estimates of the probability of a word given its thesaurus category. The equations above assume these probabilities are uniformly constant. Section 3.2 below shows the result of making these two additions to the method. Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model. The schemes used are: The accuracy on the test set for all these experiments is shown in figure 2. As can be seen, the dependency model is more accurate than the adjacency model. This is true across the whole spectrum of training schemes. The proportion of cases in which the procedure was forced to guess, either because no data supported either analysis or because both were equally supported, is quite low. For the pattern and two-word window training schemes, the guess rate is less than 4% for both models. In the three-word window training scheme, the guess rates are less than 1%. For all larger windows, neither model is ever forced to guess. In the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (p = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia. In no case do any of the windowed training schemes outperform the pattern scheme. It seems that additional instances admitted by the windowed schemes are too noisy to make an improvement. Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995), and support the conclusion that the dependency model is superior to the adjacency model. Lauer and Dras (1994) suggest two improvements to the method used above. These are: tion in the probability of categories. While these changes are motivated by the dependency model, I have also applied them to the adjacency model for comparison. To implement them, equations 1 and 2 must be modified to incorporate a factor of in each term of the sum and the l 1 td1t2iissi entire ratio must be multiplied by two. Five training schemes have been applied with these extensions. The accuracy results are shown in figure 3. For comparison, the untuned accuracy figures are shown with dotted lines. A marked improvement is observed for the adjacency model, while the dependency model is only slightly improved. To determine the difference made by conceptual association, the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model, but only for the words in the test set. If the same system were to be applied across all of Ar (a total of 90,000 nouns), then around 8.1 billion parameters would be required. Left-branching is favoured by a factor of two as described in the previous section, but no estimates for the category probabilities are used (these being meaningless for the lexical association method). Accuracy and guess rates are shown in figure 4. Conceptual association outperforms lexical association, presumably because of its ability to generalise. One problem with the training methods given in section 2.3 is the restriction of training data to nouns in H. Many nouns, especially common ones, have verbal or adjectival usages that preclude them from being in N. Yet when they occur as nouns, they still provide useful training information that the current system ignores. To test whether using tagged data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results. Three training schemes have been used and the tuned analysis procedures applied to the test set. Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines. If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy. However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%. The experiments above demonstrate a number of important points. The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns. At the very least, this information can be applied in broad coverage parsing to assist in the control of search. I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern. While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed. In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature. This result is in accordance with the informal reasoning given in section 1.3. The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets. In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching. Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing. This work has received valuable input from people too numerous to mention. The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.
Building a Large Annotated Corpus of English: The Penn Treebank Mitchell P. Marcus* University of Pennsylvania Mary Ann Marcinkiewicz~ University of Pennsylvania Beatrice Santorini t Northwestern University 1. Introduction There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora re beginning to serve as important research tools for investigators in natural anguage processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic onstruction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models. In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1. The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and pre- senting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of ? Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. f Department of Linguistics, Northwestern University, Evanston, IL 60208. :~ Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. 1 A distinction is sometimes made between a corpus as a carefully structured set of materials gathered together to jointly meet some design principles, and a collection, which may be much more opportunistic n construction. We acknowledge that from this point of view, the raw materials of the Penn Treebank form a collection. (~) 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 2 the POS tagging phase is automatically parsed and simplified to yield a skeletal syn- tactic representation, which is then corrected by human annotators. After presenting the set of syntactic tags that we use, we illustrate and discuss the bracketing process. In particular, we will outline various factors that affect the speed with which annotators are able to correct bracketed structures, a task that--not surprisingly--is considerably more difficult than correcting POS-tagged text. Finally, Section 5 describes the com- position and size of the current Treebank corpus, briefly reviews some of the research projects that have relied on it to date, and indicates the directions that the project is likely to take in the future. Part-of-Speech Tagging 2.1 A Simplified POS Tagset for English The POS tagsets used to annotate large corpora in the past have traditionally been fairly extensive. The pioneering Brown Corpus distinguishes 87 simple tags (Francis 1964; Francis and Ku~era 1982) and allows the formation of compound tags; thus, the contraction I m is tagged as PPSS+BEM (PPSS for "non-third person nominative per- sonal pronoun" and BEM for "am, m". 2 Subsequent projects have tended to elaborate the Brown Corpus tagset. For instance, the Lancaster-Oslo/Bergen (LOB) Corpus uses about 135 tags, the Lancaster UCREL group about 165 tags, and the London-Lund Cor- pus of Spoken English 197 tags. 3 The rationale behind developing such large, richly articulated tagsets is to approach "the ideal of providing distinct codings for all classes of words having distinct grammatical behaviour" (Garside, Leech, and Sampson 1987, p. 167). 2.1.1 Recoverability. Like the tagsets just mentioned, the Penn Treebank tagset is based on that of the Brown Corpus. However, the stochastic orientation of the Penn Tree- bank and the resulting concern with sparse data led us to modify the Brown Corpus tagset by paring it down considerably. A key strategy in reducing the tagset was to eliminate redundancy by taking into account both lexical and syntactic information. Thus, whereas many POS tags in the Brown Corpus tagset are unique to a particular lexical item, the Penn Treebank tagset strives to eliminate such instances of lexical re- dundancy. For instance, the Brown Corpus distinguishes five different forms for main verbs: the base form is tagged VB, and forms with overt endings are indicated by appending D for past tense, G for present participle/gerund, N for past participle, and Z for third person singular present. Exactly the same paradigm is recognized for have, but have (regardless of whether it is used as an auxiliary or a main verb) is as- signed its own base tag HV. The Brown Corpus further distinguishes three forms of do--the base form (DO), the past tense (DOD), and the third person singular present (DOZ), 4 and eight forms of be--the five forms distinguished for regular verbs as well as the irregular forms am (BEM), are (BER), and was (BEDZ). By contrast, since the distinctions between the forms of VB on the one hand and the forms of BE, DO, and HV on the other are lexically recoverable, they are eliminated in the Penn Treebank, as shown in Table 1. 5 2 Counting both simple and compound tags, the Brown Corpus tagset contains 187 tags. 3 A useful overview of the relation of these and other tagsets to each other and to the Brown Corpus tagset is given in Appendix B of Garside, Leech, and Sampson (1987). 4 The gerund and the participle of do are tagged VBG and VBN in the Brown Corpus, respectively--presumably because they are never used as auxiliary verbs in American English. 5 The irregular present ense forms am and are are tagged as VBP in the Penn Treebank (see Section 2.1.3), just like any other non-third person singular present ense form. 314 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 1 Elimination of lexically recoverable distinctions. sing/VB be/VB do/VB have/VB sings/VBZ is/VBZ does/VBZ has/VBZ sang/VBD was/VBD did/VBD had/VBD singing/VBG being/VBG doing/VBG having/VBG sung/VBN been/VBN done/VBN had/VBN A second example of lexical recoverability concerns those words that can precede articles in noun phrases. The Brown Corpus assigns a separate tag to pre-qualifiers (quite, rather, such), pre-quantifiers (all, half, many, nary) and both. The Penn Treebank, on the other hand, assigns all of these words to a single category PDT (predeterminer). Further examples of lexically recoverable categories are the Brown Corpus categories PPL (singular eflexive pronoun) and PPLS (plural reflexive pronoun), which we col- lapse with PRP (personal pronoun), and the Brown Corpus category RN (nominal adverb), which we collapse with RB (adverb). Beyond reducing lexically recoverable distinctions, we also eliminated certain POS distinctions that are recoverable with reference to syntactic structure. For instance, the Penn Treebank tagset does not distinguish subject pronouns from object pronouns even in cases where the distinction is not recoverable from the pronouns form, as with you, since the distinction is recoverable on the basis of the pronouns position in the parse tree in the parsed version of the corpus. Similarly, the Penn Treebank tagset conflates ubordinating conjunctions with prepositions, tagging both categories as IN. The distinction between the two categories i not lost, however, since subor- dinating conjunctions can be recovered as those instances of IN that precede clauses, whereas prepositions are those instances of IN that precede noun phrases or preposi- tional phrases. We would like to emphasize that the lexical and syntactic recoverability inherent in the POS-tagged version of the Penn Treebank corpus allows end users to employ a much richer tagset han the small one described in Section 2.2 if the need arises. 2.1.2 Consistency. As noted above, one reason for eliminating a POS tag such as RN (nominal adverb) is its lexical recoverability. Another important reason for doing so is consistency. For instance, in the Brown Corpus, the deictic adverbs there and now are always tagged RB (adverb), whereas their counterparts here and then are inconsistently tagged as RB (adverb) or RN (nominal adverb) even in identical syntactic ontexts, such as after a preposition. It is clear that reducing the size of the tagset reduces the chances of such tagging inconsistencies. 2.1.3 Syntactic Function. A further difference between the Penn Treebank and the Brown Corpus concerns the significance accorded to syntactic ontext. In the Brown Corpus, words tend to be tagged independently of their syntactic function. 6 For in- stance, in the phrase the one, one is always tagged as CD (cardinal number), whereas 6 An important exception is there, which the Brown Corpus tags as EX (existential there) when it is used as a formal subject and as RB (adverb) when it is used as a locative adverb. In the case of there, we did not pursue our strategy oftagset reduction toits logical conclusion, which would have implied tagging existential there as NN (common noun). 315 Computational Linguistics Volume 19, Number 2 in the corresponding plural phrase the ones, ones is always tagged as NNS (plural com- mon noun), despite the parallel function of one and ones as heads of the noun phrase. By contrast, since one of the main roles of the tagged version of the Penn Treebank corpus is to serve as the basis for a bracketed version of the corpus, we encode a words syntactic function in its POS tag whenever possible. Thus, one is tagged as NN (singular common noun) rather than as CD (cardinal number) when it is the head of a noun phrase. Similarly, while the Brown Corpus tags both as ABX (pre-quantifier, double conjunction), regardless of whether it functions as a prenominal modifier (both the boys), a postnominal modifier (the boys both), the head of a noun phrase (both of the boys) or part of a complex coordinating conjunction (both boys and girls), the Penn Treebank tags both differently in each of these syntactic ontexts--as PDT (predeter- miner), RB (adverb), NNS (plural common noun) and coordinating conjunction (CC), respectively. There is one case in which our concern with tagging by syntactic function has led us to bifurcate Brown Corpus categories rather than to collapse them: namely, in the case of the uninflected form of verbs. Whereas the Brown Corpus tags the bare form of a verb as VB regardless of whether it occurs in a tensed clause, the Penn Treebank tagset distinguishes VB (infinitive or imperative) from VBP (non-third person singular present ense). 2.1.4 Indeterminacy. A final difference between the Penn Treebank tagset and all other tagsets we are aware of concerns the issue of indeterminacy: both POS ambiguity in the text and annotator uncertainty. In many cases, POS ambiguity can be resolved with reference to the linguistic context. So, for instance, in Katharine Hepburns witty line Grant can be outspoken--but not by anyone I know, the presence of the by-phrase forces us to consider outspoken as the past participle of a transitive derivative of speak-- outspeak rather than as the adjective outspoken. However, even given explicit criteria for assigning POS tags to potentially ambiguous words, it is not always possible to assign a unique tag to a word with confidence. Since a major concern of the Treebank is to avoid requiring annotators to make arbitrary decisions, we allow words to be associated with more than one POS tag. Such multiple tagging indicates either that the words part of speech simply cannot be decided or that the annotator is unsure which of the alternative tags is the correct one. In principle, annotators can tag a word with any number of tags, but in practice, multiple tags are restricted to a small number of recurring two-tag combinations: JJINN (adjective or noun as prenominal modifier), JJIVBG (adjective or gerund/present participle), JJ[VBN (adjective or past participle), NNIVBG (noun or gerund), and RBIRP (adverb or particle). 2.2 The POS Tagset The Penn Treebank tagset is given in Table 2. It contains 36 POS tags and 12 other tags (for punctuation and currency symbols). A detailed description of the guidelines governing the use of the tagset is available in Santorini (1990). 7 2.3 The POS Tagging Process The tagged version of the Penn Treebank corpus is produced in two stages, using a combination of automatic POS assignment and manual correction. 7 In versions of the tagged corpus distributed before November 1992, singular proper nouns, plural proper nouns, and personal pronouns were tagged as "NP," "NPS," and "PP," respectively. The current tags "NNP," "NNPS," and "PRP" were introduced in order to avoid confusion with the syntactic tags "NP" (noun phrase) and "PP" (prepositional phrase) (see Table 3). 316 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 2 The Penn Treebank POS tagset. CC Coordinating conjunction 25. CD Cardinal number 26. UH Interjection 3. DT Determiner 27. VB Verb, base form 4. EX Existential there 28. VBD Verb, past tense 5. FW Foreign word 29. VBG Verb, gerund/present 6. IN Preposition/subordinating participle conjunction 30. VBN Verb, past participle 7. JJ Adjective 31. VBP Verb, non-3rd ps. JJR Adjective, comparative 32. VBZ Verb, 3rd ps. JJS Adjective, superlative 33. WDT wh-determiner 10. LS List item marker 34. WP wh-pronoun 11. WP$ Possessive wh-pronoun 12. NN Noun, singular or mass 36. WRB wh-adverb 13. NNS Noun, plural 37. # Pound sign 14. NNP Proper noun, singular 38. $ Dollar sign 15. NNPS Proper noun, plural 39.. Sentence-final punctuation 16. PDT Predeterminer 40. , Comma 17. POS Possessive nding 41. : Colon, semi-colon 18. PRP Personal pronoun 42. ( Left bracket character 19. PP$ Possessive pronoun 43. ) Right bracket character 20. RB Adverb 44. " Straight double quote 21. RBR Adverb, comparative 45. Left open single quote 22. RBS Adverb, superlative 46. " Left open double quote 23. RP Particle 47. Right close single quote 24. SYM Symbol (mathematical or scientific) 48. " Right close double quote 2.3.1 Automated Stage. During the early stages of the Penn Treebank project, the initial automatic POS assignment was provided by PARTS (Church 1988), a stochastic algorithm developed at AT&T Bell Labs. PARTS uses a modif ied version of the Brown Corpus tagset close to our own and assigns POS tags with an error rate of 3-5%. The output of PARTS was automatically tokenized 8 and the tags assigned by PARTS were automatically mapped onto the Penn Treebank tagset. This mapp ing  introduces about 4% error, since the Penn Treebank tagset makes certain distinctions that the PARTS tagset does not. 9 A sample of the resulting tagged text, which has an error rate of 7-9%, is shown in Figure 1. More recently, the automatic POS assignment is provided by a cascade of stochastic and rule-driven taggers developed on the basis of our early experience. Since these taggers are based on the Penn Treebank tagset, the 4% error rate introduced as an artefact of mapping from the PARTS tagset o ours is eliminated, and we obtain error rates of 2-6%. 2.3.2 Manual Correction Stage. The result of the first, automated stage of POS tagging is given to annotators to correct. The annotators use a mouse-based package written 8 In contrast to the Brown Corpus, we do not allow compound tags of the sort illustrated above for Im. Rather, contractions and the Anglo-Saxon genitive of nouns are automatically split into their component morphemes, and each morpheme is tagged separately. Thus, childrens is tagged "children/NNS s/POS," and wont is tagged "wo-/MD nt/RB." 9 The two largest sources of mapping error are that the PARTS tagset distinguishes neither infinitives from non-third person singular present tense forms of verbs, nor prepositions from particles in cases like run up a hill and run up a bill. 317 Computational Linguistics Volume 19, Number 2 Battle-tested/NNP industrial/JJ managers/NNS here/RB always/RB buck/VB up/IN nervous/JJ newcomers/NNS with/IN the/DT tale/NN of/IN the/DT first/JJ of/IN their/PP$ countrymen/NNS to/TO visit/VB Mexico/NNP ,/, a/DT boatload/NN of/IN samurai/NNS warriors/NNS blown/VBN ashore/RB 375/CD years/NNS ago/RB ./. "/" From/IN the/DT beginning/NN ,/, it/PRP took/VBD a/DT man/NN with/IN extraordinary/JJ qualities/NNS to/TO succeed/VB in/IN Mexico/NNP ,/, "/" says/VBZ Kimihide/NNP Takimura/NNP ,/, president/NN of/IN Mitsui/NNS group/NN s/POS Kensetsu/NNP Engineering/NNP Inc./NNP unit/NN ./. Figure 1 Sample tagged text--before correction. Battle-tested/NNP*/JJ industrial/JJ managers/NNS here/RB always/RB buck/VB*/VBP up/IN*/RP nervous/JJ newcomers/NNS with/IN the/DT tale/NN of/IN the/DT first/JJ of/IN their/PP$ countrymen/NNS to/TO visit/VB Mexico/NNP ,/, a/DT boatload/NN of/IN samurai/NNS*/FW warriors/NNS blown/VBN ashore/RB 375/CD years/NNS ago/RB ./. "/" From/IN the/DT beginning/NN ,/, it/PRP took/VBD a/DT man/NN with/IN extraordinary/JJ qualities/NNS to/TO succeed/VB in/IN Mexico/NNP ,/, " /"  says/VBZ Kimihide/NNP Takimura/NNP ,/, president/NN of/IN Mitsui/NNS*/NNP group/NN s/POS Kensetsu/NNP Engineering/NNP Inc./NNP unit/NN ./. Figure 2 Sample tagged text--after correction. in GNU Emacs Lisp, which is embedded within the GNU Emacs editor (Lewis et al. The package allows annotators to correct POS assignment errors by positioning the cursor on an incorrectly tagged word and then entering the desired correct ag (or sequence of multiple tags). The annotators input is automatically checked against the list of legal tags in Table 2 and, if valid, appended to the original word-tag pair separated by an asterisk. Appending the new tag rather than replacing the old tag allows us to easily identify recurring errors at the automatic POS assignment s age. We believe that the confusion matrices that can be extracted from this information should also prove useful in designing better automatic taggers in the future. The result of this second stage of POS tagging is shown in Figure 2. Finally, in the distribution version of the tagged corpus, any incorrect tags assigned at the first, automatic stage are removed. The learning curve for the POS tagging task takes under a month (at 15 hours a week), and annotation speeds after a month exceed 3,000 words per hour. Two Modes of AnnotationwAn Experiment To determine how to maximize the speed, inter-annotator consistency, and accuracy of POS tagging, we performed an experiment a the very beginning of the project o com- pare two alternative modes of annotation. In the first annotation mode ("tagging"), annotators tagged unannotated text entirely by hand; in the second mode ("correct- ing"), they verified and corrected the output of PARTS, modified as described above. 318 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English This experiment showed that manual tagging took about twice as long as correcting, with about twice the inter-annotator disagreement rate and an error rate that was about 50% higher. Four annotators, all with graduate training in linguistics, participated in the exper- iment. All completed a training sequence consisting of 15 hours of correcting followed by 6 hours of tagging. The training material was selected from a variety of nonfiction genres in the Brown Corpus. All the annotators were familiar with GNU Emacs at the outset of the experiment. Eight 2,000-word samples were selected from the Brown Cor- pus, two each from four different genres (two fiction, two nonfiction), none of which any of the annotators had encountered in training. The texts for the correction task were automatically tagged as described in Section 2.3. Each annotator first manually tagged four texts and then corrected four automatically tagged texts. Each annotator completed the four genres in a different permutation. A repeated measures analysis of annotation speed with annotator identity, genre, and annotation mode (tagging vs. correcting) as classification variables howed a sig- nificant annotation mode effect (p = .05). No other effects or interactions were signif- icant. The average speed for correcting was more than twice as fast as the average speed for tagging: 20 minutes vs. 44 minutes per 1,000 words. (Median speeds per 1,000 words were 22 vs. 42 minutes.) A simple measure of tagging consistency is inter-annotator disagreement rate, the rate at which annotators disagree with one another over the tagging of lexical tokens, expressed as a percentage of the raw number of such disagreements over the number of words in a given text sample. For a given text and n annotators, there are disagreement ratios (one for each possible pair of annotators). Mean inter-annotator disagreement was 7.2% for the tagging task and 4.1% for the correcting task (with me- dians 7.2% and 3.6%, respectively). Upon examination, a disproportionate amount of disagreement i  the correcting case was found to be caused by one text that contained many instances of a cover symbol for chemical and other formulas. In the absence of an explicit guideline for tagging this case, the annotators had made different decisions on what part of speech this cover symbol represented. When this text is excluded from consideration, mean inter-annotator disagreement for the correcting task drops to 3.5%, with the median unchanged at 3.6%. Consistency, while desirable, tells us nothing about he validity of the annotators corrections. We therefore compared each annotators output not only with the output of each of the others, but also with a benchmark version of the eight texts. This benchmark version was derived from the tagged Brown Corpus by (1) mapping the original Brown Corpus tags onto the Penn Treebank tagset and (2) carefully hand- correcting the revised version in accordance with the tagging conventions in force at the time of the experiment. Accuracy was then computed as the rate of disagreement between each annotators results and the benchmark version. The mean accuracy was 5.4% for the tagging task (median 5.7%) and 4.0% for the correcting task (median 3.4%). Excluding the same text as above gives a revised mean accuracy for the correcting task of 3.4%, with the median unchanged. We obtained a further measure of the annotators accuracy by comparing their error rates to the rates at which the raw output of Churchs PARTS program--appropri- ately modified to conform to the Penn Treebank tagset--disagreed with the benchmark version. The mean disagreement rate between PARTS and the benchmark version was 319 Computational Linguistics Volume 19, Number 2 9.6%, while the corrected version had a mean disagreement rate of 5.4%, as noted above. The annotators were thus reducing the error rate by about 4.2%. Bracketing 4.1 Basic Methodology The methodology for bracketing the corpus is completely parallel to that for tagging-- hand correction of the output of an errorful automatic process. Fidditch, a deterministic parser developed by Donald Hindle first at the University of Pennsylvania nd sub- sequently at AT&T Bell Labs (Hindle 1983, 1989), is used to provide an initial parse of the material. Annotators then hand correct he parsers output using a mouse-based interface implemented in GNU Emacs Lisp. Fidditch has three properties that make it ideally suited to serve as a preprocessor to hand correction: ? Fidditch always provides exactly one analysis for any given sentence, so that annotators need not search through multiple analyses. Fidditch never attaches any constituent whose role in the larger structure it cannot determine with certainty. In cases of uncertainty, Fidditch chunks the input into a string of trees, providing only a partial structure for each sentence. Fidditch has rather good grammatical coverage, so that the grammatical chunks that it does build are usually quite accurate. Because of these properties, annotators do not need to rebracket much of the parsers output--a relatively time-consuming task. Rather, the annotators main task is to "glue" together the syntactic hunks produced by the parser. Using a mouse-based interface, annotators move each unattached chunk of structure under the node to which it should be attached. Notational devices allow annotators to indicate uncertainty concerning constituent labels, and to indicate multiple attachment sites for ambiguous modifiers. The bracketing process is described in more detail in Section 4.3. 4.2 The Syntactic Tagset Table 3 shows the set of syntactic tags and null elements that we use in our skeletal bracketing. More detailed information on the syntactic tagset and guidelines concern- ing its use are to be found in Santorini and Marcinkiewicz (1991). Although different in detail, our tagset is similar in delicacy to that used by the Lancaster Treebank Project, except hat we allow null elements in the syntactic anno- tation. Because of the need to achieve a fairly high output per hour, it was decided not to require annotators to create distinctions beyond those provided by the parser. Our approach to developing the syntactic tagset was highly pragmatic and strongly influenced by the need to create a large body of annotated material given limited hu- man resources. Despite the skeletal nature of the bracketing, however, it is possible to make quite delicate distinctions when using the corpus by searching for combinations of structures. For example, an SBAR containing the word to immediately before the VP will necessarily be infinitival, while an SBAR containing a verb or auxiliary with a 10 We would like to emphasize that the percentage given for the modified output of PARTS does not represent an error rate for PARTS. It reflects not only true mistakes in PARTS performance, but also the many and important differences in the usage of Penn Treebank POS tags and the usage of tags in the original Brown Corpus material on which PARTS was trained. 320 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 3 The Penn Treebank syntactic tagset. X Null elements 2. NIL Adjective phrase Adverb phrase Noun phrase Prepositional phrase Simple declarative clause Clause introduced by subordinating conjunction or 0 (see below) Direct question introduced by wh-word or wh-phrase Declarative sentence with subject-aux inversion Subconstituent of SBARQ excluding wh-word or wh-phrase Verb phrase wh-adverb phrase wh-noun phrase wh-prepositional phrase Constituent of unknown or uncertain category "Understood" subject of infinitive or imperative Zero variant of that in subordinate clauses Trace--marks position where moved wh-constituent is interpreted Marks position where preposition is interpreted in pied-piping contexts tense feature will necessarily be tensed. To take another example, so-called that-clauses can be identified easily by searching for SBARs containing the word that or the null element 0 in initial position. As can be seen from Table 3, the syntactic tagset used by the Penn Treebank in- cludes a variety of null elements, a subset of the null elements introduced by Fidditch. While it would be expensive to insert null elements entirely by hand, it has not proved overly onerous to maintain and correct hose that are automatically provided. We have chosen to retain these null elements because we believe that they can be exploited in many cases to establish a sentences predicate-argument structure; at least one recipient of the parsed corpus has used it to bootstrap the development of lexicons for partic- ular NLP projects and has found the presence of null elements to be a considerable aid in determining verb transitivity (Robert Ingria, personal communication). While these null elements correspond more directly to entities in some grammatical theories than in others, it is not our intention to lean toward one or another theoretical view in producing our corpus. Rather, since the representational framework for grammatical structure in the Treebank is a relatively impoverished flat context-free notation, the eas- iest mechanism to include information about predicate-argument structure, although indirectly, is by allowing the parse tree to contain explicit null items. 4.3 Sample Bracketing Output Below, we illustrate the bracketing process for the first sentence of our sample text. Figure 3 shows the output of Fidditch (modified slightly to include our POS tags). As Figure 3 shows, Fidditch leaves very many constituents unattached, labeling them as "? ", and its output is perhaps better thought of as a string of tree fragments than as a single tree structure. Fidditch only builds structure when this is possible for a purely syntactic parser without access to semantic or pragmatic information, and it 321 Computational Linguistics Volume 19, Number 2 ( (s (NP (? Figure 3 (NBAK (ADJP (ADJ "Battle-tested/JJ") (ADJ "industrial/JJ")) (NPL "managers/NNS"))) (? (ADV "here/KB")) (? (ADV "always/KB")) (AUX (TNS *)) (VP (VPKES "buck/VBP"))) (? (PP (PKEP "up/KP") (NP (NBAR (ADJ "nervous/JJ") (NPL "newcomers/NNS"))))) (? (PP (PREP "with/IN") (NP (DART "the/DT") (NBAK (N "tale/NN")) (PP of/PKEP (NP (DART "the/DT") (NBAK (ADJP (ADJ "first/JJ")))))))) (? (PP of/PREP (NP (PROS "their/PP$") (NBAK (NPL "countrymen/NNS")))) (? (S (NP (PRO *)) (AUX to/TNS) (VP (V "visit/VB") (NP (PNP "Mexico/NNP"))))) (? (MID ",/,")) (? (NP (IAKT "a/DT") (NBAK (N "boatload/NN")) (PP of/PKEP (NP (NBAK (NPL "warriors/NNS")))) (VP (VPPKT "blown/VBN") (? (ADV "ashore/KB")) (NP (NBAR (CARD "375/CD") (NPL "years/NNS")))))) (ADV "ago/KB")) (FIN "./."))) Sample bracketed text--full structure provided by Fidditch. always errs on the side of caution. Since determining the correct attachment point of prepositional phrases, relative clauses, and adverbial modifiers almost always requires extrasyntactic information, Fidditch pursues the very conservative strategy of always leaving such constituents unattached, even if only one attachment point is syntacti- cally possible. However, Fidditch does indicate its best guess concerning a fragments attachment site by the fragments depth of embedding. Moreover, it attaches preposi- tional phrases beginning with of if the preposition immediately follows a noun; thus, tale of... and boatload of... are parsed as single constituents, while first of... is not. Since Fidditch lacks a large verb lexicon, it cannot decide whether some constituents serve as adjuncts or arguments and hence leaves subordinate clauses such as infini- 322 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English tives as separate fragments. Note further that Fidditch creates adjective phrases only when it determines that more than one lexical item belongs in the ADJP. Finally, as is well known, the scope of conjunctions and other coordinate structures can only be determined given the richest forms of contextual information; here again, Fidditch simply turns out a string of tree fragments around any conjunction. Because all de- cisions within Fidditch are made locally, all commas (which often signal conjunction) must disrupt he input into separate chunks. The original design of the Treebank called for a level of syntactic analysis compa- rable to the skeletal analysis used by the Lancaster Treebank, but a limited experiment was performed early in the project o investigate the feasibility of providing greater levels of structural detail. While the results were somewhat unclear, there was ev- idence that annotators could maintain a much faster rate of hand correction if the parser output was simplified in various ways, reducing the visual complexity of the tree representations and eliminating a range of minor decisions. The key results of this experiment were as follows: ? Annotators take substantially longer to learn the bracketing task than the POS tagging task, with substantial increases in speed occurring even after two months of training. Annotators can correct he full structure provided by Fidditch at an average speed of approximately 375 words per hour after three weeks and 475 words per hour after six weeks. Reducing the output from the full structure shown in Figure 3 to a more skeletal representation similar to that used by the Lancaster UCREL Treebank Project increases annotator productivity by approximately 100-200 words per hour. It proved to be very difficult for annotators to distinguish between a verbs arguments and adjuncts in all cases. Allowing annotators to ignore this distinction when it is unclear (attaching constituents high) increases productivity by approximately 150-200 words per hour. Informal examination of later annotation showed that forced distinctions cannot be made consistently. As a result of this experiment, the originally proposed skeletal representation was adopted, without a forced distinction between arguments and adjuncts. Even after extended training, performance varies markedly by annotator, with speeds on the task of correcting skeletal structure without requiring a distinction between arguments and adjuncts ranging from approximately 750 words per hour to well over 1,000 words per hour after three or four months experience. The fastest annotators work in bursts of well over 1,500 words per hour alternating with brief rests. At an average rate of 750 words per hour, a team of five part-time annotators annotating three hours a day should maintain an output of about 2.5 million words a year of "treebanked" sentences, with each sentence corrected once. It is worth noting that experienced annotators can proofread previously corrected material at very high speeds. A parsed subcorpus of over I million words was recently proofread at an average speed of approximately 4,000 words per annotator per hour. At this rate of productivity, annotators are able to find and correct gross errors in parsing, but do not have time to check, for example, whether they agree with all prepositional phrase attachments. 323 Computational Linguistics Volume 19, Number 2 (S (NP (ADJP Battle-tested industrial) managers) (? always) (VP buck)) (? (PP up (NP nervous newcomers))) (? (PP with (NP the tale (PP of (NP the (ADJP first)))))) (? (PP of (NP their countrymen))) (? (S (NP *) to (VP visit (NP Mexico)))) (? (NP a boatload (PP of (NP warriors)) (VP blown (? ashore) (NP 375 years)))) (? Figure 4 Sample bracketed text--after simplification, before correction. The process that creates the skeletal representations to be corrected by the anno- tators simplifies and flattens the structures shown in Figure 3 by removing POS tags, nonbranching lexical nodes, and certain phrasal nodes, notably NBAR. The output of the first automated stage of the bracketing task is shown in Figure 4. Annotators correct his simplified structure using a mouse-based interface. Their primary job is to "glue" fragments ogether, but they must also correct incorrect parses and delete some structure. Single mouse clicks perform the following tasks, among others. The interface correctly reindents the structure whenever necessary. Attach constituents labeled ?. This is done by pressing down the appropriate mouse button on or immediately after the ?, moving the mouse onto or immediately after the label of the intended parent and releasing the mouse. Attaching constituents automatically deletes their ? Promote a constituent up one level of structure, making it a sibling of its current parent. Delete a pair of constituent brackets. 324 Mitchell R Marcus et al. Building a Large Annotated Corpus of English (S (NP Battle-tested industrial managers here) always (VP buck up (NP nervous newcomers) (PP with (NP the tale (PP of (NP (NP the (ADJP first (PP of (NP (S (NP *) to (VP visit (NP (VP-1 .) Figure 5 Sample bracketed text--after correction. their countrymen))) (NP Mexico)))) (NP a boatload (PP of (NP (NP warriors) (VP-I blown ashore (ADVP (NP 375 years) ago))))) *pseudo-attach*)))))))) ? Create a pair of brackets around a constituent. This is done by typing a constituent tag and then sweeping out the intended constituent with the mouse. The tag is checked to assure that it is a legal label. Change the label of a constituent. The new tag is checked to assure that it is legal. The bracketed text after correction is shown in Figure 5. The fragments are now connected together into one rooted tree structure. The result is a skeletal analysis in that much syntactic detail is left unannotated. Most prominently, all internal structure of the NP up through the head and including any single-word post-head modifiers is left unannotated. As noted above in connection with POS tagging, a major goal of the Treebank project is to allow annotators only to indicate structure of which they were certain. The Treebank provides two notational devices to ensure this goal: the X constituent label and so-called "pseudo-attachment." The X constituent label is used if an annotator is sure that a sequence of words is a major constituent but is unsure of its syntactic category; in such cases, the annotator simply brackets the sequence and labels it X. The second notational device, pseudo-attachment, hastwo primary uses. On the one hand, 325 Computational Linguistics Volume 19, Number 2 it is used to annotate what Kay has called permanent predictable ambiguities, allowing an annotator to indicate that a structure isglobally ambiguous even given the surrounding context (annotators always assign structure to a sentence on the basis of its context). An example of this use of pseudo-attachment is shown in Figure 5, where the participial phrase blown ashore 375 years ago modifies either warriors or boatload, but there is no way of settling the question--both attachments mean exactly the same thing. In the case at hand, the pseudo-attachment notation indicates that the annotator of the sentence thought hat VP-1 is most likely a modifier of warriors, but that it is also possible that it is a modifier of boatload. 11A second use of pseudo-attachment is toallow annotators to represent the "underlying" position of extraposed elements; in addition to being attached in its superficial position in the tree, the extraposed constituent is pseudo- attached within the constituent to which it is semantically related. Note that except for the device of pseudo-attachment, the skeletal analysis of the Treebank is entirely restricted to simple context-free trees. The reader may have noticed that the ADJP brackets in Figure 4 have vanished in Figure 5. For the sake of the overall efficiency of the annotation task, we leave all ADJP brackets in the simplified structure, with the annotators expected to remove many of them during annotation. The reason for this is somewhat complex, but provides a good example of the considerations that come into play in designing the details of annotation methods. The first relevant fact is that Fidditch only outputs ADJP brackets within NPs for adjective phrases containing more than one lexical item. To be consistent, he final structure must contain ADJP nodes for all adjective phrases within NPs or for none; we have chosen to delete all such nodes within NPs under normal circumstances. (This does not affect the use of the ADJP tag for predicative adjective phrases outside of NPs.) In a seemingly unrelated guideline, all coordinate structures are annotated in the Treebank; such coordinate structures are represented by Chomsky-adjunction when the two conjoined constituents bear the same label. This means that if an NP contains coordinated adjective phrases, then an ADJP tag will be used to tag that coordination, even though simple ADJPs within NPs will not bear an APJP tag. Experience has shown that annotators can delete pairs of brackets extremely quickly using the mouse-based tools, whereas creating brackets is a much slower operation. Because the coordination of adjectives i quite common, it is more efficient o leave in ADJP labels, and delete them if they are not part of a coordinate structure, than to reintroduce them if necessary. Progress to Date 5.1 Composition and Size of Corpus Table 4 shows the output of the Penn Treebank project at the end of its first phase. All the materials listed in Table 4 are available on CD-ROM to members of the Linguistic Data Consortium. 12About 3 million words of POS-tagged material and a small sam- piing of skeletally parsed text are available as part of the first Association for Com- putational Linguistics/Data Collection Initiative CD-ROM, and a somewhat larger subset of materials is available on cartridge tape directly from the Penn Treebank Project. For information, contact he first author of this paper or send e-mail to tree- bank@unagi.cis.upenn.edu. 11 This use of pseudo-attachment is identical to its original use in Churchs parser (Church 1980). 12 Contact he Linguistic Data Consortium, 441 Williams Hall, University of Pennsylvania, Philadelphia PA 19104-6305, or send e-mail to ldc@unagi.cis.upenn.edu for more information. 326 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English Table 4 Penn Treebank (as of 11/92). Description Tagged for Skeletal Part-of-Speech Parsing (Tokens) (Tokens) Dept. of Energy abstracts Dow Jones Newswire stories Dept. of Agriculture bulletins Library of America texts MUC-3 messages IBM Manual sentences WBUR radio transcripts ATIS sentences Brown Corpus, retagged 231,404 231,404 3,065,776 1,061,166 78,555 78,555 105,652 105,652 111,828 111,828 89,121 89,121 11,589 11,589 19,832 19,832 1,172,041 1,172,041 Total: 4,885,798 2,881,188 Some comments on the materials included: ? Department of Energy abstracts are scientific abstracts from a variety of disciplines. All of the skeletally parsed Dow Jones Newswire materials are also available as digitally recorded read speech as part of the DARPA WSJ-CSR1 corpus, available through the Linguistic Data Consortium. The Department of Agriculture materials include short bulletins on such topics as when to plant various flowers and how to can various vegetables and fruits. The Library of America texts are 5,000-10,000 word passages, mainly book chapters, from a variety of American authors including Mark Twain, Henry Adams, Willa Cather, Herman Melville, W. E. B. Dubois, and Ralph Waldo Emerson. The MUC-3 texts are all news stories from the Federal News Service about terrorist activities in South America. Some of these texts are translations of Spanish news stories or transcripts of radio broadcasts. They are taken from training materials for the Third Message Understanding Conference. The Brown Corpus materials were completely retagged by the Penn Treebank project starting from the untagged version of the Brown Corpus (Francis 1964). The IBM sentences are taken from IBM computer manuals; they are chosen to contain a vocabulary of 3,000 words, and are limited in length. The ATIS sentences are transcribed versions of spontaneous sentences collected as training materials for the DARPA Air Travel Information System project. The entire corpus has been tagged for POS information, at an estimated error rate 327 Computational Linguistics Volume 19, Number 2 of approximately 3%. The POS-tagged version of the Library of America texts and the Department of Agriculture bulletins have been corrected twice (each by a different annotator), "and the corrected files were then carefully adjudicated; we estimate the error rate of the adjudicated version at well under 1%. Using a version of PARTS retrained on the entire preliminary corpus and adjudicating between the output of the retrained version and the preliminary version of the corpus, we plan to reduce the error rate of the final version of the corpus to approximately 1%. All the skeletally parsed materials have been corrected once, except for the Brown materials, which have been quickly proofread an additional time for gross parsing errors. 5.2 Future Direct ions A large number of research efforts, both at the University of Pennsylvania nd else- where, have relied on the output of the Penn Treebank Project o date. A few examples already in print: a number of projects investigating stochastic parsing have used either the POS-tagged materials (Magerman and Marcus 1990; Brill et al. 1990; Brill 1991) or the skeletally parsed corpus (Weischedel et al. 1991; Pereira and Schabes 1992). The POS-tagged corpus has also been used to train a number of different POS taggers in- cluding Meteer, Schwartz, and Weischedel (1991), and the skeletally parsed corpus has been used in connection with the development of new methods to exploit intonational cues in disambiguating the parsing of spoken sentences (Veilleux and Ostendorf 1992). The Penn Treebank has been used to bootstrap the development of lexicons for particu- lar applications (Robert Ingria, personal communication) and is being used as a source of examples for linguistic theory and psychological modelling (e.g. To aid in the search for specific examples of grammatical phenomena using the Treebank, Richard Pito has developed tgrep, a tool for very fast context-free pattern matching against he skeletally parsed corpus, which is available through the Linguistic Data Consortium. While the Treebank is being widely used, the annotation scheme mployed has a variety of limitations. Many otherwise clear argument/adjunct relations in the corpus are not indicated because of the current Treebanks essentially context-free represen- tation. For example, there is at present no satisfactory representation for sentences in which complement oun phrases or clauses occur after a sentential level adverb. Either the adverb is trapped within the VP, so that the complement can occur within the VP where it belongs, or else the adverb is attached to the S, closing off the VP and forcing the complement to attach to the S. This "trapping" problem serves as a limitation for groups that currently use Treebank material semiautomatically to derive lexicons for particular applications. For most of these problems, however, solutions are possible on the basis of mechanisms already used by the Treebank Project. For example, the pseudo-attachment no ation can be extended to indicate a variety of crossing depen- dencies. We have recently begun to use this mechanism to represent various kinds of dislocations, and the Treebank annotators themselves have developed a detailed proposal to extend pseudo-attachment to a wide range of similar phenomena. A variety of inconsistencies in the annotation scheme used within the Treebank have also become apparent with time. The annotation schemes for some syntactic categories should be unified to allow a consistent approach to determining predicate- argument structure. To take a very simple example, sentential adverbs attach under VP when they occur between auxiliaries and predicative ADJPs, but attach under S when they occur between auxiliaries and VPs. These structures need to be regularized. As the current Treebank has been exploited by a variety of users, a significant number have expressed a need for forms of annotation richer than provided by the projects first phase. Some users would like a less skeletal form of annotation of surface 328 Mitchell P. Marcus et al. Building a Large Annotated Corpus of English grammatical structure, expanding the essentially context-free analysis of the current Penn Treebank to indicate a wide variety of noncontiguous structures and dependen- cies. A wide range of Treebank users now strongly desire a level of annotation that makes explicit some form of predicate-argument structure. The desired level of rep- resentation would make explicit the logical subject and logical object of the verb, and would indicate, at least in clear cases, which subconstituents serve as arguments of the underlying predicates and which serve as modifiers. During the next phase of the Treebank project, we expect o provide both a richer analysis of the existing corpus and a parallel corpus of predicate-argument structures. This will be done by first enriching the annotation of the current corpus, and then automatically extracting predicate-argument structure, at the level of distinguishing logical subjects and objects, and distinguishing arguments from adjuncts for clear cases. Enrichment will be achieved by automatically transforming the current Penn Treebank into a level of structure close to the intended target, and then completing the conversion by hand. Acknowledgments The work reported here was partially supported by DARPA grant No. N0014-85-K0018, byDARPA and AFOSR jointly under grant No. AFOSR-90-0066 and by ARO grant No. DAAL 03-89-C0031 PRI. Seed money was provided by the General Electric Corporation under grant No. We gratefully acknowledge this support. We would also like to acknowledge the contribution of the annotators who have worked on the Penn Treebank Project: Florence Dong, Leslie Dossey, Mark Ferguson, Lisa Frank, Elizabeth Hamilton, Alissa Hinckley, Chris Hudson, Karen Katz, Grace Kim, Robert MacIntyre, Mark Parisi, Britta Schasberger, Victoria Tredinnick and Matt Waters; in addition, Rob Foye, David Magerman, Richard Pito and Steven Shapiro deserve our special thanks for their administrative and programming support. We are grateful to AT&T Bell Labs for permission to use Kenneth Churchs PARTS part-of-speech labeler and Donald Hindles Fidditch parser. Finally, we would like to thank Sue Marcus for sharing with us her statistical expertise and providing the analysis of the time data of the experiment reported in Section 3. The design of that experiment is due to the first two authors; they alone are responsible for its shortcomings. References Brill, Eric (1991). "Discovering the lexical features of a language." In Proceedings, 29th Annual Meeting of the Association for Computational Linguistics. Brill, Eric; Magerman, David; Marcus, Mitchell P.; and Santorini, Beatrice (1990). "Deducing linguistic structure from the statistics of large corpora." In Proceedings, DARPA Speech and Natural Language Workshop. June 1990, 275-282. Church, Kenneth W. (1980). Memory limitations in natural language processing. Masters dissertation, Massachusetts Institute of Technology, Cambridge MA. Church, Kenneth W. (1988). "A stochastic parts program and noun phrase parser for unrestricted text." In Proceedings, Second Conference on Applied Natural Language Processing. Francis, W. Nelson (1964). "A standard sample of present-day English for use with digital computers." Report o the U.S Office of Education on Cooperative Research Project No. Brown University, Providence RI. Francis, W. Nelson, and Ku~era, Henry (1982). Frequency Analysis of English Usage: Lexicon and Grammar. Houghton Mifflin. Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey (1987). The Computational Analysis of English: A Corpus-Based Approach. Hindle, Donald (1983). "User manual for Fidditch." Technical memorandum 7590-142, Naval Research Laboratory. Hindle, Donald (1989). "Acquiring disambiguation rules from text." In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics. Lewis, Bil; LaLiberte, Dan; and the GNU Manual Group (1990). The GNU Emacs Lisp reference manual. Free Software Foundation, Cambridge, MA. Magerman, David, and Marcus, Mitchell P. (1990). "Parsing a natural language using 329 Computational Linguistics Volume 19, Number 2 mutual information statistics." In Proceedings of AAAI-90. Meteer, Marie; Schwartz, Richard; and Weischedel, Ralph (1991). "Studies in part of speech labelling." In Proceedings, Fourth DARPA Speech and Natural Language Workshop. Niv, Michael (1991). "Syntactic disambiguation." In The Penn Review of Linguistics, 14, 120-126. Pereira, Fernando, and Schabes, Yves (1992). "Inside-outside r estimation from partially bracketed corpora." In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics. Santorini, Beatrice (1990). "Part-of-speech tagging guidelines for the Penn Treebank Project." Technical report MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania. Santorini, Beatrice, and Marcinkiewicz, Mary Ann (1991). "Bracketing uidelines for the Penn Treebank Project." Unpublished manuscript, Department of Computer and Information Science, University of Pennsylvania. Veilleux, N. M., and Ostendorf, Mari (1992). "Probabilistic parse scoring based on prosodic features." In Proceedings, Fifth DARPA Speech and Natural Language Workshop. Weischedel, Ralph; Ayuso, Damaris; Bobrow, R.; Boisen, Sean; Ingria, Robert; and Palmucci, Jeff (1991). "Partial parsing: a report of work in progress." In Proceedings, Fourth DARPA Speech and Natural Language Workshop.
The Automated Acquisit ion of Topic Signatures for Text Summarizat ion Chin -Yew L in  and  Eduard  Hovy In fo rmat ion  S(:i(umes I l l s t i tu te Un ivers i ty  of Southern  Ca l i fo rn ia Mar ina  del Rey, CA  90292, USA { cyl,hovy }C~isi.edu Abst rac t In order to produce, a good summary, one has to identify the most relevant portions of a given text. We describe in this t)at)er a method for au- tomatically training tel)it, signatures--sets of related words, with associated weights, organized around head topics and illustrate with signatmes we cre- ;tt.ed with 6,194 TREC collection texts over 4 se- lected tot)ics. We descril)e the l)ossible integration of tolli(: signatures with ontoh)gies and its evaluaton on an automate(l text summarization system. 1 I n t roduct ion This t)aper describes the automated (:reation of what we call topic signatures, constructs that can I)lay a central role. in automated text summarization and information retrieval. ToI)ic signatures can lie used to identify the t)resence of a (:omph~x conce.pt a concept hat consists of several related coinl)onents in fixed relationships. ]~.c.stauvant-uisit, for examph~, invoh,es at h,ast the concel)ts lltCgFIt, t.(tt, pay, and possibly waiter, all(l Dragon Boat PcstivaI (in Tat- wan) involves the Ct)llC(!l)t,S cal(tlztlt,s (a talisman to ward off evil), rnoza (something with the t)ower of preventing pestilen(:e and strengthening health), pic- tures of Ch, un9 Kuei (a nemesis of evil spirits), eggs standing on end, etc. Only when the concepts co- occur is one licensed to infer the comph:x concept; cat or moza alone, for example, are not sufficient. At this time, we do not c.onsider the imerrelationships among tile concepts. Since many texts may describe all the compo- nents of a comI)lex concept without ever exI)lic- itly mentioning the mlderlying complex concel/t--a tol)ic--itself, systems that have to identify topic(s), for summarization or information retrieval, require a method of infcuring comt)hx concelltS flom their component words in the text. 2 Re la ted  Work In late 1970s, ])e.long (DeJong, 1982) developed a system called I"tIUMP (Fast Reading Understand- ing and Memory Program) to skim newspaper sto- ries and extract the main details. FRUMP uses a data structure called sketchy script to organize its world knowhdge. Each sketchy script is what FRUMI ) knows al)out what can occur in l)articu- lar situations such as denmnstrations, earthquakes, labor strike.s, an(t so on. FRUMP selects a t)artic- ular sketchy script based on clues to styled events in news articles. In other words, FRUMP selects an eml)t3 ~ t(uni)late 1whose slots will be tilled on the fly as t"F[UMP reads a news artMe. A summary is gen- erated })ased on what has been (:al)tured or filled in the teml)Iate. The recent success of infornmtion extractk)n re- search has encoreaged the FI{UM1 ) api)roach. The SUMMONS (SUMMarizing Online News artMes) system (McKeown and Radev, 1999) takes tem- l)late outputs of information extra(:tion systems de- velofmd for MUC conference and generating smn- maries of multit)le news artMes. FRUMP and SUM- MONS both rely on t/rior knowledge of their do- mains, th)wever, to acquire such t)rior knowledge is lal)or-intensive and time-consuming. I~)r exam-- l)le, the Unive.rsity of Massa(:husetts CIRCUS sys- l.enl use(l ill the MUC-3 (SAIC, 1998) terrorism do- main required about 1500 i)erson-llours to define ex- traction lmtterns 2 (Rilotf, 1996). In order to make them practical, we need to reduce the knowhxlge n- gineering bottleneck and iml)rove the portability of FI{UMI ) or SUMMONS-like systems. Since the worhi contains thousands, or perhal)s millions, of COml)lex (:on(:et)ts , it is important; to be able to learn sketchy scripts or extraction patterns automatically from corpora -no existing knowledge base contains nearly enough information. (Rilotf aim Lorenzen, 1999) 1)resent a system AutoSlog-TS that generates extraction i)atterns and learns lexical con- straints automatically flom t)rec]assified text to al- leviate the knowledge ngineering I)ottleneck men- tioned above. Although Riloff al)plied AutoSlog-TS lVe viewed sketchy s(:lil)tS and teml)lates as equivalent (ollstrllctS ill the sense that they sl)ecil ~, high level entities and relationships for specific tot)its. 2Aii extra(:l;iOll pattt!rlk is essentially ;t case fraine contains its trigger word, enabling conditions, variable slots, and slot constraints. C IRCUS uses a database of extraction patterns to t~alSe texts (l{ilolI, 1996). 495 to text categorization and information extraction, the concept of relevancy signatures introduced by her is very similar to the topic si.qnatures we pro- posed in this paper. Relevancy signatures and topic signatures arc both trained on preclassitied ocu- ments of specific topics and used to identify the presence of the learned topics in previously unseen documents. The main differences to our approach are: relevancy signatures require a parser. They are sentence-based and applied to text categorization. On the contrary, topic signatures only rely on cor- pus statistics, arc docmnent-based a and used in text smnmarization. In the next section, we describe the automated text smmnarization system SUMMARIST that we used in the experiments to provide the context of discussion. We then define topic signatures and de- tail the procedures for automatically constructing topic signatures. In Section 5, we give an overview of the corpus used in the evaluation. In Section 6 we present he experimental results and the possibility of enriching topic signatures using an existing ontol- ogy. Finally, we end this paper with a conclusion. 3 SUMMARIST SUMMARIST (How and Lin, 1999) is a system designed to generate summaries of multilingual in- put texts. At this time, SUMMARIST can process English, Arabic, Bahasa Indonesia, Japanese, Ko- rean, and Spanish texts. It combines robust natural language processing methods (morl)hologieal trans- formation and part-of-speech tagging), symbolic world knowledge, and information retrieval tech- niques (term distribution and frequency) to achieve high robustness and better concept-level generaliza-- tion. The core of SUMMARIST is based on the follow- ing equation! : summarization = topic identification + topic interpretation + generation. These three stages are: Topic Ident i f ieat lon:  Identify the most imtmrtant (central) topics of the texts. SUMMARIST uses positional importance, topic signature, and term frequency. Importance based on discourse structure will be added later. This is tile most developed stage in SUMMARIST. Topic I n te rpretat ion :  ~i~-) fllse concepts such as waiter, menu, and food into one generalized concept restaurant, we need more than the sin> pie word aggregation used in traditional infor- mation retrieval. We have investigated concept aWe would like to use only the relevant parts of documents to generate topic signatures in the future, qkext segmentation algorithms uch as TextTiling (Ilearst, 1997) can be used to find subtopic segments in text. ABCNEWS.cona  : De lay  in  Hand ing  F l ight  990   [  robe  to  FB I NI SI3 C l la i tn lan  Jarl leS t la l l  says  Egypt ian  clff icials Iv811l to  I,~view res t l l t s of  t i le  invest igat ion  intcl lhe  cras l l  o f  l lggyptA i r  F l ight  990 before  t i le  case i~ lu r l led  over  Ic, t i le Fi31, Nt lv. IG - U S. i lxvestigl~lo[~ lLppear to  be leat l i l lg i i Iore thg l l  eveF low~trd t i le poss ib i l i ty  that  one  o f  the  cc~-pilot~ o f  EgyptA i r  F l ight  990 may have de  [ ihera le ly  c rashed t i le p lane  las t  I lafl l lth, k i l l i ng  all 217 peop le  on  board . f la i l  ever . o f f i c ia ls  say  t i le  Nat iona l  T ran~por  ta t ion  Sa fety  Board  wi l l de lay  t rans fer r ing  tile invegt iga l ion  o f  the  Oct  31 c rash  to  tilt: FI31 - the agency  that  wot l id  lead i~ c r imina l  p robe  - for at  least  tt few days . to  M Iow Egypt ian  exper ts  to rev iew ev idence  ill t i le case. gtts l ) ic iot l~ of  fou l  p lay  were  ra i sed  a f te r  invest igators  l i s ten ing  to  rt tape ftol l l  l i l t  cockp i t  vo ice recorder  i so la ted  a re l ig ious  prayer  or s ta te l l l e l l t made by t i le co -p i lo t  jus t  be fore  t i le  p lane s  autop i lo t  was turned  o f f s l id  the  p lane  began i ts  in i t ia l  p lunge  in to  t i le A t lant i c  Ocean of f  Mas - s r tcht tset t$   Na l l tucket   [s ia l ld . Over  tile pas t  week . a f te r  muc i l  e f fo r t ,  t i le  NTSJB and  t i le  Navy  succeeded ill I ocat i l lg  the  p lane s  two  "b lack  boxes , "  th~ cockp i t  vo ice recorder  and lhe  f l ight  data  recorder . The  tape  ind icates  t l l a t  shor t ly  a f te r  the  p lane  leve led ~ff  a t  i ts c ru i s ing a l t i tude  o f  as ,000  feet ,  t i le  cl~ief p i lo t  o f  t i le a i rc ra f t  left  the  p lane s cockp i t ,  l eav ing  one  o f  t i le  twc~ co-p i lo ts  nIol le t i lere as the  a i rc ra f t  began its descent . Figure 1: A Nov. 16 1999 ABC News page sumnmry generated by SUMMARIST. counting and topic signatures to tackle tile fll- sion problem. Summary Generat ion :  SUMMARIST can pro- duce keyword and extract type summaries. Figure 1 shows an ABC News page summary about EgyptAir Flight 990 by SUMMARIST. SUM- MARIST employs several different heuristics in tile topic identification stage to score terms and sen- tences. The score of a sentence is simply the sum of all the scores of content-bearing terms in the sen- tence. These heuristics arc implemented in separate modules using inputs from preprocessing modules such as tokenizer, part-of-speech tagger, morpholog- ical analyzer, term frequency and tfidf weights cal- culator, sentence length calculator, and sentence lo- cation identifier. Ve only activate the position mod- ule, tile tfidfmodule, and the. topic signature module for comparison. We discuss the effectiveness of these modules in Section 6. 4 Top ic  S ignatures Before addressing the problem of world knowledge acquisition head-on, we decided to investigate what type of knowledge would be useflfl for summariza- tion. After all, one can spend a lifetime acquir- ing knowledge in just a small domain. But what is tile minimum amount of knowledge we need to enable effective topic identification ms illustrated by the restaurant-visit example? Our idea is simple. We would collect a set of terms 4 that were typi- cally highly correlated with a target concept from a preclassified corpus such as TREC collections, and then, during smnmarization, group the occurrence of the related terms by the target concept. For exam- pie, we would replace joint instances of table, inertu, waiter, order, eat, pay, tip, and so on, by the single phrase restaurant-visit, in producing an indicative 4Terms can be stemmed words, bigrams, or trigrams. 496 sulnlllary. Ve thus defined a tot)it signat.ure as a family of related terms, as follows: ~IS = { topic, sifl~zutu.rc. } = {topic,< ( t , ,w l ) , . , ( t , , ,w , , )  >} (1) where topic is the target concet)t and .,d.q)zat~Lrc is a vector of related ternls. Each t, is an term ldghly correlated to topic with association weight w/. The number of related terms 7z can tie set empirically according to a cutot[ associated weight. describe how to acquire related terms and their associated weights in the next section. 4.1  S ignature  Term Ext rac t ion  and  Weight Es t imat ion ()n the assumption that semantically related terms tend to co-occur, on( can construct topic signa- tures flom preclassified text using the X 2 test, mu-. tual information, or other standard statistic tests and infornlation-theoreti(: measures. Instead of X 2, we use likclih.ood ratio (Dunniug, 1993) A, sin(:e A i,; more apI)rot)riate for si/arse data than X 2 test and the quantity -21o9A is asymi)t(/tically X~ dis- tril)ute(15. Therefore, we Call (leterndnc the (:onti- ( lence level for a specific -21o9A value l/y looking ut) X :~ (tistril)ution table and use tlm value to sel(,,ct an at)i)rot)riate cutoff associated weight. We have documents l)[e.classitied into a :;(~t, "R. of relevant exts and a set ~. of nonrelewmt exl;s for a given topic. Assuming the following two hyl)othe,~es: t typothes is  1 ( I f l ) :  t(~Pvlti) = P = P(PvltT/), i.e. the r(.,lewmcy of a d()(:|lment is in(teI)en(hmt, of t i . I  ]  [ypothes is  2 ( t t2) :  I(Pv[ti) == lh ~ 1)2 - t)(Pvlt, i), i.e. :;(;n(:(~ of t i indi(:~Lt(.~. ; strong r(~levan(:y ~ssunling ]h >> 1)2 ? and the following 2-10=2 contingency tabl(;: where Ol~ is the fiequency of term ti occurring in the. l e lev;tnt  set ,  012 is the  [ r ( !qu(nlcy of Lerm t i t)c- curring in the  ] lol lreleval lt ,  set ,  O21 is the  f le(l l lel l( :y of tt;rnl  [ i? ti occurring in the rtdevant set, O._,~ is the flequ(mcy of term l.i ? ti o(:curring in the non- l  e leva i i t  seL. -kssmning a l)inomial distril)ution: C;) b(~; ,,., :/.) = :,:~(1 - .~:)(" ") (2 ) 5This assumes |ha l  the ratio is between the inaximuni like> [ihood est, im&t.(! over a .qll})part of l;}l(! i)alatlllCt(~r sl)a(:(~ ;tll(] l.h(! lllaxillUllll likelihood (}sI.i|II}tlA~ ov(!r the (Hltill! i)alaillt~tt!r si);t(:e. Set! (Manning ;tnd Sch/itze, I999) t)ag, es 172 l.o 175 for d(!t.ails. then the likelihood for HI is: L(H~) = b(Ot~; 0~ + Ou,,p)b(O:,~; 0:,, + Om,,p) and for //2 is: L(H2)  = D(OI 1; O11 Jr" ()12, Pl )b(O21; ()21 Jr- (,)22,1)2) The -2log, value is then computed ms follows: 1. (f/1 ) m --21o 9 - - L( i t  2 ) b(O 11 ; O I  1 + O12,  P) I J (021 : O21 + 022 , P) - -21o 9 1((-)1 l ; ( )11  + O1-),  P I )h (O21 ; O21 q- ( )22  , P2 ) : - -2 ( (O l l  +021) lo r_ Jp+( ( )12+022) lo9(1 - - l~) - -  (,~1) (? ) l l l o  JP l+Ol21og( l  " t 1 )+0211ogp2-~0221o0(1- f~2) ) ) -- .2.,~ x (~  i (7~) -  ;~(~19- ) )  (4 ) = 2,v x Z(P~;  T )  (5 ) whel e  N = O l t  -F O12 -1- O21 -I- 022 is the  to ta l  l lum-. her of t, ernl occurrence, in the corpus, 7/(/~) is the entropy of terms over relevant and nonrelevant sets of documents, 7/ (  fe l t  ) is the entropy of a given term OVel" relev;inL ~/nd nonl  ( .qeval l t  sets  of doel l inel lLS, ~tll(1 Z(R.; T) i:; the inutual information between docu- ment relevancy and a given t(.rm. Equation 5 indi- cates that mutual inforntation 6 is an e(tuiwdent mea- sur(. t() lik(.qiho(id ratio when we assume a binomial distribution and a 2-by-2 (ontingency table. To crest(; topic .~dgnature for a given tot)ic , we: 1. (:lassify doctunents as relevant or nonrclcwmt according t() tile given topic 2. comt)ut.e the -21oflA wdue using Equation 3 for each Lcrm in the document colle(:Lion "{. rank t, erms according 1o their -2lo9~ value 4. select a c(mfid(mce l , vel fiom the A;: (listril)utiotl table; (letermin(~ the cutotf associated weight, mid the numl)(n" of t(nms to he included iIl the signatures 5 The Corpus The training data derives Kern the Question and Answering summary evahmtion data provided l)y T IPSTEI / . -SUMMAC (Mani et al., 1998) that is a sttbset of the TREC collectioliS. The TREC data is a collection of texts, classified into various topics, used for formal ewduaLions of information retrieval sys- tems in a seri(~s of annual (:omparisons. This data set: contains essential text fragnients (phrases, (:Iausos, iuld sentences) which must 1)e included in SUllltIlarios to ~tnswer some TI{EC tel)its. These flagments are each judged 1)y a hmnan judge. As described in Se(:- tion 3, SUMMAI~IST employs several independent nlo(hlles to assign a score to each SelltA:llCe~ and Chell COlll})illeS the st.or(..% L() decide which sentences to ex- tract from the input text;. can gauge the efticacy (>lhe lllll[lla} inrormalion is defined according to chapter 2 of ((;over and Thomas, i991) and is not tile i)airwis(~ mutual inforlnalion us(!d in ((;hur(:h and llanks, 1990). 497 TREC Top ic  Da~cr lp t ion (nunQ Number :  151 ( t i t le}  Top ic :  Co, p ing  w i th  overc rowded pr i sons (dese} Deser i l l t io l l : The  doeu l laent  will p rov ide  in f ,~rn lat ion ol~ jai l  and  pr ison overc rowd iuK and  how i r lmates  are forced to cope  wi th  th,~se cond i t ions ;  or it wil l revea l  p lan~ to  re l ieve  ti le overc rowded ?o l ld i t lon . (nar t )  Nar ra t ive : A re levant  docunaent  will descr ibe  scene~ of overcro~vdi l lg that  have beco lne  all too  crlllllllOll ill ja i l s  and  pr i sons  a ro t tnd  the  count ry ,  T i le document  will i dent i fy  how inmates  are forced to  cope w i th  those  over - crowded cond i t ion~,  and/or  what  ti le Cc l r reet iona l  Syste l l l  is do ing ,  or ph lnn ing  to do,  to a l lev ia te  ti le c rowded col ld i t io l l . (/top) Test  Quest ions QI  Mehat are  name and/or  locat ion  of ti le cor rec t ion  fae i l i l ies where  the  repor ted  ~vercrowd ing  ex is ts? Q2 x~Vhat negat ive  exper iences  have  there  been  at t i le overc rowded fac i l i t ies  (whether  or not tile)" are thought  to have  been  caused by  the  overc rowd lng)? Q3 What  measures  have  been  taken/p la ia i led / recommended (e tc . ) to aecon l lnod~te  more  i l l l l la Ies zlt pena l  fac i l i t ies ,  e .g . ,  doub l i l l g tip, Ile~y COllStructlon? Q,I ~,Vhat measures  have  been  taken/planned/rec~mnlel,ded (etc .} to reduce  ti le l lt l l l lber of Dew il l l i ]ate$, e .g . ,  morator iums on admisMon,  a l te rnat ive  pena l t ies ,  p rograme to reduce c r ime/ rec ld iv i sm? Q5 What  measures  have  been  taken/p lanned/ recommended (e tc . ) to reduce  ti le number  of ex i s t ing  inmates  at an overcrc~wded fac i l i ty ,  e .g . g rant ing  ear ly  re lease ,  t rnns fer ing  to  uncrowded fac i l i t ies? Sample  Answer  Keys (DOCNO)  AP891027-0063 ( /DOCNO) (F ILE ID)  AP -NR-  10-27-89 0615EDT( /F ILE ID) ( IST_L INE) r  a PM-Cha ined Inmates  10-27 0335( / IST .L INE) (2ND-L INE)PM-Cha ined  lnmates ,0344 ( /2ND_L INE) ( I IEAD)  lnmates  Cha ined  to 1.Vails in 13Mtimore Po l i ce S ta t ions ( / l lEAD) (DATEL INE)BALT IMOIT IE  (AP)  ( /DATEL INE} (tEXT) (Q ,q )pr i soner~ are  kept  cha ined  to the wall~ of local po l ice  lcJekup~ for as long as th ree  days  at a t ln~e I)ecattse of overc rowd ing  ill regu la r  je l l cel ls ,  pol ice sa id . ( /Q3} Overcrowd ing  at  the  (Q1) lqMt l rnore  County  Detent ion  Center ( /Q1) h~ forced pn l lee  tn  . (/TEXT) Table 1: TREC topic description for topic 151, test questions expected to be answered by relewmt doc- uments, and a smnple document with answer key, s. of each module by comparing, for ditferent amounts of extraction, how many :good sentences the module selects by itself. We rate a sentence as good simply if it also occurs in the ideal human-made xtract, and measure it using combined recall and precision (F-score). We used four topics r of total 6,194 doc- uments from the TREC collection. 138 of them are relevant documents with T IPSTER-SUMMAC pro- vided answer keys for the question and answering evaluation. Model extracts are created automati- cally from sentences contailfing answer keys. Table 1 shows TREC topic description for topic 151, test questions expected to be answered by relevant doc- uments , and a sample relevant document with an- swer keys markup. 7These four topics are: topic 151: Overcrowded Prisons, 1211 texts, 85 relevant; topic 257: Cigarette Consumption, 1727 texts, 126 relevant; topic 258: Computer Security, 1701 texts, 49 relevant; topic 271: Solar Power, 1555 texts, 59 relevant. SA relevant: document only needs to answer at least one of the five questions. 6 Experimental Results In order to assess the utility of topic signatures in text sununarization, we follow the procedure de- scribed at the end of Section 4.1 to create topic signature for each selected TREC topic. Docu- ments are separated into relevant and nomelevant sets according to their TREC relevancy judgments for each topic. We then run each document hrough a part-of-speech tagger and convert each word into its root form based on the \h)rdNct lexical database. We also collect individual root word (unigram) fie- quency, two consecutive non-stopword 9 (bigram) fie- quency, and three consecutive non-stopwords (tri- gram) fiequeney to facilitate the computation of the -21ogA value for each term. We expect high rank- ing bigram and trigram signature terms to be very informative. We set the cutoff associated weight at 10.83 with confidence level ~t = 0.001 by looking up a X 2 statistical table. Table 2 shows the top 10 unigrmn, bigram, and tri- gram topic signature terms for each topic m. Several conclusions can be drawn directly. Terms with high -21ogA are indeed good indicators for their corre- sponding topics. The -2logA values decrease as the number of words in a term increases. This is rea- sonable, since longer terms usually occur less often than their constituents. However, bigram terms are more informative than nnigrant erms as we can ob- serve: jail//prison overervwding of topic 151, tobacco industry of topic 257, computer security of topic 258, and solar en, ergy/imwer of topic 271. These mLto- matically generated signature terms closely resemble or equal the given short TREC topic descriptions. Although trigram terms shown in the table, such as federal court order, philip morris 7~r, jet propul.. sion laboratory, and mobile telephone s:qstem are also meaningflfl, they do not demonstrate he closer term relationship among other terms in their respective topics that is seen in tlm bigram cases. We expect that more training data can improve tile situation. We notice that the -2logA values for topic 258 are higher than those of the other three topics. As indicated by (Mani et al., 1998) the majority of rel- evant documents for topic 258 have the query topic as their main theme; while the others mostly have the query topics as their subsidiary themes. This implies that it is too liberal to assume all the terms in relevant documents of the other three topics are relevant. We plan to apply text segmentation algo- rithms such as TextTiling (Hearst, t997) to segment documents into subtopic units. We will then per- form the topic signature creation procedure only on tile relevant units to prevent inchlsion of noise terms. 9,Ve use the stopword list supplied with the SMAIIT re- trieval system. l?qhe -2logA values are not comparable across ngram cat- egories, since each ngraln category has its own sample space. 498 Top ic I :ll h~l al l l  -21,~gX  ] l i~ la l l I  -21,,9X j a i l  t)3L I)1,1 e()tH~t 2, ja i l  Dit) 27:1 c+,l l l l l} .IIJN ~21 eae ly  le+]+.~lSt ? N,~ :{t;] , )v , . ) , ~ , ,wd ln~;  :?12:1. , ,n  7.1 R72 i l l ln?lt ," 2 : t l  7d5  s ta l , "  1,) i~, ,n, . i  67  ,3(~t~ ~h+. l i f  [  IF, I . i l o  ,1:~ 3 fill," l ; l  t(;2") s ta le  151 9t t~ ia i l  r l%l  lctr~%vI] l r ld I;1 ~[ i I}l l~t l l i l  l  I I  ~" I ";~ C(,tlt I + , l , i " t  t{ll.O! )l} i+tl-s,,rl 1,17, 3t),i h . .a l  j a i l  56  t i t+ C l )y  133177 p l l . , )D  ( )vcy lc l , , i v th l l~  55 37:  +, , , v , . r , ,wd ,+d 12N I)t)S i-(*lllt :l[ fac i l i t  3 52 9o9 10 S ignat t t ro  Torms o f  Tup ic  151  Ovorcrowdod Pr i sons "II I~I al I l  -21,,~11 f - , I t . l~t l  c , ,u l~ <, l t t , . l  -I., :),;11 C, , l l l p  ]y  c+,ll~(lll ,]+c[+++" 3,5 12L +l,.kali+ ii)iI i,[~ +h,  l l  [  [  [15 121 ~,11  i,) t l ; ,nk  :;.5 L21 j , )o t l ; l l l k  IH ) l i5  :~.,.121 pl l~C, l l ,  r  c+)l l : l l~ la i l  :~5 121 91: i f , .   ] ) l l~t ) l l  i21) l l l t l~ ~N t).l[] t put  pl is+, l l  .2t~ :t-II c+~uuly  jaiL ~l ; l l , . 2 t~31 l h,,hl l,~e~l ja i l  2d  :l I I Top ic  10  S ig l ln t t l re  Tern ls  o f  Top ic  257  - ( l igar~t t~ Co| l s l l l ) l | ) t lo | l l :n i<r tun  21ogX I+i ~.rarll -- 2 / , , f / . i r i4~am - 21, ,~A c lgrt l , . I t?+ .171; [}:iN ~tlb:xtc+) LIt(  ] l l~l l~ ~il 7)iN I ,h i l ip  I I l , ) l l i+ t j l  2.~ ~DSI l ( )ht lcc~) : l l ; l  017  hn  t - lg / l l , - l l t -  t ;7 t2}I I r ) l  ] i l I l a l l s  beDs~, l l  h<.([~f. 211 ~)t~[l s I I IOk i l l~  28.t  19~ ph i l ip  t l l ( , l  [ i~  5t  ()7;~ [1111~ Likll(e[ d + [ l l l l  22 21. t ~n l~,ke  15913.1  clarxl<t1, :  %, at  t80 . t5  q t t  iri l ln cl l~ .21 I IS I ,~ lh l l l a l l? )375 to lh l l l l l l l~  i t l Y ,  l I l a t l t ) l lgk l  -t.t .13.1 qt l  q t t  f i~ ln  21 - I lS , ,~ha  I . , )  elll()k.~ 112){}I  bll  b[i bl l  20  22t i s ,~i la 12)i .121 ~il pat r i ck  t0 . t55  c+)l lst l l l l l} l lo l l  bn  c lgar , . l te  2022d Illtll 113 ~+1~) c l~at+ l l~"  c~l l lpa l lV  :ID [$1)D ~[t+gtt a l l l . r  [ iCtt l l  ~llI,lk? (}llt 20226 al l l , )k(  l  10.1 I i0  (el l l  l l l a lk+ l  36223 [ l l l l~ Ca l l t e  [  ht:gl[ I  2(~ 22{i b[~t 79 .90:1  ?~IN illt+ll+il++t :1t;.22:1 i l l a  [ay~ia l l  +illk~[tl>,ll+e t4)l l lpi l l IV 2( I .22t  ] Top ic  I0 S ignatur . "I~r)ns of Top ic  258 -- Co)nputor  Secur i ty I ~llial /lilt "2Ionia I t  i+/,r al l l  21,QIX "I1 i~ratn  --21o9, X (:+lll l l)l ltOr 115!~ :151 C4, l l lp I l l l  t  ae l  t l l l l y  213331 )e l  I l t t /p l l  [~i() l l   ] l th t )h l t ( l l y   [~  ~5.t v i rus  927.G7-1 ~[ ; idt lgt l , "  s l t l  [ t l  l l l  17~ 5NN I l lh . l l  I lilt) 9R 85, t hacker  867 .377 FOl l lp t l t ,  t  +yS le l l l  1 -16.32~ C,+ltl++]l I l l l iVet~il~,  ~ lad l l  [ t le  7}) IJNI in,) l  rl+ +i+;+~ 2i13.! ) l , .~+-arch c,+ulte[  l ; i2  .l I :i l awte l l c l "  b ,  rk ,   *j~ lal l , ) l  al+,l  ,,. 79.0N [ c , , rn ,  l l  3P+5 6+4 c , : ,ml ,u t ,  r  wrus  12~k033 I~+,++, je t  p tO l , t l L+ io t+ 79 .0~1 un lv+ l? i ty  31)5 .95~ corne l i  U lX iVe le i ty  1(1~4 7-t l  U l ; iV ,q+i )y  ~; radu lx t , . lll 79U~1 +ysl+ l l l  290 .3"17  Iltl(:l,P;ll %t++npl)ll 107 .283 lawt l l l e ,+ l i v+: r tn ( . te  I igt l i () l lal  i][) l[I;~ I / tb , . ra lL : ) ly  2N7 521 in i l i ta ry  t  ( , l l lp , l l . : r  106 .522 l iv~qll l ,) l?" i lu) i ,maL  lubora lo ry  {59195 [ab  225 .51) ;  v i tu~ plo~t< l l l l  1U6 522 c,) l l lp l l I (~r S ,~CUl i ly  eXpet l  66 .19G mecLa ly  128 .515 %vesl ~et l l l a l l  82  2  [0  ~ecu [ i t? ,   cenl{~[  13ethesda  -19423 Top ic  10  S ignature  Ter lns  o f  Top ic  271  So lar  Power I  l l i g ta ln  - -21oqX t i ig t  ~ltn --2logX "Ir i ~;r hi l l  - -21o!~A so la r  -1S- l .315 e,~la~ e l te t l4y  2{Di 521 d iv i~ i ,m Inu l l ip l ,~  acress  31 3-17 ) t lazd i t  :10Pt 0IY) s<,lal l , t lw ,  t  9,1 210  n l , )b i l , :  l , , l , -ph , ,n , . #c iv ic , ,  313 .17 le,) 271; .932  ( h r i~t ia l l  a id  8 f i .211  b l i l l sh  It .cl l l l i l l l )R} g  [ , , l l p  23510 it JtLi It l l l  2.5N.71):"+ l++,a S3Sl,*III 711 5:{5 el l l I} l  he iNht  llXile 23+5111 pax+lh , ,n  2133 81 I ill++tlllt. Ie i t  j ) l l l ) l le  (115;l+i IillllllCilll I lack i l l+;  I l Jd l l l l l l  22i+51(1 i)(~tltld 12 / ,121  i t i , l i un l  p l , , j , . c l  112.697 ~l,~l lal  In r )h i l ,  + sa l ,  l l i te  23  511J t , lw~r  12G.35:1 lei l i  <+, , ,d -  61.~111 ha l ld l le ld  IIled~il," t ,  l eph , , l l , :  23510 [ , , , , k , ,u t  125 .ll3t; scie. l lc, ,  pa lk  ~>.1 NS{) i l l ( ,h i le  ~ate l l l l . v>tetn  23  510 i i l  [ l l l i lSRl  1O9728 ~()llkl t  i l l l t  l  l l t l i l I l , l  51t ~5{} I l l l l t l l lvl i l l  i g id i l ln l  I> l , l j ec t  23 ,510 hc ,ydsh , t l  7N :173 l)p s l l la l  ?+1 ; /17  act iv t -  s+,la[ *ys tern  15673 Tattle 2: Top 10 signat.me t.erm.~; of mfigram, bigram, and trigram for fore" TREe  t.opics. 6.1 Comparing Summary Extraction Effectiveness Using Topic Signatures+ TFII)t",  and Bas(,line Algor i thms In orde)" I() (~vahla(. (~ the (d[+:ct.iv(,im.~s nf l(>l)i(: .~dgna- l;lll(~S llS(~(] ill SlllllIIN/ly (~Xtlit(:t;iOll, W{,  ~ CtIllll)~ll(~ +flit! Sllltllll~tly StHII~011(CS ex(~ract,(~d 1)y the tol) ic si~Ilil[lll0, module+, basulin(. module, and tfidf lnothll(~s with lm- ntan annot,  at(~(l lllo(lo,] Sllllllll}llios. VC III(+~}/SIlI(+ + l;h(; l)crfl)rmanc(~ using a c()ml)ined umasure of lncall (I~) and pr(~cisi(m (P), F. F-score is defined by: I " - -  (1 +H2)Il? where /3-P + I~ t ) 2 7 . ) f~rln fVln ~,, # of  .sc,tcncc.~ c:rtratcd th,t  olso atqwar in. tim model ,s.mn)?lr!l # of  sc+lt(!ncc,s i11 tim nlo,h:l .~um.tav!l # of  ,s(./Itclwcs c:rlv?lclcd t)1,t ll*c .Sll.Slcln rclaticc iml,ortancc of  l~ aml 1: (6) (7) Ve as.~um(~ (,(lual importance of re(:all iIIld preci- sion aim set H to 1 in our (+,Xl)(+rimtml;s. The Imselitm (I)ositi(m) module scores (at:h S(!llt(:llC{} hy its I)osi- ti(>n in the text. The first sent(race gets the high- esc s(:ortL the last S(HIt(H1Co the lowest. The l)as(~liIl(~ method is eXlmCted to lm (.f[ectiv(~ for news geme. The tfidf module assigns a score t.o a tt++rllI ti at:cord- ing to the product; of its flequc, ncy within a dot:- lllll(Hlt .j ( t f i j )  and its illV(~IS(} doctmmnt  t?equoncy (idfi lo.q ,~). N is the total mmfl)or of document.s in the (:()rlms and dfj is the, numl)er of (Io(:HnloAll;.q (:OlH:nining te rm ti. The topic sigjlla(.lll(++ module sciliis each ,q(~llt;(H1C(~: assigning to (ach word that occurs in a topic signa- (ure thu weigh(, of that, keyword in t.hc tol)ic signa- tltltL Eit{h s(++llt(,+ItC(~ Ill(ill l(:c(:ive.q a top ic  s ignature score equal to tlm total of all signature word scores it (:Olllailis, normalizcd 1) 3 the. highest sentence score. This s(:ol( 3 indical.es l;h(~ l(!l(wall(:(~ of l.h(; S(!llt.t~n(:(! to t, lw sigmmlre topic. SU.~[.MAt/IST Inoduced (!xttat:ts of tlm samu l(~xI.q sui)aralely for each ,,lodul0, for a s(~li(,s of ex- tracts ranging from ()cX; to 100% of the. original l;(}xI. Althottgh many rel<want docttments are avaita})l+, for each t01>ic, Ollly SOlll0 o[ [h0111 htlv(~ allSWOl kc!y 499 markut)s. The mnnber of documents with answer keys are listed in the row labeled: "# of Relevant Does Used in Training". To ensure we utilize all the available data and conduct a sound evaluation, we perform a three-fold (:ross validation. We re- serve one-third of documents as test set, use the rest as training set, and ret)eat three times with non- overlapl)ing test set. Furthernmre, we use only uni- gram topic signatures fin" evaluation. The result is shown in Figure 2 and TaMe 3. We find that the topic signature method outperforms the other two methods and the tfidfmethod performs poorly. Among 40 possibh,, test points fl)r four topics with 10% SUmlnary length increment (0% means se- lect at least one sentence) as shown in Table 3, the topic signature method beats the baseline method 34 times. This result is really encouraging and in- dicates that the topic signature method is a worthy addition to a variety of text summarization methods. 6.2 Enriching Topic Signatures Using Existing Ontologies We have shown in the previous sections that topic signatures can be used to al)I)roximate topic iden- tification at the lexieal level. Although the au- tomatically acquired signature terms for a specific topic seem to 1)e bound by unknown relationships as shown in Table 2, it is hard to image how we can enrich the inherent fiat structure of tol)ie signatures as defined in Equation 1 to a construct as complex as a MUC template or script. As discussed in (Agirre et al., 2000), we propose using an existing ontology such as SENSUS (Knight and Luk, 1994) to identify signature term relations. The external hierarchical framework can be used to generalize topic signatures and suggest richer rep- resentations for topic signatures. Automated entity recognizers can be used to (:lassify unknown enti- ties into their appropriate SENSUS concept nodes. We are also investigating other approaches to attto- matieally learn signature term relations. The idea mentioned in this paper is just a starting point. 7 Conc lus ion In this paI)er we l)resented a t)rocedure to automati- (:ally acquire topic signatures and valuated the eflk~c- tiveness of applying tol)i(: signatures to extract ot)i(: relevant senten(:es against two other methods. The tot)ie signature method outt)erforms the baseline and the tfidfmethods for all test topics. Topic signatures can not only recognize related terms (topic identifi- (:ation), but grout) related terms togetlmr under one target concept (topic interpretation). IbI)i(: identi- fication and interpretation are two essential steps in a typical automated text summarization system as we l)resent in Section 3. ]))pic: signatures (:an also been vie.wed as an in- verse process of query expansion. Query expansion intends to alleviate the word mismatch lnoblenl in infornmtion retrieval, since documents are normally written in different vocabulary, ttow to atttomati- (ally identify highly e(nrelated terms and use them to improve information retrieval performance has been a main research issue since late 19611s. Re- cent advances in the query expansion (Xu and Croft, 1996) can also shed some light on the creation of topic signatures. Although we focus the ltse of topic signatures to aid text summarization i this paper, we plan to explore the possibility of applying topic signatures to perform query expansion in the future. The results reported are encouraging enough to allow us to contimm with topic signatures as the ve- hMe for a first approximation to worht knowledge. We are now busy creating a large nmnber of signa- ture.s to overcome the world knowledge acquisition problem and use them in topic interpretation. 8 Acknowledgements YVe thank the anonymous reviewers for very use- tiff suggestions. This work is supported in part by DARPA contract N66001-97-9538. References Eneko .~girre, Olatz Ansa, Edumd Hovy, and David Martinez. Enriching very large ontologies using the www. In Proceedings of the Work,,;hop on Ontology Construction of the European Con- fl:rencc of AI (ECAI). Kenneth Church and Patrick Hanks. Word as- sociation IIOrlllS, mutual information and lexicog- raphy. In Proceedings of the 28th Annual Meeting of the Association for Computational Lingui.vtic.~" (,4CL-90), pages 76~-83. Thomas Cover and Joy A. Thomas. Elcment.~ of Information Theory..John Wiley & Sons. An overview of the FRUMP system. In ~2mdy G. Lehnert and Martin H. Ringle, editors, Strategies for natural language processing, pages 149-76. Lawrence Erlbaum A.s- so(lares. A~i:eurate methods for the statistics of surprise and coincidence. Computa- tional Linguistics, 19:61--74. TextTiling: Segmenting text into nmlti-l)aragraph subtopic passages. Compu- tational Linguistics, 23:33-64. Eduard Hovy and Chin-Yew Lin. Automated text summarization i SUMMAIRIST. In Inder- jeer Mani and Mark T. Maybury, editors, Ad- vances in Automatic 71xxt Summarization, chap- ter 8, pages 81 94. Kevin Knight and Steve K. Luk. Building a large knowledge base for machine translation, ht Proceedings of the Eleventh National Coy@renee on Arti]icial Intelligence (AAAI-9/~). 500 -~  ..~:,., . - -=-"  _..  _ . .&ass 0 50000 n ~ .~ . 1,* +  .~  *+-  . ; -5; , :~:;  . :~.7~.~ ~ ~ ^ - -~- . o 400OO f ,  " +- ~-" + ~ -, "~2x-+, [ ? : [ - ...... ; ""7 ........ 2,=_ ~ 0 =0000 j   +J" J j  1" " .,::iff "4. -a  + -a--  -#. -~-- - .a  ~  . 0 ~o00o d-;9~7~ -7 + 5~:7~:=-+: ; :  ~ . =-~++:7:: ~ -:~ +--~ ....... " ~5_~Ztt::~:ll;: ; i I " , ; . A  / , -?~-  <F" ~. " "~" ~ 257 44" ? o ;oo~ ._~-.c_-__~ / 0 00000 I 000 005 010 015 020 025 030 ,335 040 045 050 055 060 065 070 o75 050 085 090 095 ~00 .~ umrn~i-~ Lenqth Figure 2:  F-measur(: w;. summary length for all fimr topics. ~bi)ic signature cl(mrly outperforin tfidf and baselin(, ex(:ei)t for tit(: case of topic 258 where t)(~rforman(:(; for tim thr(;e methods are roughly equal. I__ I - - - - - -~~g-  lO% I ___~o~a -ao~~a--- -  4o~ I ~0%- -  [  ~o~ [ ~,o~ ~o% I 9o~ I lOO% I [ ~.+,_~.,~dl .... i . :ms o.a-~9 I o..~.o o.aa4 o . ,~:c -  I ...ao=, [ __2 :~r  I~  o.ara oar , ;  I . .a t , ,  I e.a.~w-I +4.58 +7.48 +15.6a +14.17 +8.66 +3. s i~  I -2 ,7d  -2 .19- - [ 257-h , , * , , l i  .... r--- (1.1-}98 {~.15.__.5 I c,,, ,,.is., ".~L I o.,~~--F--,~.~,, I - -o  t,l o.1~, I ?. !s~ [ _,.~r_,a,,r [ -55.11- -38.56 I -".5U ~"> ~".0;   " +   I S ~:    I ~ ~ " " "  I +r  0 ~t  . , 257_,~i,ic.~ig +45.5~ +64.06 +31.88 ~ +20.40 [ +20.60 [ 4_-~01 +12.4&- I14 .24  - O. (h.~] [_ 25u_h~,~.,li . L_  o l  tk_ o 270 I "4-2 ~ *~:~ I ,, ~,r_, L_  "47t_ J  .4 r , ,  1 - - ~ -  1  o.~,__,+~ o s_,Z._J [ 271_l,aseli . I <,at tT_.._,~. :,,~; T--,Ta77--- ..a:~ _L ,, :s:,r, .L .... ~~~~:~- i~-  T -  o.ae~ ] , ) . lO  j _ _  + 4 ~ _ ~ ~ s . ~  +~.a,~ I +~.~o_ l l  0.,~, ] Table 3: F..measule t)erformanc(~ differen(:e compared to 1)aselin(~ nt(:thod in t)ercentage. Cohmms indicate at diffe.rent summary lengths related to fldl length docum(mts. Values in the 1)aselin(,. rows are F-measure s(:ores. Vahms in the tfidf and tot)i(: signatur(~ rows arc i)(.rformmlc(~ increase or (h,.crease divide(l by their (:orr(.,sI)ontling baseline scores and shown in I)er(:(mtag(!. Inderje(?t Mani, David House, Gary KMn, Lyn(~tt(~ ttirschman, Leo ()brst, Thdr6se Firmin, Micha(d Chrzanowski, and Beth Sundheim. The T IPSTER SUMMAC t~xl smmnmiza- tion evaluation final r(:t)ort. %~(:hnical I/,ol)orl; MTR98W0000138, The MITRE Corporation. Christopher Manning and Hinrich Schiitzc. 1999. t}mdatious of Statistical Natural Language Pro~ cessing. Kathh~(m M(:K(!own and l)rag(mfir R. I ladev. I I (  ra t ,  i l l g  S l l l l l l l l ; l l  i ( : s  o f  I l t l l l t ,  i  [ ) l  [~  l l ( !~vs  articles. In hMtu.iet~t Mani and Mark T. Maybury, edi t,ors, Admm.ces in Automatic Text Sv,.mmarization, chapter 24, pagc+s 381 :/89. Ellen Riloff and Jeffrey Lorenzen. Ext:raction- t)a:;e,d text cateI,dorization: Generating donmin- qmcitic role relationships atttonmtically. In Tomek Strzalkowski, editor, Natural Language In- formation, Retrieval. Kluwer Academic Publishc, r.q. An ompirical study of automated dictionary construction for information extraction in three domains. Artificial Intelligence ,Journal, 85, August. Introduction to information extraction. http://www.mu(:.sai(:.(:om. Jinxi Xu and W. Bruc(! Query ex- pal>ion using local and gh)bal document analysis. In lrocee.dings of the 17th Annual International A(JM SIGIR Cot@rence. on Research and Devel- opment in Information l{etrieval, pages 4 -11.
Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles). One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora. Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information. Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc. Such texts are widely available on the Web for many language pairs and domains. They often contain many sentence pairs that are fairly good translations of each other. The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora. However, identifying good translations in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004). We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context. Using this classifier, we extract parallel sentences from very large comparable corpora of newspaper articles. We demonstrate the quality of our A pair of comparable texts. extracted sentences by showing that adding them to the training data of an SMT system improves the system’s performance. We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data. Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains. In such a situation, the test data is in-domain, and the training data is out-of-domain. The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn’t really help the system to produce good translations. What is needed is additional in-domain training data. Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system. We work in the context of Arabic-English and Chinese-English statistical machine translation systems. Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles. In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium:1 In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles. Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings. We are interested in the situation that exists when we need to translate news data but only have UN data available for training. The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set. The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution. We can train baseline systems on UN parallel data (using the data from the first bullet in the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet). In the next section, we give a high-level overview of our parallel sentence extraction system. In Section 3, we describe in detail the core of the system, the parallel sentence classifier. In Section 4, we discuss several data extraction experiments. In Section 5, we evaluate the extracted data by showing that adding it to out-of-domain parallel data improves the in-domain performance of an out-of-domain MT system, and in Section 6, we show that in certain cases, even larger improvements can be obtained by using bootstrapping. In Section 7, we present examples of sentence pairs extracted by our method and discuss some of its weaknesses. Before concluding, we discuss related work. The general architecture of our extraction system is presented in Figure 2. Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1). From each such pair, we generate all possible sentence pairs and pass them through a simple word-overlapbased filter (Section 2.2), thus obtaining candidate sentence pairs. The candidates are presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the sentences in each pair are mutual translations of each other. The resources required by the system are minimal: a bilingual dictionary and a small amount of parallel data (used for training the ME classifier). The dictionaries used in our experiments are learned automatically from (out-of-domain) parallel corpora;2 thus, the only resource used by our system consists of parallel sentences. 2 If such a resource is unavailable, other dictionaries can be used. Our comparable corpus consists of two large, non-parallel, news corpora, one in English and the other in the foreign language of interest (in our case, Chinese or Arabic). The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one. This step of the process emphasizes recall rather than precision. For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents. The subsequent components of the system are robust enough to filter out the extra noise introduced by the selection of additional (possibly bad) English documents. We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan 2001). We first index all the English documents into a database. For each foreign document, we take the top five translations of each of its words (according to our probabilistic dictionary) and create an English language query. The translation probabilities are only used to choose the word translations; they do not appear in the query. We use the query to run TF-IDF retrieval against the database, take the top 20 English documents returned by Lemur, and pair each of them with the foreign query document. This document matching procedure is both slow (it looks at all possible document pairs, so it is quadratic in the number of documents) and imprecise (due to noise in the dictionary, the query will contain many wrong words). We attempt to fix these problems by using the following heuristic: we consider it likely that articles with similar content have publication dates that are close to each other. Thus, each query is actually run only against English documents published within a window of five days around the publication date of the foreign query document; we retrieve the best 20 of these documents. Each query is thus run against fewer documents, so it becomes faster and has a better chance of getting the right documents at the top. Our experiments have shown that the final performance of the system does not depend too much on the size of the window (for example, doubling the size to 10 days made no difference). However, having no window at all leads to a decrease in the overall performance of the system. From each foreign document and set of associated English documents, we take all possible sentence pairs and pass them through a word-overlap filter. The filter verifies that the ratio of the lengths of the two sentences is no greater than two. It then checks that at least half the words in each sentence have a translation in the other sentence, according to the dictionary. Pairs that do not fulfill these two conditions are discarded. The others are passed on to the parallel sentence selection stage. This step removes most of the noise (i.e., pairs of non-parallel sentences) introduced by our recall-oriented document selection procedure. It also removes good pairs that fail to pass the filter because the dictionary does not contain the necessary entries; but those pairs could not have been handled reliably anyway, so the overall effect of the filter is to improve the precision and robustness of the system. However, the filter also accepts many wrong pairs, because the word-overlap condition is weak; for instance, stopwords almost always have a translation on the other side, so if a few of the content For each candidate sentence pair, we need a reliable way of deciding whether the two sentences in the pair are mutual translations. This is achieved by a Maximum Entropy (ME) classifier (described at length in Section 3), which is the core component of our system. Those pairs that are classified as being translations of each other constitute the output of the system. In the Maximum Entropy (ME) statistical modeling framework, we impose constraints on the model of our data by defining a set of feature functions. These feature functions emphasize properties of the data that we believe to be useful for the modeling task. For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel. We therefore define a feature function f (sp), whose value is the word overlap of the sentences in sp. According to the ME principle, the optimal parametric form of the model of our data, taking into account the constraints imposed by the feature functions, is a log linear combination of these functions. Thus, for our classification problem, we have: where ci is the class (c0=”parallel”, c1=”not parallel”), Z(sp) is a normalization factor, and fij are the feature functions (indexed both by class and by feature). The resulting model has free parameters λj, the feature weights. The parameter values that maximize the likelihood of a given training corpus can be computed using various optimization algorithms (see [Malouf 2002] for a comparison of such algorithms). For our particular classification problem, we need to find feature functions that distinguish between parallel and non-parallel sentence pairs. For this purpose, we compute and exploit word-level alignments between the sentences in each pair. A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other. Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990). Since then, they were found useful in many other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question answering [Echihabi and Marcu 2003]). Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus. Each figure contains two alignments. The one on the left is a correct alignment, produced by a human, while the one on the right Alignments between two parallel sentences. was computed automatically. As can be seen from the gloss next to the Arabic words, the sentences in Figure 3 are parallel while the sentences in Figure 4 are not. In a correct alignment between two non-parallel sentences, most words would have no translation equivalents; in contrast, in an alignment between parallel sentences, most words would be aligned. Automatically computed alignments, however, may have incorrect connections; for example, on the right side of Figure 3, the Arabic word issue is connected to the comma; and in Figure 4, the Arabic word at is connected to the English phrase its case to the. Such errors are due to noisy dictionary entries and to Alignments between two non-parallel sentences. shortcomings of the model used to generate the alignments. Thus, merely looking at the number of unconnected words, while helpful, is not discriminative enough. Still, automatically produced alignments have certain additional characteristics that can be exploited. We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to. The presence, in an automatically computed alignment between a pair of sentences, of words of high fertility (such as the Arabic word at in Figure 4) is indicative of non-parallelism. Most likely, these connections were produced because of a lack of better alternatives. Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring. Such a span may contain a few words without any connection (a small percentage of the length of the span), but no word with a connection outside the span. Examples of such spans can be seen in Figure 3: the English strings after saudi mediation failed or to the international court ofjustice together with their Arabic counterparts. Long contiguous connected spans are indicative of parallelism, since they suggest that the two sentences have long phrases in common. And, in contrast, long substrings whose words are all unconnected are indicative of non-parallelism. To summarize, our classifier uses the following features, defined over two sentences and an automatically computed alignment between them. General features (independent of the word alignment): In order to compute word alignments we need a simple and efficient model. We want to align a large number of sentences, with many out-of-vocabulary words, in reasonable time. We also want a model with as few parameters as possible—preferably only wordfor-word translation probabilities. One such model is the IBM Model 1 (Brown et al. 1993). According to this model, given foreign sentence (fj1<=j<=m), English sentence (ei1<=i<=l), and translation probabilities t(fj|ei), the best alignment f → e is obtained by linking each foreign word fj to its most likely English translation argmaxeit(fj|ei). Thus, each foreign word is aligned to exactly one English word (or to a special NULL token). Due to its simplicity, this model has several shortcomings, some more structural than others (see Moore [2004] for a discussion). Thus, we use a version that is augmented with two simple heuristics that attempt to alleviate some of these shortcomings. One possible improvement concerns English words that appear more than once in a sentence. According to the model, a foreign word that prefers to be aligned with such an English word could be equally well aligned with any instance of that word. In such situations, instead of arbitrarily choosing the first instance or a random instance, we attempt to make a ”smarter” decision. First, we create links only for those English words that appear exactly once; next, for words that appear more than once, we choose which instance to link with so that we minimize the number of crossings with already existing links. The second heuristic attempts to improve the choice of the most likely English translation of a foreign word. Our translation probabilities are automatically learned from parallel data, and we learn values for both t(fj|ei) and t(ei|fj). We can therefore decide that the most likely English translation of fj is argmaxei{t(fj|ei),t(ei|fj)}. Using both sets of probabilities is likely to help us make a better-informed decision. Using this alignment strategy, we follow (Och and Ney 2003) and compute one alignment for each translation direction (f - 4e and e -4 f), and then combine them. Och and Ney present three combination methods: intersection, union, and refined (a form of intersection expanded with certain additional neighboring links). Thus, for each sentence pair, we compute five alignments (two modified-IBMModel-1 plus three combinations) and then extract one set of general features and five sets of alignment features (as described in the previous section). We create training instances for our classifier from a small parallel corpus. The simplest way to obtain classifier training data from a parallel corpus is to generate all possible sentence pairs from the corpus (the Cartesian product). This generates 5,0002 training instances, out of which 5,000 are positive (i.e., belong to class ”parallel”) and the rest are negative. One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones. Classifiers trained on such data do not achieve good performance; they generally tend to predict the majority class, i.e., classify most sentences as non-parallel (which has indeed been the case in our experiments). Our solution to this is to downsample, i.e., eliminate a number of (randomly selected) negative instances. Another problem is that the large majority of sentence pairs in the Cartesian product have low word overlap (i.e., few words that are translations of each other). As explained in Section 2 (and shown in Figure 2), when extracting data from a comparable corpus, we only apply the classifier on the output of the word-overlap filter. Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples. We therefore use for training only those pairs from the Cartesian product that are accepted by the word-overlap filter. This has the additional advantage that, since all these pairs have many words in common, the classifier learns to make distinctions that cannot be made based on word overlap alone. To summarize, we prepare our classifier training set in the following manner: starting from a parallel corpus of about 5,000 sentence pairs, we generate all the sentence pairs in the Cartesian product; we discard the pairs that do not fulfill the conditions of the word-overlap filter; if the resulting set is imbalanced, i.e., the ratio of non-parallel to parallel pairs is greater than five, we balance it by removing randomly chosen nonparallel pairs. We then compute word alignments and extract feature values. Using the training set, we compute values for the classifier feature weights using the YASMET4 implementation of the GIS algorithm (Darroch and Ratcliff 1974). Since we are dealing with few parameters and have sufficiently many training instances, using more advanced training algorithms is unlikely to bring significant improvements. We test the performance of the classifier by generating test instances from a different parallel corpus (also around 5,000 sentence pairs) and checking how many of these instances are correctly classified. We prepare the test set by creating the Cartesian product of the sentences in the test parallel corpus and applying the word-overlap filter (we do not perform any balancing). Although we apply the filter, we still conceptually classify all pairs from the Cartesian product in a two-stage classification process: all pairs discarded by the filter are classified as ”non-parallel,” and for the rest, we obtain predictions from the classifier. Since this is how we apply the system on truly unseen data, this is the process in whose performance we are interested. We measure the performance of the classification process by computing precision and recall. Precision is the ratio of sentence pairs correctly judged as parallel to the total number of pairs judged as parallel by the classifier. Recall is the ratio of sentence pairs correctly identified as parallel by the classifier to the total number of truly parallel pairs—i.e., the number of pairs in the parallel corpus used to generate the test instances. Both numbers are expressed as percentages. More formally: let classified parallel be the total number of sentence pairs from our test set that the classifier judged as parallel, classified well be the number of pairs that the classifier correctly judged as parallel, and true parallel be the total number of parallel pairs in the test set. Then: classified parallel true parallel There are two factors that influence a classifier’s performance: dictionary coverage and similarity between the domains of the training and test instances. We performed evaluation experiments to account for both these factors. All our dictionaries are automatically learned from parallel data; thus, we can create dictionaries of various coverage by learning them from parallel corpora of different sizes. We use five dictionaries, learned from five initial out-of-domain parallel corpora, whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English side. Since we want to use the classifier to extract sentence pairs from our in-domain comparable corpus, we test it on instances generated from an in-domain parallel corpus. In order to measure the effect of the domain difference, we use two training sets: one generated from an in-domain parallel corpus and another one from an out-ofdomain parallel corpus. In summary, for each language pair, we use the following corpora: Precision and recall of the Arabic-English classifiers. From each initial, out-of-domain corpus, we learn a dictionary. We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances. We train two classifiers (one on each training set) and evaluate both of them on the test set. The parallel corpora used for generating training and test instances have around 5k sentence pairs each (approximately 150k English tokens), and generate around 10k training instances (for each training set) and 8k test instances. Precision and recall of the Chinese-English classifiers. Figures 5 and 6 show the recall and precision of our classifiers, for both ArabicEnglish and Chinese-English. The results show that the precision of our classification process is robust with respect to dictionary coverage and training domain. Even when starting from a very small initial parallel corpus, we can build a high-precision classifier. Having a good dictionary and training data from the right domain does help though, mainly with respect to recall. The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments. The low recall results are partly due to the word-overlap filter (the first stage of the classification process), which discards many parallel pairs. If we don’t apply the filter before the classifier, the recall results increase by about 20% (with no loss in precision). However, the filter plays a very important role in keeping the extraction pipeline robust and efficient (as shown in Figure 7, the filter discards 99% of the candidate pairs), so this loss of recall is a price worth paying. Classifier evaluations using different subsets of features show that most of the classifier performance comes from the general features together with the alignment features concerning the percentage and number of words that have no connection. However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no The amounts of data processed by our system during extraction from the Chinese-English comparable corpus. longer be accounted for only by counting the linked words; thus, the other features should become more important. The comparable corpora that we use for parallel sentence extraction are collections of news stories published by the Agence France Presse and Xinhua News agencies. They are parts of the Arabic, English, and Chinese Gigaword corpora which are available from the Linguistic Data Consortium. From these collections, for each language pair, we create an in-domain comparable corpus by putting together articles coming from the same agency and the same time period. Table 1 presents in detail the sources and sizes of the resulting comparable corpora. The remainder of the section presents the various data sets that we extracted automatically from these corpora, under various experimental conditions. In the experiments described in Section 3.4, we started out with five out-of-domain initial parallel corpora of various sizes and obtained five dictionaries and five out-ofdomain trained classifiers (per language pair). We now plug in each of these classifiers (and their associated dictionaries) in our extraction system (Section 2) and apply it to our comparable corpora. We thus obtain five Arabic-English and five Chinese-English extracted corpora. Note that in each of these experiments the only resource used by our system is the initial, out-of-domain parallel corpus. Thus, the experiments fit in the framework of interest described in Section 1, which assumes the availability of (limited amounts of) out-of-domain training data and (large amounts of) in-domain comparable data. Table 2 shows the sizes of the extracted corpora for each initial corpus size, for both Chinese-English and Arabic-English. As can be seen, when the initial parallel corpus is very small, the amount of extracted data is also quite small. This is due to the low coverage of the dictionary learned from that corpus. Our candidate pair selection step (Section 2.2) discards pairs with too many unknown (or unrelated) words, according to the dictionary; thus, only few sentences fulfill the word-overlap condition of our filter. As mentioned in Section 1, our goal is to use the extracted data as additional MT training data and obtain better translation performance on a given in-domain MT test set. A simple way of estimating the usefulness of the data for this purpose is to measure its coverage of the test set, i.e., the percentage of running n-grams from the test corpus that are also in our corpus. Tables 3 and 4 present the coverage of our extracted corpora. For each initial corpus size, the first column shows the coverage of that initial corpus, and the second column shows the coverage of the initial corpus plus the extracted corpus. Each cell contains four numbers that represent the coverage with respect to unigrams, bigrams, trigrams, and 4-grams. The numbers show that unigram coverage depends only on the size of the corpus (and not on the domain), but for longer n-grams, our in-domain extracted data brings significant improvements in coverage. The extraction experiments from the previous section are controlled experiments in which we only use limited amounts of parallel data for our extraction system. In this section, we describe experiments in which the goal is to assess the applicability of our method to data that we mined from the Web. We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently. In order to get as many articles as possible, we used the web site’s search engine to get lists of articles and their URLs, and then crawled those lists. We used the AgentBuilder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling. The tool can be programmed to automatically initiate searches with different parameters and to identify and extract the desired article URLs (as well as other information such as dates and titles) from the result pages. Table 5 shows the sources, time periods, and size of the datasets that we downloaded. For the extraction experiments, we used dictionaries of high coverage, learned from all our available parallel training data. The sizes of these training corpora, measured in number of English tokens, are as follows: We applied our extraction method on both the LDC-released Gigaword corpora and the Web-downloaded comparable corpora. For each language pair, we used the highest precision classifier from those presented in Section 3.4. In order to obtain data of higher quality, we didn’t use all the sentences classified as parallel, but only those for which the probability computed by our classifier was higher than 0.70. Table 6 shows the amounts of extracted data, measured in number of English tokens. For ArabicEnglish, we were able to extract from the Gigaword corpora much more data than in our previous experiments (see Table 2), clearly due to the better dictionary. For ChineseEnglish, there was no increase in the size of extracted data (although the amount from Table 6 is smaller than that from Table 2, it counts only sentence pairs extracted with confidence higher than 0.70). In the previous section, we measured, for our training corpora, their coverage of the test set (Tables 3 and 4). We repeated the measurements for the training data from Table 6 and obtained very similar results: using the additional extracted data improves coverage, especially for longer n-grams. To give the reader an idea of the amount of data that is funneled through our system, we show in Figure 7 the sizes of the data processed by each of the system’s components during extraction from the Gigaword and Web-based Chinese-English comparable corpora. We use a dictionary learned from a parallel corpus on 190M English tokens and a classifier trained on instances generated from a parallel corpus of 220k English tokens. We start with a comparable corpus consisting of 500k Chinese articles and 600k English articles. The article selection step (Section 2.1) outputs 7.5M similar article pairs; from each article pair we generate all possible sentence pairs and obtain 2,400M pairs. Of these, less than 1% (17M) pass the candidate selection stage (Section 2.2) and are presented to the ME classifier. The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7). The figure also presents, in the lower part, the parameters that control the filtering at each stage. the particular sentence pair to be parallel; the higher the value, the higher the classifier’s confidence. Thus, in order to obtain higher precision, we can choose to define as parallel only those pairs for which the classifier probability is above a certain threshold. In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7. Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system. Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance. We first evaluate the extracted corpora presented in Section 4.1. The extraction system used to obtain each of those corpora made use of a certain initial out-of-domain parallel corpus. We train a Baseline MT system on that initial corpus. We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus. In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data. For each initial corpus, we use the same amount of human-translated data as there is extracted data (see Table 2). Thus, for each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline, PlusExtracted, and UpperBound. All our MT systems were trained using a variant of the alignment template model described in (Och 2003). Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system. This ensured that any differences in performance are caused only by differences in the training data. The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations. Figures 8 and 9 show the BLEU scores obtained by our MT systems. The 95% confidence intervals of the scores computed by bootstrap resampling (Koehn 2004) are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for Chinese-English. As the results show, the automatically extracted additional training data yields significant improvements in performance over most initial training corpora for both language pairs. At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data. And, as can be expected, the impact of the extracted data decreases as the size of the initial corpus increases. In order to check that the classifier really does something important, we performed a few experiments without it. After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus. The resulting BLEU scores were practically the same as the baseline; thus, our classifier does indeed help to discover higher-quality parallel data. We also measured the MT performance impact of the extracted corpora described in Section 4.2. We trained a Baseline MT system on all our available (in-domain and MT performance improvements for Arabic-English. out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the extracted in-domain data. Clearly, we have access to no UpperBound system in this case. The results are presented in the first two rows of Table 7. Adding the extracted corpus lowers the score for the Arabic-English system and improves the score for the Chinese-English one; however, none of the differences are statistically significant. Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact. In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data. The BLEU scores obtained by the systems we trained on them are presented in the third row of Table 7. For comparison purposes, the last line of the table shows the scores of systems trained on 10M English tokens of outof-domain data. As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size. It’s true that this is not a fair comparison, since the extracted corpora were obtained using all our available parallel data. The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation. As can be seen from Table 2, the amount of data we can extract from our comparable corpora is adversely affected by poor dictionary coverage. Thus, if we start with very little parallel data, we do not make good use of the comparable corpora. One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again. Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004). We performed bootstrapping iterations starting from two very small corpora: 100k English tokens and 1M English tokens, respectively. After each iteration, we trained MT performance improvements for Chinese-English. (and evaluated) an MT system on the initial data plus the data extracted in that iteration. We did not use any of the data extracted in previous iterations since it is mostly a subset of that extracted in the current iteration. We iterated until there were no further improvements in MT performance on our development data. Figures 10 and 11 show the sizes of the data extracted at each iteration, for both initial corpus sizes. Iteration 0 is the one that uses the dictionary learned from the initial corpus. Starting with 100k words of parallel data, we eventually collect 20M words of in-domain Arabic-English data and 90M words of in-domain Chinese-English data. Figures 12 and 13 show the BLEU scores of these MT systems. For comparison purposes, we also plotted on each graph the performance of our best MT system for that language pair, trained on all our available parallel data (Table 7). As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores. Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7–10 BLEU points of systems trained on parallel corpora of more than 100M English tokens. This shows that using our method, a good-quality MT system can be built from very little parallel data and a large amount of comparable, non-parallel data. We conclude the description of our method by presenting a few sentence pairs extracted by our system. We chose the examples by looking for cases when a given foreign sentence was judged parallel to several different English sentences. Figures 14 and 15 show the foreign sentence in Arabic and Chinese, respectively, followed by a human-produced translation in bold italic font, followed by the automatically extracted matching English sentences in normal font. The sentences are picked from the data sets presented in Section 4.2. The examples reveal the two main types of errors that our system makes. The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7. The second concerns pairs in which the two sentences convey different amounts of information. In such pairs, one of the sentences contains a transSizes of the Chinese-English corpora extracted using bootstrapping, in millions of English tokens. BLEU scores of the Arabic-English MT systems using bootstrapping. lation of the other, plus additional (often quite long) phrases (Figure 15, examples 1 and 5). These errors are caused by the noise present in the automatically learned dictionaries and by the use of a weak word alignment model for extracting the classifier BLEU scores of the Chinese-English MT systems using bootstrapping. features. In an automatically learned dictionary, many words (especially the frequent, non-content ones) will have a lot of spurious translations. The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words. Alignments computed using this model and a noisy, automatically learned, dictionary will contain many incorrect links. Thus, if two sentences share several content words, these incorrect links together with the correct links between the common content words will yield an alignment good enough to make the classifier judge the sentence pair as parallel. The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6. The sentences in that example are tables of soccer team statistics. They are judged parallel because corresponding digits align to each other, and according to our dictionary, the Arabic word for “Mexico” can be translated as any of the country names listed in the example. These examples also show that the problem of finding only true translation pairs is hard. Two sentences may share many content words and yet express different meanings (see Figure 14, example 1). However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance. While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004). We are aware of only three previous efforts aimed at discovering parallel sentences. Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus. Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stories from different sources and time periods. The first two systems extend algorithms designed to perform sentence alignment of parallel texts. They start by attempting to identify similar article pairs from the two corpora. Then they treat each of those pairs as parallel texts and align their sentences by defining a sentence pair similarity score and use dynamic programming to find the least-cost alignment over the whole document pair. In the article pair selection stage, the researchers try to identify, for an article in one language, the best matching article in the other language. Zhao and Vogel (2002) measure article similarity by defining a generative model in which an English story generates a Chinese story with a given probability. Utiyama et al. (2003) use the BM25 (Robertson and Walker 1994) similarity measure. The two works also differ in the way they define the sentence similarity score. Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model. Utiyama et al. (2003) define a score based on word overlap (i.e., number of word pairs from the two sentences that are translations of each other), which also includes the similarity score of the article pair from which the sentence pair originates. The performance of these approaches depends heavily on the ability to reliably find similar document pairs. Moreover, comparable article pairs, even those similar in content, may exhibit great differences at the sentence level (reorderings, additions, etc). Therefore, they pose hard problems for the dynamic programming alignment approach. In contrast, our method is more robust. The document pair selection part plays a minor role; it only acts as a filter. We do not attempt to find the best-matching English document for each foreign one, but rather a set of similar documents. And, most importantly, we are able to reliably judge each sentence pair in isolation, without need for context. On the other hand, the dynamic programming approach enables discovery of many-to-one sentence alignments, whereas our method is limited to finding one-toone alignments. The approach of Fung and Cheung (2004) is a simpler version of ours. They match each foreign document with a set of English documents, using a threshold on their cosine similarity. Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel. Using the set of extracted sentences, they learn a new dictionary, try to extend their set of matching document pairs (by looking for other documents that contain these sentences), and iterate. The evaluation methodologies of these previous approaches are less direct than ours. Utiyama et al. (2003) evaluate their sentence pairs manually; they estimate that about 90% of the sentence pairs in their final corpus are parallel. Fung and Cheung (2004) also perform a manual evaluation of the extracted sentences and estimate their precision to be 65.7% after bootstrapping. In addition, they also estimate the quality of a lexicon automatically learned from those sentences. Zhao and Vogel (2002) go one step further and show that the sentences extracted with their method improve the accuracy of automatically computed word alignments, to an F-score of 52.56% over a baseline of 46.46%. In a subsequent publication, Vogel (2003) evaluates these sentences in the context of an MT system and shows that they bring improvement under special circumstances (i.e., a language model constructed from reference translations) designed to reduce the noise introduced by the automatically extracted corpus. We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing. Moreover, we show that our approach works even when only a limited amount of initial parallel data (i.e., a low-coverage dictionary) is available. The problem of aligning sentences in comparable corpora was also addressed for monolingual texts. Barzilay and Elhadad (2003) present a method of aligning sentences in two comparable English corpora for the purpose of building a training set of text-totext rewriting examples. Monolingual parallel sentence detection presents a particular challenge: there are many sentence pairs that have low lexical overlap but are nevertheless parallel. Therefore pairs cannot be judged in isolation, and context becomes an important factor. Barzilay and Elhadad (2003) make use of contextual information by detecting the topical structure of the articles in the two corpora and aligning them at paragraph level based on the topic assigned to each paragraph. Afterwards, they proceed and align sentences within paragraph pairs using dynamic programming. Their results show that both the induced topical structure and the paragraph alignment improve the precision of their extraction method. A line of research that is both complementary and related to ours is that of Resnik and Smith (2003). Their STRAND Web-mining system has a purpose that is similar to ours: to identify translational pairs. However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences. Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs. Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method. The most important feature of our parallel sentence selection approach is its robustness. Comparable corpora are inherently noisy environments, where even similar content may be expressed in very different ways. Moreover, out-of-domain corpora introduce additional difficulties related to limited dictionary coverage. Therefore, the ability to reliably judge sentence pairs in isolation is crucial. Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms. The computational processes involved in our system are quite modest. All the operations necessary for the classification of a sentence pair (filter, word alignment computation, and feature extraction) can be implemented efficiently and scaled up to very large amounts of data. The task can be easily parallelized for increased speed. For example, extracting data from 600k English documents and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors. The data that we extract is useful. Its impact on MT performance is comparable to that of human-translated data of similar size and domain. Thus, although we have focused our experiments on the particular scenario where there is little in-domain training data available, we believe that our method can be useful for increasing the amount of training data, regardless of the domain of interest. As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available. By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system. We suspect our approach can be used on comparable corpora coming from any domain. The only domain-dependent element of the system is the date window parameter of the article selection stage (Figure 7); for other domains, this can be replaced with a more appropriate indication of where the parallel sentences are likely to be found. For example, if the domain were that of technical manuals, one would cluster printer manuals and aircraft manuals separately. It is important to note that our work assumes that the comparable corpus does contain parallel sentences (which is the case for our data). Whether this is true for comparable corpora from other domains is an empirical question outside the scope of this article; however, both our results and those of Resnik and Smith (2003) strongly indicate that good data is available on the Web. Lack of parallel corpora is a major bottleneck in the development of SMT systems for most language pairs. The method presented in this paper is a step towards the important goal of automatic acquisition of such corpora. Comparable texts are available on the Web in large quantities for many language pairs and domains. In this article, we have shown how they can be efficiently mined for parallel sentences. This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276. The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc). We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments. Any remaining errors are of course our own.
Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950s. Sense disambiguation is an &quot;intermediate task&quot; (Wilks and Stevenson 1996), which is not an end in itself, but rather is necessary at one level or another to accomplish most natural language processing tasks. It is obviously essential for language understanding applications, such as message understanding and man-machine communication; it is at least helpful, and in some instances required, for applications whose aim is not language understanding: analysis is to analyze the distribution of predefined categories of words—i.e., words indicative of a given concept, idea, theme, etc.—across a text. The need for sense disambiguation in such analysis, in order to include only those instances of a word in its proper sense, has long been recognized (see, for instance, Stone et al. [1966], Stone [1969], Kelly and Stone [1975]; for a more recent discussion see Litowski [1997]). and is masculine in the former sense, feminine in the latter) to properly tag it as a masculine noun. Sense disambiguation is also necessary for certain syntactic analyses, such as prepositional phrase attachment (Jensen and Binot 1987; Whittemore, Ferrara, and Brunner 1990; Hindle and Rooth 1993), and, in general, restricts the space of competing parses (Alshawi and Carter 1994). The problem of word sense disambiguation (WSD) has been described as &quot;AI-complete,&quot; that is, a problem which can be solved only by first resolving all the difficult problems in artificial intelligence (Al), such as the representation of common sense and encyclopedic knowledge. The inherent difficulty of sense disambiguation was a central point in Bar-Hillel's well-known treatise on machine translation (Bar-Hillel 1960), where he asserted that he saw no means by which the sense of the word pen in the sentence The box is in the pen could be determined automatically. Bar-Hillel's argument laid the groundwork for the ALPAC report (ALPAC 1966), which is generally regarded as the direct cause for the abandonment of most research on machine translation in the early 1960s. At about the same time, considerable progress was being made in the area of knowledge representation, especially the emergence of semantic networks, which were immediately applied to sense disambiguation. Work on word sense disambiguation continued throughout the next two decades in the framework of AI-based natural language understanding research, as well as in the fields of content analysis, stylistic . and literary analysis, and information retrieval. In the past ten years, attempts to automatically disambiguate word senses have multiplied, due, like much other similar activity in the field of computational linguistics, to the availability of large amounts of machine-readable text and the corresponding development of statistical methods to identify and apply information about regularities in this data. Now that other problems amenable to these methods, such as part-of-speech disambiguation and alignment of parallel translations, have been fairly thoroughly addressed, the problem of word sense disambiguation has taken center stage, and it is frequently cited as one of the most important problems in natural language processing research today. Given the progress that has been recently made in WSD research and the rapid development of methods for solving the problem, it is appropriate at this time to stand back and assess the state of the field and to consider the next steps that need to be taken. To this end, this paper surveys the major, well-known approaches to word sense disambiguation and considers the open problems and directions of future research. In general terms, word sense disambiguation involves the association of a given word in a text or discourse with a definition or meaning (sense) which is distinguishable from other meanings potentially attributable to that word. The task therefore necessarily involves two steps: (1) the determination of all the different senses for every word relevant (at least) to the text or discourse under consideration; and (2) a means to assign each occurrence of a word to the appropriate sense. Much recent work on WSD relies on predefined senses for step (1), including: The precise definition of a sense is, however, a matter of considerable debate within the community. The variety of approaches to defining senses has raised concern about the comparability of much WSD work, and given the difficulty of the problem of sense definition, no definitive solution is likely to be found soon (see Section 3.2). However, since the earliest days of WSD work, there has been general agreement that the problems of morpho-syntactic disambiguation and sense disambiguation can be disentangled (see, e.g., Kelly and Stone [19751). That is, for homographs with different parts of speech (e.g., play as a verb and noun), morphosyntactic disambiguation accomplishes sense disambiguation, and therefore (especially since the development of reliable part-of-speech taggers), WSD work has focused largely on distinguishing senses among homographs belonging to the same syntactic category. Step (2), the assignment of words to senses, is accomplished by reliance on two major sources of information: All disambiguation work involves matching the context of the instance of the word to be disambiguated with either information from an external knowledge source (knowledge-driven WSD), or information about the contexts of previously disambiguated instances of the word derived from corpora (data-driven or corpus-based WSD). Any of a variety of association methods is used to determine the best match between the current context and one of these sources of information, in order to assign a sense to each word occurrence. The following sections survey the approaches applied to date. The first attempts at automated sense disambiguation were made in the context of machine translation (MT). In his famous memorandum (available mimeographed in 1949, but not printed until 1955) Weaver discusses the need for WSD in machine translation and outlines the basis of an approach to WSD that underlies all subsequent work on the topic: If one examines the words in a book, one at a time as through an opaque mask with a hole in it one word wide, then it is obviously impossible to determine, one at a time, the meaning of the words.... But if one lengthens the slit in the opaque mask, until one can see not only the central word in question but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word.... The practical question is: &quot;What minimum value of N will, at least in a tolerable fraction of cases, lead to the correct choice of meaning for the central word?&quot; (1955, 20) A well-known early experiment by Kaplan (1950) attempted to answer this question at least in part, by presenting ambiguous words in their original context and in a variant context providing one or two words on either side to seven translators. Kaplan observed that sense resolution given two words on either side of the word was not significantly better or worse than when given the entire sentence. The same phenomenon has been reported by several researchers since Kaplan's work appeared: e.g., Masterman (1962), Koutsoudas and Korthage (1956) on Russian, and Gougenheim and Michea (1961) and Choueka and Lusignan (1985) on French. Reifler 's (1955) &quot;semantic coincidences&quot; between a word and its context quickly became the determining factor in WSD. The complexity of the context, and in particular the role of syntactic relations, was also recognized; for example, Reifler (1955) says: Grammatical structure can also help disambiguate, as, for instance, the word keep, which can be disambiguated by determining whether its object is gerund (He kept eating), adjectival phrase (He kept calm), or noun phrase (He kept a record). The goal of MT was initially modest, focused primarily on the translation of technical texts and in all cases dealing with texts from particular domains. Weaver discusses the role of the domain in sense disambiguation, making a point that was reiterated several decades later by Gale, Church, and Yarowsky (1992c): In mathematics, to take what is probably the easiest example, one can very nearly say that each word, within the general context of a mathematical article, has one and only one meaning. (1955, 20) Following directly from this observation, much effort in the early days of machine translation was devoted to the development of specialized dictionaries or &quot;microglossaries&quot; (Oswald 1952, 1957; Oswald and Lawson 1953; Oettinger 1955; Dostert 1955; Gould 1957; Panov 1960). Such microglossaries contain only the meaning of a given word relevant for texts in a particular domain of discourse; e.g., a microglossary for the domain of mathematics would contain only the relevant definition of triangle, and not the definition of triangle as a musical instrument. The need for knowledge representation for WSD was also acknowledged from the outset: Weaver concludes by noting the &quot;tremendous amount of work [needed] in the logical structure of languages&quot; (1995, 23). Several researchers attempted to devise Ide and Veronis Introduction an &quot;interlingua&quot; based on logical and mathematical principles that would solve the disambiguation problem by mapping words in any language to a common semantic/conceptual representation. Among these efforts, those of Richens and Masterman eventually led to the notion of the &quot;semantic network&quot; (Richens [1958], Masterman [1962]; see Section 2.2.1); following on this, the first machine-implemented knowledge base was constructed from Roget's Thesaurus (Masterman 1957). Masterman applied this knowledge base to the problem of WSD: in an attempt to translate Virgil's Georgics by machine, she looked up, for each Latin word stem, the translation in a Latin-English dictionary and then looked up this word in the word-to-head index of Roget's. In this way, each Latin word stem was associated with a list of Roget head numbers associated with its English equivalents. The numbers for words appearing in the same sentence were then examined for overlaps. Finally, English words appearing under the multiply-occurring head categories were chosen for the translation.' Masterman's methodology is strikingly similar to that underlying much of the knowledge-based WSD accomplished recently (see Section 2.3). It is interesting to note that Weaver's text also outlined the statistical approach to language analysis prevalent now, nearly fifty years later: This approach brings into the foreground an aspect of the matter that probably is absolutely basic—namely, the statistical character of the problem.... And it is one of the chief purposes of this memorandum to emphasize that statistical semantic studies should be undertaken, as a necessary primary step. (1955, 22) Several authors followed this approach in the early days of machine translation (e.g., Richards 1953; Yngve 1955; Parker-Rhodes 1958). Estimations of the degree of polysemy in texts and dictionaries were made: Harper, working on Russian texts, determined the number of polysemous words in an article on physics to be approximately 30% (Harper 1957a) and 43% in another sample of scientific writing (Harper 1957b); he also found that Callaham's Russian-English dictionary provides, on average, 8.6 English equivalents for each Russian word, of which 5.6 are quasi-synonyms, thus yielding approximately three distinct English equivalents for each Russian word. Bel'skaja (1957) reports that in the first computerized Russian dictionary, 500 out of 2,000 words are polysemous. Pimsleur (1957) introduced the notion of levels of depth for a translation: level 1 uses the most frequent equivalent (e.g., German schwer heavy), producing a text where 80% of the words are correctly translated; level 2 distinguishes additional meanings (e.g., schwer = difficult), producing a translation which is 90% correct; etc. Although the terminology is different, this is very similar to the notion of baseline tagging used in modern work (see, e.g., Gale, Church, and Yarowsky [1992b]). A convincing implementation of many of these ideas was made several years later, paradoxically at the moment when MT began its decline. Madhu and Lytle (1965), working from the observation that domain constrains sense, calculated sense frequency for texts in different domains and applied a Bayesian formula to determine the probability of each sense in a given context—a technique similar to that applied in much later work and which yielded a similar 90% correct disambiguation result (see Section 2.4). The striking fact about this early work on WSD is the degree to which the fundamental problems and approaches to the problem were foreseen and developed at that time. However, without large-scale resources, most of these ideas remained untested and to a large extent, forgotten, until several decades later. Al methods began to flourish in the early 1960s and began to attack the problem of language understanding. As a result, WSD in Al work was typically accomplished in the context of larger systems intended for full language understanding. In the spirit of the times, such systems were almost always grounded in some theory of human language understanding that they attempted to model, and often involved the use of detailed knowledge about syntax and semantics to perform their task, which was exploited for WSD. in the late 1950s and were immediately applied to the problem of representing word meanings.2 Masterman (1962), working in the area of machine translation, used a semantic network to derive the representation of sentences in an interlingua comprised of fundamental language concepts; sense distinctions are implicitly made by choosing representations that reflect groups of closely related nodes in the network. She developed a set of 100 primitive concept types (THING, DO, etc. ), in terms of which her group built a 15,000-entry concept dictionary, where concept types are organized in a lattice with inheritance of properties from superconcepts to subconcepts. Building on this and on work on semantic networks by Richens (1958), Quillian (1961, 1962a, 1962b, 1967, 1968, 1969) built a network that includes links among words (tokens) and concepts (types), in which links are labeled with various semantic relations or simply indicate associations between words. The network is created starting from dictionary definitions, but is enhanced by human knowledge that is hand-encoded. When two words are presented to the network, Quillian's program simulates the gradual activation of concept nodes along a path of links originating from each input word by means of marker passing; disambiguation is accomplished because only one concept node associated with a given input word is likely to be involved in the most direct path found between the two input words. Quillian's work informed later dictionary-based approaches to WSD (see Section 2.3.1). Subsequent AI-based approaches exploited the use of frames containing information about words and their roles and relations to other words in individual sentences. For example, Hayes (1976, 1977a, 1977b, 1978) uses a combination of a semantic network and case frames. The network consists of nodes representing noun senses and links represented by verb senses; case frames impose IS-A and PART-OF relations on the network. As in Quillian's system, the network is traversed to find chains of connections between words. Hayes work shows that homonyms can be fairly accurately disambiguated using this approach, but it is less successful for other kinds of polysemy. Hirst (1987) also uses a network of frames and, again following Quillian, marker passing to find minimum-length paths of association between frames for senses of words in context in order to choose among them. He introduces &quot;polaroid words,&quot; a mechanism which progressively eliminates inappropriate senses based on syntactic evidence provided by the parser, together with semantic relations found in the frame network. Eventually only one sense remains; however, Hirst reports that in cases where some word (including words other than the target) in the sentence is used metaphorically, metonymically, or in an unknown sense, the polaroids often end by eliminating all possible senses, and fail. Wilks' preference semantics ([1968, 1969, 1973, 1975a, 1975b, 1975c, 1975d]; see the survey by Wilks and Fass [1990]), which uses Masterman's primitives, is essentially a case-based approach to natural language understanding and one of the first specifically designed to deal with the problem of sense disambiguation. Preference semantics specifies selectional restrictions for combinations of lexical items in a sentence that can be relaxed when a word with the preferred restrictions does not appear, thus enabling, especially, the handling of metaphor (as in My car drinks gasoline, where the restrictions on drink prefer an animate subject but allow an inanimate one). Boguraev (1979) shows that preference semantics is inadequate to deal with polysemous verbs and attempts to improve on Wilks' method by using a combination of evidence, including selectional restrictions, preferences, case frames, etc. He integrates semantic disambiguation with structural disambiguation to enable judgments about the semantic coherence of a given sense assignment. Like many other systems of the era, these systems are sentencebased and do not account for phenomena at other levels of discourse, such as topical and domain information. The result is that some kinds of disambiguation are difficult or impossible to accomplish. A rather different approach to language understanding, which contains a substantial sense discrimination component, is the Word Expert Parser (Small 1980, 1983; Small and Reiger 1982; Adriaens 1986, 1987, 1989; Adriaens and Small 1988). The approach derives from the somewhat unconventional theory that human knowledge about language is organized primarily as knowledge about words rather than rules. Their system models what its authors feel is the human language understanding process: a co-ordination of information exchange among word experts about syntax and semantics as each determines its involvement in the environment under question. Each expert contains a discrimination net for all senses of the word, which is traversed on the basis of information supplied by the context and other word experts, ultimately arriving at a unique sense, which is then added to a semantic representation of the sentence. The well-known drawback of the system is that the word experts need to be extremely large and complex to accomplish the goal, which is admittedly greater than sense disambiguation.' Dahlgren's (1988) language understanding system includes a sense disambiguation component that uses a variety of types of information: fixed phrases, syntactic information (primarily, selectional restrictions), and commonsense reasoning. The reasoning module, because it is computationally intensive, is invoked only in cases where the other two methods fail to yield a result. Although her original assumption was that much disambiguation could be accomplished based on paragraph topic, she found that half of the disambiguation was actually accomplished using fixed phrase and syntactic information, while the other half was accomplished using commonsense reasoning. Reasoning often involves traversing an ontology to find common ancestors for words in context; her work anticipates Resnik's (1993a, 1993b, 1995a) results by determining that ontological similarity, involving a common ancestor in the ontology, is a powerful disambiguator. She also notices that verb selectional restrictions are an lished that semantic priming—a process in which the introduction of a certain concept will influence and facilitate the processing of subsequently introduced concepts that are semantically related—plays a role in disambiguation by humans (see, e.g., Meyer and Schvaneveldt [19711). This idea is realized in spreading activation models (see Collins and Loftus [1975]; Anderson [1976, 1983]), where concepts in a semantic network are activated upon use, and activation spreads to connected nodes. Activation is weakened as it spreads, but certain nodes may receive activation from several sources and be progressively reinforced. McClelland and Rumelhart (1981) added to the model by introducing the notion of inhibition among nodes, where the activation of a node might suppress, rather than activate, certain of its neighbors (see also Feldman and Ballard [1982]). Applied to lexical disambiguation, this approach assumes that activating a node corresponding to, say, the concept THROW will activate the &quot;physical object&quot; sense of ball, whose activation would in turn inhibit the activation of other senses of ball, such as &quot;social event.&quot; Quillian's semantic network, described above, is the earliest implementation of a spreading activation network used for word sense disambiguation. A similar model is implemented by Cottrell and Small (1983); see also Cottrell (1985). In both of these models, each node in the network represents a specific word or concept.' Waltz and Pollack (1985) and Bookman (1987) hand-encode sets of semantic &quot;microfeatures,&quot; corresponding to fundamental semantic distinctions (animate/inanimate, edible/inedible, threatening/safe, etc. ), characteristic durations of events (second, minute, hour, day, etc. ), locations (city, country, continent, etc. ), and other similar distinctions, in their networks. In Waltz and Pollack (1985), sets of microfeatures have to be manually primed by a user to activate a context for disambiguating a subsequent input word, but Bookman (1987) describes a dynamic process in which the microfeatures are automatically activated by the preceding text, thus acting as a short-term context memory. In addition to these local models (i.e., models in which one node corresponds to a single concept), distributed models have also been proposed (see, for example, Kawamoto [1988]). However, whereas local models can be constructed a priori, distributed models require a learning phase using disambiguated examples, which limits their practicality. The difficulty of hand-crafting the knowledge sources required for AI-based systems restricted them to &quot;toy&quot; implementations handling only a tiny fraction of the language. Consequently, disambiguation procedures embedded in such systems are most usually tested on only a very small test set in a limited context (most often, a single sentence), making it impossible to determine their effectiveness on real texts. For less obvious reasons, many of the AI-based disambiguation results involve highly ambiguous words and fine sense distinctions (e.g., ask, idea, hand, move, use, work, etc.) and unlikely test sentences (The astronomer married the star), which make the results even less easy to evaluate in the light of the now-known difficulties of discriminating even gross sense distinctions. The AI-based work of the 1970s and 1980s was theoretically interesting but not at all practical for language understanding in any but extremely limited domains. A significant roadblock to generalizing WSD work was the difficulty and cost of hand-crafting the enormous amounts of knowledge required for WSD: the so-called &quot;knowledge acquisition bottleneck&quot; (Gale, Church, and Yarowsky 1993). Work on WSD reached a turning point in the 1980s when large-scale lexical resources, such as dictionaries, thesauri, and corpora, became widely available. Attempts were made to automatically extract knowledge from these sources (Sections 2.3.1 and 2.3.2) and, more recently, to construct large-scale knowledge bases by hand (Section 2.3.3). A corresponding shift away from methods based in linguistic theories and towards empirical methods also occurred at this time, as well as a decrease in emphasis on do-all systems in favor of &quot;intermediate&quot; tasks such as WSD. Heidon 1985; Markowitz, Ahlswede, and Evens 1986; Byrd et al. 1987; Nakamura and Nagao 1988; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1990). This work contributed significantly to lexical semantic studies, but it appears that the initial goal—the automatic extraction of large knowledge bases—was not fully achieved: the only currently widely available large-scale lexical knowledge base (WordNet, see below) was created by hand. We have elsewhere demonstrated the difficulties of automatically extracting relations as simple as hyperonymy (Wronis and Ide 1991; Ide and Wronis 1993a, 1993b), in large part due to the inconsistencies in dictionaries themselves (well-known to lexicographers, cf. Atkins and Levin [19881, Kilgarriff [1994]) as well as the fact that dictionaries are created for human use, and not for machine exploitation. Despite its shortcomings, the machine-readable dictionary provides a ready-made source of information about word senses and therefore rapidly became a staple of WSD research. The methods employed attempt to avoid the problems cited above by using the text of dictionary definitions directly, together with methods sufficiently robust to reduce or eliminate the effects of a given dictionary's inconsistencies. All of these methods (and many of those cited elsewhere in this paper) rely on the notion that the most plausible sense to assign to multiple co-occurring words is the one that maximizes the relatedness among the chosen senses. Lesk (1986) created a knowledge base that associated with each sense in a dictionary a &quot;signature&quot;6 composed of the list of words appearing in the definition of that sense. Disambiguation was accomplished by selecting the sense of the target word whose signature contained the greatest number of overlaps with the signatures of neighboring words in its context. The method achieved 50-70% correct disambiguation, using a relatively fine set of sense distinctions such as those found in a typical learner's dictionary. Lesk's method is very sensitive to the exact wording of each definition: the presence or absence of a given word can radically alter the results. However, Lesk's method has served as the basis for most subsequent MRD-based disambiguation work. Wilks et al. (1990) attempted to improve the knowledge associated with each sense by calculating the frequency of co-occurrence for the words in definition texts, from which they derive several measures of the degree of relatedness among words. This metric is then used with the help of a vector method that relates each word and its context. In experiments on a single word (bank), the method achieved 45% accuracy on sense identification, and 90% accuracy on homograph identification. Lesk's method has been extended by creating a neural network from definition texts in the Collins English Dictionary (CED), in which each word is linked to its senses, which are themselves linked to the words in their definitions, which are in turn linked to their senses, etc. (Veronis and Ide 1990).7 Experiments on 23 ambiguous words, each in six contexts (138 pairs of words), produced correct disambiguation, using the relatively fine sense distinctions in the CED, in 71.7% of the cases (three times better than chance: 23.6%) (Ide and Veronis 1990b); in later experiments, improving the parameters and only distinguishing homographs enabled a rate of 85% (vs. chance: 39%) (Veronis and Ide 1995). Applied to the task of mapping the senses of the CED and OALD for the same 23 words (59 senses in all), this method obtained a correct correspondence in 90% of the cases at the sense level, and 97% at the level of homographs (Ide and Veronis 1990a). Sutcliffe and Slater (1995) replicated this method on full text (samples from Orwell's Animal Farm) and found similar results (72% correct sense assignment, compared with a 33% chance baseline, and 40% using Lesk's method). Several authors (for example, Krovetz and Croft [1989], Guthrie et al. [1991], Slator [19921, Cowie, Guthrie, and Guthrie [1992], Janssen [1992], Braden-Harder [1993], Liddy and Paik [19931) have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English (LDOCE), in particular, the box codes and subject codes provided for each sense. Box codes include primitives such as ABSTRACT, ANIMATE, HUMAN, etc., and encode type restrictions on nouns and adjectives and on the arguments of verbs. Subject codes use another set of primitives to classify senses of words by subject (ECONOMICS, ENGINEERING, etc.). Guthrie et al. (1991) demonstrate a typical use of this information: in addition to using the Lesk-based method of counting overlaps between definitions and contexts, they impose a correspondence of subject codes in an iterative process. No quantitative evaluation of this method is available, but Cowie, Guthrie, and Guthrie (1992) improve the method using simulated annealing and report results of 47% for sense distinctions and 72% for homographs. The use of LDOCE box codes, however, is problematic: the codes are not systematic (see, for example, Fontenelle [1990]); in later work, Braden-Harder (1993) showed that simply matching box or subject codes is not sufficient for disambiguation. For example, in I tipped the driver, the codes for several senses of the words in the sentence satisfy the necessary constraints (e.g., tip-money + human object or tip-tilt + movable solid object). 7 Note that the assumptions underlying this method are very similar to Quillian's (1968): Thus one may think of a full concept analogically as consisting of all the information one would have if he looked up what will be called the &quot;patriarch&quot; word in a dictionary, then looked up every word in each of its definitions, then looked up every word found in each of these, and so on, continually branching outward... (p. 238). However, Quillian's network also keeps track of semantic relationships among the words encountered along the path between two words, which are encoded in his semantic network; the neural network avoids the overhead of creating the semantic network but loses this relational information. Ide and Wronis Introduction In many ways, the supplementary information in the LDOCE, and in particular the subject codes, is similar to that in a thesaurus, which, however, is more systematically structured. Inconsistencies in dictionaries, noted earlier, are not the only and perhaps not the major source of their limitations for WSD. While dictionaries provide detailed information at the lexical level, they lack pragmatic information that enters into sense determination (see, e.g., Hobbs [1987]). For example, the link between ash and tobacco, cigarette, or tray in a network such as Quillian's is very indirect, whereas in the Brown corpus, the word ash co-occurs frequently with one of these words. It is therefore not surprising that corpora have become a primary source of information for WSD; this development is outlined below in Section 2.3. 2.3.2 Thesauri. Thesauri provide information about relationships among words, most notably synonymy. Roget's International Thesaurus, which was put into machine-tractable form in the 1950s and has been used in a variety of applications including machine translation (Masterman 1957), information retrieval (Sparck-Jones 1964, 1986), and content analysis (Sedelow and Sedelow [1969], see also Sedelow and Sedelow [1986, 1992]), also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels.8 Typically, each occurrence of the same word under different categories of the thesaurus represents different senses of that word; i.e., the categories correspond roughly to word senses (Yarowsky 1992). A set of words in the same category are semantically related. The earliest known use of Roget's for WSD is the work of Masterman (1957), described above in Section 2.1. Several years later, Patrick (1985) used Roget's to discriminate among verb senses, by examining semantic clusters formed by &quot;e-chains&quot; derived from the thesaurus (Bryan [1973, 1974]; see also Sedelow and Sedelow [1986]). He uses &quot;word-strong neighborhoods,&quot; comprising word groups in low-level semicolon groups, which are the most closely related semantically in the thesaurus, and words connected to the group via chains. He is able, he claims, to discriminate the correct sense of verbs such as inspire (to raise the spirits vs. to inhale, breathe in, sniff, etc. ), and question (to doubt vs. to ask a question) with high reliability. Bryan's earlier work had already demonstrated that homographs can be distinguished by applying a metric based on relationships defined by his chains (Bryan 1973, 1974). Similar work is described in Sedelow and Mooney (1988). Yarowsky (1992) derives classes of words by starting with words in common categories in Roget's (4th edition). A 100-word context of each word in the category is extracted from a corpus (the 1991 electronic text of Grolier's Encyclopedia), and a mutualinformation-like statistic is used to identify words most likely to co-occur with the category members. The resulting classes are used to disambiguate new occurrences of a polysemous word: the 100-word context of the polysemous occurrence is examined for words in various classes, and Bayes' Rule is applied to determine the class most likely to be that of the polysemous word. Since class is assumed by Yarowsky to represent a particular sense of a word, assignment to a class identifies the sense. He reports 92% accuracy on a mean three-way sense distinction. Yarowsky notes that his method is best for extracting topical information, which is in turn most successful for disambiguating nouns (see Section 3.1.2). He uses the broad category distinctions supplied by Roget's, although he points out that the lower-level information may provide rich information for disambiguation. Patrick's much earlier study, on the other hand, exploits the lower levels of the concept hierarchy, in which words are more closely related semantically, as well as connections among words within the thesaurus itself; however, despite its promise this work has not been built upon since. Like machine-readable dictionaries, a thesaurus is a resource created for humans and is therefore not a source of perfect information about word relations. It is widely recognized that the upper levels of its concept hierarchy are open to disagreement (although this is certainly true for any concept hierarchy), and that they are so broad as to be of little use in establishing meaningful semantic categories. Nonetheless, thesauri provide a rich network of word associations and a set of semantic categories potentially valuable for language-processing work; however, Roget's and other thesauri have not been used extensively for WSD.9 WordNet combines the features of many of the other resources commonly exploited in disambiguation work: it includes definitions for individual senses of words within it, as in a dictionary; it defines &quot;synsets&quot; of synonymous words representing a single lexical concept, and organizes them into a conceptual hierarchy,1° like a thesaurus; and it includes other links among words according to several semantic relations, including hyponymy/hyperonymy, antonymy, and meronymy. As such, it currently provides the broadest set of lexical information in a single resource. Another, possibly more compelling, reason for WordNet's widespread use is that it is the first broad-coverage lexical resource that is freely and widely available; as a result, whatever its limitations, WordNet's sense divisions and lexical relations are likely to impact the field for several years to come.11 Some of the earliest attempts to exploit WordNet for sense disambiguation are in the field of information retrieval. Using the hyponomy links for nouns in WordNet, Voorhees (1993) defines a construct called a hood in order to represent sense categories, much as Roget's categories are used in the methods outlined above. A hood for a given word w is defined as the largest connected subgraph that contains w. For each content Ide and Veronis Introduction word in a document collection, Voorhees computes the number of times each synset appears above that word in the WordNet noun hierarchy, which gives a measure of the expected activity (global counts); she then performs the same computation for words occurring in a particular document or query (local counts). The sense corresponding to the hood root for which the difference between the global and local counts is the greatest is chosen for that word. Her results, however, indicate that her technique is not a reliable method for distinguishing WordNet's fine-grained sense distinctions. In a similar study, Richardson and Smeaton (1994) create a knowledge base from WordNet's hierarchy and apply a semantic similarity function (developed by Resnik—see below) to accomplish disambiguation, also for the purposes of information retrieval. They provide no formal evaluation but indicate that their results are &quot;promising.&quot; Sussna (1993) computes a semantic distance metric for each of a set of input text terms (nouns) in order to disambiguate them. He assigns weights based on the relation type (synonymy, hyperonymy, etc.) to WordNet links, and defines a metric that takes account of the number of arcs of the same type leaving a node and the depth of a given edge in the overall &quot;tree.&quot; This metric is applied to arcs in the shortest path between nodes (word senses) to compute semantic distance. The hypothesis is that for a given set of terms occurring near each other in a text, choosing the senses that minimize the distance among them selects the correct senses. Sussna's disambiguation results are demonstrated to be significantly better than chance. His work is particularly interesting because it is one of the few to date that utilizes not only WordNet's IS-A hierarchy, but other relational links as well. Resnik (1995a) draws on his body of earlier work on WordNet, in which he explores a measure of semantic similarity for words in the WordNet hierarchy (Resnik 1993a, 1993b, 1995a). He computes the shared information content of words, which is a measure of the specificity of the concept that subsumes the words in the WordNet IS-A hierarchy—the more specific the concept that subsumes two or more words, the more semantically related they are assumed to be. Resnik contrasts his method of computing similarity to those which compute path length (e.g., Sussna 1993), arguing that the links in the WordNet taxonomy do not represent uniform distances (cf. Resnik 1995b). Resnik's method, applied using WordNet's fine-grained sense distinctions and measured against the performance of human judges, approaches human accuracy. Like the other studies cited here, his work considers only nouns. WordNet is not a perfect resource for word sense disambiguation. The most frequently cited problem is the fine-grainedness of WordNet's sense distinctions, which are often well beyond what may be needed in many language-processing applications (see Section 3.2). Voorhees' (1993) hood construct is an attempt to access sense distinctions that are less fine-grained than WordNet's synsets, and less coarse-grained than the 10 WordNet noun hierarchies; Resnik's (1995a) method allows for detecting sense distinctions at any level of the WordNet hierarchy. However, it is not clear what the desired level of sense distinction should be for WSD (or if it is the same for all word categories, all applications, etc. ), or if this level is even captured in WordNet's hierarchy. Discussion within the language-processing community is beginning to address these issues, including the most difficult one of defining what we mean by &quot;sense&quot; (see Section 3.2). As outlined in Buitelaar (1997), sense disambiguation in the generative context starts with a semantic tagging that points to a complex knowledge representation reflecting all of a word's systematically related senses, after which semantic processing may derive a discourse-dependent interpretation containing more precise sense information about the occurrence. Buitelaar (1997) describes the use of CORELEX for underspecified semantic tagging (see also Pustejovsky, Boguraev, and Johnston [19951). Viegas, Mahesh, and Nirenburg (forthcoming) describe a similar approach to WSD undertaken in the context of their work on machine translation (see also Mahesh et al. [1997] and Mahesh, Nirenburg, and Beale [1997]). They access a large syntactic and semantic lexicon that provides detailed information about constraints, such as selectional restrictions, for words in a sentence, and then search a richly connected ontology to determine which senses of the target word best satisfy these constraints. They report a success rate of 97%. Like CORELEX, both the lexicon and the ontology are manually constructed, and therefore still limited, although much larger than the resources used in earlier work. However, Buitelaar (1997) describes means to automatically generate CORELEX entries from corpora in order to create domain-specific semantic lexicons, thus demonstrating the potential to access larger-scale resources of this kind. the nineteenth century, the manual analysis of corpora has enabled the study of words and graphemes (Kaeding 1897-1898, Estoup 1902, Zipf 1935) and the extraction of lists of words and collocations for the study of language acquisition or language teaching (Thorndike 1921; Fries and Traver 1940; Thorndike and Lorge 1938,1944; Gougenheim et al. 1956; etc.). Corpora have been used in linguistics since the first half of the twentieth century (e.g., Boas 1940; Fries 1952). Some of this work concerns word senses, and it is often strikingly modern: for example, Palmer (1933) studied collocations in English; Lorge (1949) computed sense frequency information for the 570 most common English words; Eaton (1940) compared the frequency of senses in four languages; and Thorndike (1948) and Zipf (1945) determined that there is a positive correlation between the frequency and the number of synonyms of a word, the latter of which is an indication of semantic richness (the more polysemous a word, the more synonyms it has). A corpus provides a bank of samples that enable the development of numerical language models, and thus the use of corpora goes hand-in-hand with empirical methods. Although quantitative/statistical methods were embraced in early MT work, in the mid-1960s interest in statistical treatment of language waned among linguists due to the trend toward the discovery of formal linguistic rules sparked by the theories of Zellig Harris (1951) and bolstered most notably by the transformational theories of Noam Chomsky (1957).12 Instead, attention turned toward full linguistic analysis and hence toward sentences rather than texts, and toward contrived examples and artificially limited domains instead of general language. During the following 10 to It would be difficult, indeed, in the face of today's activity, not to acknowledge the triumph of the theoretical approach, more precisely, of formal rules as the preferred successor of lexical and syntactic search algorithms in linguistic description. At the same time, common sense should remind us that hypothesis-making is not the whole of science, and that discipline will be needed if the victory is to contribute more than a haven from the rigors of experimentation (p. 313). Ide and VOronis Introduction 15 years, only a handful of linguists continued to work with corpora, most often for pedagogical or lexicographic ends (e.g., Quirk 1960; Michea 1964). Despite this, several important corpora were developed during this period, including the Brown Corpus (Kucera and Francis 1967), the Tresor de la Lan gue Francaise (Imbs 1971), and the Lancaster-Oslo-Bergen (LOB) corpus (Johansson 1980). In the area of natural language processing, the ALPAC report (1966) recommended intensification of corpus-based research for the creation of broad-coverage grammars and lexicons, but because of the shift away from empiricism, little work was done in this area until the 1980s. Until then, the use of statistics for language analysis was almost the exclusive property of researchers in the fields of literary and humanities computing, information retrieval, and the social sciences. Within these fields, work on WSD continued, most notably in the Harvard &quot;disambiguation project&quot; for content analysis (Stone et al. 1966; Stone 1969), and also in the work of Iker (1974, 1975), Choueka and Dreizin (1976) and Choueka and Goldberg (1979). In the context of the shift away from the use of corpora and empirical methods, the work of Weiss (1973) and Kelley and Stone (1975) on the automatic extraction of knowledge for word sense disambiguation seems especially innovative. Weiss (1973) demonstrated that disambiguation rules can be learned from a manually sense-tagged corpus. Despite the small size of his study (five words, a training set of 20 sentences for each word, and 30 test sentences for each word), Weiss's results are encouraging (90% correct). Kelley and Stone's (1975) work, which grew out of the Harvard &quot;disambiguation project&quot; for content analysis, is on a much larger scale; they extract KWIC concordances for 1,800 ambiguous words from a corpus of a half-million words. The concordances serve as a basis for the manual creation of disambiguation rules (&quot;word tests&quot;) for each sense of the 1,800 words. The tests—also very sophisticated for the time—examine the target word context for clues on the basis of collocational information, syntactic relations with context words, and membership in common semantic categories. Their rules perform even better than Weiss's, achieving 92% accuracy for gross homographic sense distinctions. In the 1980s, interest in corpus linguistics was revived (see, for example, Aarts [1990] and Leech [1991]). Advances in technology enabled the creation and storage of corpora larger than had been previously possible, enabling the development of new models most often utilizing statistical methods. These methods were rediscovered first in speech processing (e.g., Jelinek [1976]; see the overview by Church and Mercer [1993] and the collection of reprints by Waibel and Lee [1990]) and were immediately applied to written language analysis (e.g., in the work of Bahl and Mercer [1976], Debili [1977], etc.). For a discussion, see Ide and Walker (1992). In the area of word sense disambiguation, Black (1988) developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words. Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990, 1991), Hearst (1991), Leacock, Towell, and Voorhees (1993), Gale, Church, and Yarowsky (1992d, 1993), Bruce and Wiebe (1994), Miller et al. (1994), Niwa and Nitta (1994), Lehman (1994), among others. However, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness. distributes a corpus of approximately 200,000 sentences from the Brown Corpus and the Wall Street Journal in which all occurrences of 191 words are hand-tagged with their WordNet senses (see Ng and Lee [1996]). Also, the Cognitive Science Laboratory at Princeton has undertaken the hand-tagging of 1,000 words from the Brown Corpus with WordNet senses (Miller et al. 1993) (so far, 200,000 words are available via ftp), and hand-tagging of 25 verbs in a small segment of the Wall Street Journal (12,925 sentences), is also underway (Wiebe et al. 1997). However, these corpora are far smaller than those typically used with statistical methods. Several efforts have been made to automatically sense-tag a training corpus via bootstrapping methods. Hearst (1991) proposed an algorithm (CatchWord) that includes a training phase during which each occurrence of a set of nouns to be disambiguated is manually sense-tagged in several occurrences.' Statistical information extracted from the context of these occurrences is then used to disambiguate other occurrences. If another occurrence can be disambiguated with certitude, the system automatically acquires additional statistical information from these newly disambiguated occurrences, thus improving its knowledge incrementally. Hearst indicates that an initial set of at least 10 occurrences is necessary for the procedure, and that 20 or 30 occurrences are necessary for high precision. This overall strategy is more or less that of most subsequent work on bootstrapping. Recently, a class-based bootstrapping method for semantic tagging in specific domains has been proposed (Basili et al. 1997). Schiitze (1992, 1993) proposes a method that avoids tagging each occurrence in the training corpus. Using letter fourgrams within a 1,001-character window, his method, building on the vector-space model from information retrieval (see Salton, Wong, and Yang [1975]), automatically clusters the words in the text (each target word is represented by a vector); a sense is then assigned manually to each cluster, rather than to each occurrence. Assigning a sense demands examining 10 to 20 members of each cluster, and each sense may be represented by several clusters. This method reduces the amount of manual intervention but still requires the examination of a hundred or so occurrences for each ambiguous word. A more serious issue for this method is that it is not clear what the senses derived from the clusters correspond to (see, for example Pereira, Tishby, and Lee [1993]); moreover, the senses are not directly usable by other systems, since they are derived from the corpus itself. Brown et al. (1991) and Gale, Church, and Yarowsky, (1992a, 1993) propose the use of bilingual corpora to avoid hand-tagging of training data. Their premise is that different senses of a given word often translate differently in another language (for example, pen in English is stylo in French for its 'writing implement' sense, and encls for its 'enclosure' sense). By using a parallel aligned corpus, the translation of each occurrence of a word such as pen can be used to automatically determine its sense. This method has some limitations, since many ambiguities are preserved in the target language (e.g., French souris—English mouse); furthermore, the few available large-scale parallel corpora are very specialized (for example, the Hansard corpus of Canadian Parliamentary Debates), which skews the sense representation.' Dagan, Itai, and Schwall (1991) and Dagan and Itai (1994) propose a similar method, but instead of a parallel corpus use two monolingual corpora and a bilingual dictionary. This solves, in part, the problems of availability and specificity of domain that plague the parallel corpus approach, since monolingual corpora, including corpora from diverse domains and genres, are much easier to obtain than parallel corpora. Ide and Veronis Introduction Other methods attempt to avoid entirely the need for a tagged corpus, such as many of those cited in the section below (e.g., Yarowsky [1992] who attacks both the tagging and data sparseness problems simultaneously). However, it is likely that, as noted for grammatical tagging (Merialdo 1994), even a minimal phase of supervised learning improves radically on the results of unsupervised methods. Research into means to facilitate and optimize tagging is ongoing; for example, an optimization technique called committee-based sample selection has recently been proposed (Engelson and Dagan 1996), which, based on the observation that a substantial portion of manually tagged examples contribute little to performance, enables avoiding the tagging of examples that carry more or less the same information. Such methods are promising, although to our knowledge they have not been applied to the problem of lexical disambiguation. for much corpus-based work, is especially severe for work in WSD. First, enormous amounts of text are required to ensure that all senses of a polysemous word are represented, given the vast disparity in frequency among senses. For example, in the Brown Corpus (one million words), the relatively common word ash occurs only eight times, and only once in its sense as tree. The sense ashes = remains of cremated body, although common enough to be included in learner's dictionaries such as the LDOCE and the OALD, does not appear, and it would be nearly impossible to find the dozen or so senses in many everyday dictionaries such as the CED. In addition, the many possible co-occurrences for a given polysemous word are unlikely to be found in even a very large corpus, or they occur too infrequently to be significant.' Smoothing is used to get around the problem of infrequently occurring events, and in particular to ensure that non-observed events are not assumed to have a probability of zero. The best-known smoothing methods are that of Turing-Good (Good 1953), which hypothesizes a binomial distribution of events, and that of Jelinek and Mercer (1985), which combines estimated parameters on distinct subparts of the training corpus.' However, these methods do not enable distinguishing between events with the same frequency, such as the ash-cigarette and ash-room example given in footnote 15. Church and Gale (1991) have proposed a means to improve methods for the estimation of bigrams, which could be extended to co-occurrences: they take into account the frequency of the individual words that compose the bigram and make the hypothesis that each word appears independently of the others. However, this hypothesis contradicts hypotheses of disambiguation based on co-occurrence, which rightly assume that some associations are more probable than others. Class-based models attempt to obtain the best estimates by combining observations of classes of words considered to belong to a common category. Brown et al. (1992), Pereira and Tishby (1992), and Pereira, Tishby, and Lee (1993) propose methods that derive classes from the distributional properties of the corpus itself, while other authors use external information sources to define classes: Resnik (1992) uses the taxonomy of WordNet; Yarowsky (1992) uses the categories of Roget's Thesaurus, Slator (1992) and Liddy and Paik (1993) use the subject codes in the LDOCE; Luk (1995) uses conceptual sets built from the LDOCE definitions. Class-based methods answer in part the problem of data sparseness and eliminate the need for pretagged 15 For example, in a window of five words to each side of the word ash in the Brown corpus, commonly associated words such as fire, cigar, volcano, etc., do not appear. The words cigarette and tobacco co-occur with ash only once, with the same frequency as words such as room, bubble, and house. 16 See the survey of methods in Chen and Goodman (1996). data. However, there is some information loss with these methods because the hypothesis that all words in the same class behave in a similar fashion is too strong. For example, residue is a hypernym of ash in WordNet; its hyponyms form the class {ash, cotton(seed) cake, dottle} . Obviously the members of this set of words behave very differently in context: volcano is strongly related to ash, but has little or no relation to the other words in the set. Similarity-based methods Dagan, Marcus, and Markovitch 1993, Dagan, Pereira, and Lee 1994, and Grishman and Sterling 1993 exploit the same idea of grouping observations for similar words, but without regrouping them into fixed classes. Each word has a potentially different set of similar words. Like many class-based methods, such as Brown et al. (1992), similarity-based methods exploit a similarity metric between patterns of co-occurrence. Dagan, Marcus, and Markovitch (1993) give the following example: the pair (chapter, describes) does not appear in their corpus; however, chapter is similar to book, introduction, and section, which are paired with describes in the corpus. On the other hand, the words similar to book are books, documentation, and manuals (see their Figure 1). Dagan, Marcus, and Markovitch's (1993) evaluation seems to show that similarity-based methods perform better than class-based methods. Karov and Edelman (this volume) propose an extension to similarity-based methods by means of an iterative process at the learning stage, which gives results that are 92% accurate on four test words—approximately the same as the best results cited in the literature to date. These results are particularly impressive given that the training corpus contains only a handful of examples for each word, rather than the hundreds of examples required by most methods. We have already noted various problems faced in current WSD research related to specific methodologies. Here, we discuss issues and problems that all approaches to WSD must face and suggest some directions for further work. Context is the only means to identify the meaning of a polysemous word. Therefore, all work on sense disambiguation relies on the context of the target word to provide information to be used for its disambiguation. For data-driven methods, context also provides the prior knowledge with which current context is compared to achieve disambiguation. Broadly speaking, context is used in two ways: Information from microcontext, topical context, and domain contributes to sense selection, but the relative roles and importance of information from the different contexts, and their interrelations, are not well understood. Very few studies have used Ide and Veronis Introduction information of all three types, and the focus in much recent work is on microcontext alone. This is another area where systematic study is needed for WSD. 3.1.1 Microcontext. Most disambiguation work uses the local context of a word occurrence as a primary information source for WSD. Local or &quot;micro&quot; context is generally considered to be some small window of words surrounding a word occurrence in a text or discourse, from a few words of context to the entire sentence in which the target word appears. Context is very often regarded as all words or characters falling within some window of the target, with no regard for distance, syntactic structure, or other relations. Early corpus-based work, such as that of Weiss (1973) used this approach; spreading activation and dictionary-based approaches also do not usually differentiate context input on any basis other than occurrence in a window. Schtitze's vector space method (this volume) is a recent example of an approach that ignores adjacency information. Overall, the bag-of-words approach has been shown to work better for nouns than for verbs (cf. Schtitze, this volume), and to be in general less effective than methods that take other relations into consideration. However, as demonstrated in Yarowsky's (1992) work, the approach is cheaper than those requiring more complex processing and can achieve sufficient disambiguation for some applications. We examine below some of the other parameters. Distance. It is obvious from the quotation in Section 2.1 from Weaver's memorandum that the notion of examining a context of a few words around the target to disambiguate has been fundamental to WSD work since its beginnings: it has been the basis of WSD work in MT, content analysis, AI-based disambiguation, and dictionary-based WSD, as well as the more recent statistical, neural network, and symbolic machine learning, approaches. However, following Kaplan's early experiments (Kaplan 1950), there have been few systematic attempts to answer Weaver's question concerning the optimal value of N. A notable exception is the study of Choueka and Lusignan (1985), who verified Kaplan's finding that 2-contexts are highly reliable for disambiguation, and even 1-contexts are reliable in 8 out of 10 cases. However, despite these findings, the value of N has continued to vary over the course of WSD work more or less arbitrarily. Yarowsky (1993, 1994a, 1994b) examines different windows of microcontext, including 1-contexts, k-contexts, and words pairs at offsets —1 and —2, —1 and +1, and +1 and +2, and sorts them using a log-likelihood ratio to find the most reliable evidence for disambiguation. Yarowsky makes the observation that the optimal value of k varies with the kind of ambiguity: he suggests that local ambiguities need only a window of k = 3 or 4, while semantic or topic-based ambiguities require a larger window of 20-50 words (see Section 3.1.2). No single best measure is reported, suggesting that for different ambiguous words, different distance relations are more efficient. Furthermore, because Yarowsky also uses other information (such as part of speech), it is difficult to isolate the impact of window-size alone. Leacock, Chodorow, and Miller (this volume) use a local window of ±3 open-class words, arguing that this number showed best performance in previous tests. Collocation. The term &quot;collocation&quot; has been used variously in WSD work. The term was popularized by J. R. Firth in his 1951 paper &quot;Modes of meaning&quot;: &quot;One of the meanings of ass is its habitual collocation with an immediately preceding you silly. . . .&quot; He emphasizes that collocation is not simple co-occurrence but is &quot;habitual&quot; or &quot;usual.&quot;17 Halliday's (1961) definition of collocation as &quot;the syntagmatic association of lexical items, quantifiable, textually, as the probability that there will occur at n removes (a distance of n lexical items) from an item x, the items a, b, c. . .&quot; is more workable in computational terms. Based on this definition, a significant collocation can be defined as a syntagmatic association among lexical items, where the probability of item x co-occurring with items a, b, c . . . is greater than chance (Berry-Rogghe 1973). It is in this sense that most WSD work uses the term. There is some psychological evidence that collocations are treated differently from other co-occurrences. For example, Kintsch and Mross (1985) show that priming words that enter frequent collocations with test words (i.e., iron-steel, which they call associative context) activate these test words in lexical decision tasks. Conversely, priming words that are in the thematic context (i.e., relations determined by the situation, scenario, or script such as plane-gate) do not facilitate the subjects' lexical decisions (see also Fischler [1977], Seidenberg et al. [1982], De Groot [1983], Lupker [19841). Yarowsky (1993) explicitly addresses the use of collocations in WSD work, but admittedly adapts the definition to his purpose as &quot;the co-occurrence of two words in some defined relation.&quot; As noted above, he examines a variety of distance relations, but also considers adjacency by part of speech (e.g., first noun to the left). He determines that in cases of binary ambiguity, there exists one sense per collocation, that is, in a given collocation, a word is used with only one sense with 90-99% probability. Syntactic Relations. Earl (1973) used syntax exclusively for disambiguation in machine translation. In most WSD work to date, syntactic information is used in conjunction with other information. The use of selectional restrictions weighs heavily in AI-based work that relies on full parsing, frames, semantic networks, the application of selectional preferences, etc. (Hayes 1977a, 1997b; Wilks 1973 and 1975b; Hirst 1987). In other work, syntax is combined with frequent collocation information: Kelley and Stone (1975), Dahlgren (1988), and Atkins (1987) combine collocation information with rules for determining, for example, the presence or absence of determiners, pronouns, noun complements, as well as prepositions, subject-verb and verb-object relations. More recently, researchers have avoided complex processing by using shallow or partial parsing. In her disambiguation work on nouns, Hearst (1991) segments text into noun phrases, prepositional phrases, and verb groups, and discards all other syntactic information. She examines items that are within ±3 phrase segments from the target and combines syntactic evidence with other kinds of evidence, such as capitalization. Yarowsky (1993) determines various behaviors based on syntactic category; for example, that verbs derive more disambiguating information from their objects than from their subjects, adjectives derive almost all disambiguating information from the nouns they modify, and nouns are best disambiguated by directly adjacent adjectives or nouns. In recent work, syntactic information most often is simply part of speech, used invariably in conjunction with other kinds of information (McRoy 1992; Bruce and Wiebe 1994; Leacock, Chodorow, and Miller, this volume). Evidence suggests that different kinds of disambiguation procedures are needed depending on the syntactic category and other characteristics of the target word (Yarowsky 1993; Leacock, Chodorow, and Miller, this volume)—an idea reminiscent of the word expert approach. However, to date there has been little systematic study Ide and Wronis Introduction of the contribution of different information types for different types of target words. It is likely that this is a next necessary step in WSD work. a given sense of a word, usually within a window of several sentences. Unlike microcontext, which has played a role in disambiguation work since the early 1950s, topical context has been less consistently used. Methods relying on topical context exploit redundancy in a text—that is, the repeated use of words that are semantically related throughout a text on a given topic. Thus, base is ambiguous, but its appearance in a document containing words such as pitcher, and ball is likely to isolate a given sense for that word (as well as the others, which are also ambiguous). Work involving topical context typically uses the bag-of-words approach, in which words in the context are regarded as an unordered set. The use of topical context has been discussed in the field of information retrieval for several years (Anthony 1954; Salton 1968). Recent WSD work has exploited topical context: Yarowsky (1992) uses a 100-word window, both to derive classes of related words and as context surrounding the polysemous target, in his experiments using Roget's Thesaurus (see Section 2.3.2). Voorhees, Leacock, and Towell (1995) experiment with several statistical methods using a two-sentence window; Leacock, Towell, and Voorhees (1993, 1996) have similarly explored topical context for WSD. Gale, Church, and Yarowsky (1993), looking at a context of ±50 words, indicate that while words closest to the target contribute most to disambiguation, they improved their results from 86% to 90% by expanding context from ±6 (a typical span when only microcontext is considered) to ±50 words around the target. In a related study, they make a claim that for a given discourse, ambiguous words are used in a single sense with high probability (&quot;one sense per discourse&quot;) (Gale, Church, and Yarowsky 1992c). Leacock, Chodorow, and Miller (this volume) challenge this claim in their work combining topical and local context, which shows that both topical and local context are required to achieve consistent results across polysemous words in a text (see also Towell and Voorhees, this volume). Yarowsky's (1993) study indicates that while information within a large window can be used to disambiguate nouns, for verbs and adjectives the size of the usable window drops off dramatically with distance from the target word. This supports the claim that both local and topical context are required for disambiguation, and points to the increasingly accepted notion that different disambiguation methods are appropriate for different kinds of words. Methods utilizing topical context can be ameliorated by dividing the text under analysis into subtopics. The most obvious way to divide a text is by sections (Brown and Yule 1983), but this is only a gross division; subtopics evolve inside sections, often in unified groups of several paragraphs. Automatic segmentation of texts into such units would obviously be helpful for WSD methods that use topical context. It has been noted that the repetition of words within successive segments or sentences is a strong indicator of the structure of discourse (Skorochod'ko 1972; Morris 1988; Morris and Hirst 1991); methods exploiting this observation to segment a text into subtopics are beginning to emerge (see, for example, Hearst [1994], van der Eijk [1994], Richmond, Smith, and Amitay [1997]). In this volume, Leacock, Chodorow, and Miller consider the role of microcontext vs. topical context and attempt to assess the contribution of each. Their results indicate that for a statistical classifier, microcontext is superior to topical context as an indicator of sense. However, although a distinction is made between microcontext and topical context in current WSD work, it is not clear that this distinction is meaningful. It may be more useful to regard the two as lying along a continuum, and to consider the role 3.1.3 Domain. The use of domain for WSD is first evident in the microglossaries developed in early MT work (see Section 2.1). The notion of disambiguating senses based on domain is implicit in various AI-based approaches, such as Schank's script approach to natural language processing (Schank and Abelson 1977), which matched words to senses based on the context or &quot;script&quot; activated by the general topic of the discourse. This approach, which activates only the sense of a word relevant to the current discourse domain, demonstrates its limitations of this approach when used in isolation; in the famous example The lawyer stopped at the bar for a drink, the incorrect sense of bar will be assumed if one relies only on the information in a script concerned with law.18 Gale, Church, and Yarowsky's (1992c) claim for one sense per discourse is disputable. Dahlgren (1988) observes that domain does not eliminate ambiguity for some words: she remarks that the noun hand has 16 senses (or so) and retains 10 of them in almost any text. The influence of domain likely depends on factors such as the type of text (how technical the text is, etc. ), the relation among the senses of the target word (strongly or weakly polarized, common vs. specialized usage, etc.). For example, in the French Encyclopaedia Universalis, the word interet (&quot;interest&quot;) appears 62 times in the article on INTEREST—FINANCE, in all cases in its financial sense; the word appears 139 times in the article INTEREST—PHILOSOPHY AND HUMANITIES in its common, nonfinancial, sense. However, in the article THIRD WORLD, the word interet appears two times in each of these senses. 3.2.1 The Bank Model. Most researchers in WSD are currently relying on the sense distinctions provided by established lexical resources, such as machine-readable dictionaries or WordNet (which uses the OALD's senses), because they are widely available. The dominant model in these studies is the &quot;bank&quot; model, which attempts to extend the clear delineation between bank-money and bank-riverside to all sense distinctions. However, it is clear that this convenient delineation is by no means applicable to all or even most other words. Although there is some psychological validity to the notion of senses (Simpson and Burgess 1988; Jorgensen 1990), lexicographers themselves are well aware of the lack of agreement on senses and sense divisions (see, for example, Malakhovski [1987], Robins [1987], Ayto [1983], Stock [19831). The problem of sense division has been an object of discussion since antiquity: Aristotle' devoted a section of his Topics to this subject in 350 B.C. Since then, philosophers and linguists have continued to discuss the topic at length (see Quine [1960], Asprejan [1974], Lyons [1977], Weinrich [1980], Cruse [1986]), but the lack of resolution over 2,000 years is striking. 3.2.2 Granularity. One of the foremost problems for WSD is to determine the appropriate degree of sense granularity. Several authors (for example, Slator and Wilks [1987]) have remarked that the sense divisions one finds in dictionaries are often too fine for the purposes of NLP work. Overly fine sense distinctions create practical difficulIde and Veronis Introduction ties for automated WSD: they introduce significant combinatorial effects (for example, Slator and Wilks [1987] note that the sentence There is a huge envelope of air around the surface of the earth has 284,592 different potential combined sense assignments using the moderately-sized LDOCE); they require making sense choices that are extremely difficult, even for expert lexicographers; and they increase the amount of data required for supervised methods to unrealistic proportions. In addition, the sense distinctions made in many dictionaries are sometimes beyond those which human readers themselves are capable of making. In a well-known study, Kilgarriff (1992, 1993) shows that it is impossible for human readers to assign many words to a unique sense in LDOCE (see, however, the discussion in Wilks [forthcoming]). Recognizing this, Dolan (1994) proposes a method for &quot;ambiguating&quot; dictionary senses by combining them to create grosser sense distinctions. Others have used the grosser sense divisions of thesauri such as Roget's; however, it is often difficult to assign a unique sense, or even find an appropriate one among the options (see, for example, Yarowsky [1992]). Chen and Chang (this volume) propose an algorithm that combines senses in a dictionary (LDOCE) and links them to the categories of a thesaurus (LLOCE). Combining dictionary senses does not solve the problem. First of all, the degree of granularity required is task dependent. Only homograph distinction is necessary for tasks such as speech synthesis or restoration of accents in text, while tasks such as machine translation require fine sense distinctions—in some cases finer than what monolingual dictionaries provide (see, for example, ten Hacken [1990]). For example, the English word river is translated as fleuve in French when the river flows into the ocean, and otherwise as riviere. There is not, however, a strict correspondence between a given task and the degree of granularity required. For example, as noted earlier, the word mouse, although it has two distinct senses (animal, device), translates into French in both cases to souris. On the other hand, for information retrieval the distinction between these two senses of mouse is important, whereas it is difficult to imagine a reason to distinguish river (sense fleuve) - river (sense riviere). Second, and more generally, it is unclear when senses should be combined or split. Even lexicographers do not agree: Fillmore and Atkins (1991) identify three senses of the word risk but find that most dictionaries fail to list at least one of them. In many cases, meaning is best considered as a continuum along which shades of meaning fall (see, for example, Cruse [1986]), and the points at which senses are combined or split can vary dramatically. 3.2.3 Senses or usages? The Aristotelian idea that words correspond to specific objects and concepts was displaced in the twentieth century by the ideas of Saussure and others (Meillet [1926], Hjemslev [1953], Martinet [1960], etc.). For Antoine Meillet, for example, the sense of a word is defined only by the average of its linguistic uses. Wittgenstein takes a similar position in his Philosophische Utersuchungen' in asserting that there are no senses, but only usages: &quot;For a large class of cases—though not for all—in which we employ the word 'meaning' it can be defined thus: the meaning of a word is its use in the language&quot; (1953, Sect. 43). Similar views are apparent in more recent theories of meaning, for example, Bloomfield (1933) and Harris (1954), for whom meaning is a function of distribution; and in Barwise and Perry's (1953) situation semantics, where the sense or senses of a word are seen as an abstraction of the role that it plays systematically in the discourse. The COBUILD project (Sinclair 1987) adopts this view of meaning by attempting to anchor dictionary senses in current usage by creating sense divisions on the basis of clusters of citations in a corpus. Atkins (1987) and Kilgarriff (forthcoming) also implicitly adopt the view of Harris (1954), according to which each sense distinction is reflected in a distinct context. A similar view underlies the class-based methods cited in Section 2.4.3 (Brown et al. 1992; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993). In this volume, Schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list. 3.2.4 Enumeration or generation? The development of generative lexicons (Pustejovsky 1995) provides a view of word senses that is very different from that of almost all WSD work to date. The enumerative approach assumes an a priori, established set of senses that exist independent of context—fundamentally the Aristotelian view. The generative approach develops a discourse-dependent representation of sense, assuming only underspecified sense assignments until context is taken into account, and bears closer relation to distributional and situational views of meaning. Considering the difficulties of determining an adequate and appropriate set of senses for WSD, it is surprising that little attention has been paid to the potential of the generative view in WSD research. As larger and more complete generative lexicons become available, there is merit to exploring this approach to sense assignment. Given the variety in the studies cited throughout the previous survey, it is obvious that it is very difficult to compare one set of results, and consequently one method, with another. The lack of comparability results from substantial differences in test conditions from study to study. For instance, different types of texts are involved, including both highly technical or domain-specific texts where sense use is limited and general texts where sense use may be more variable. It has been noted that in a commonly used corpus such as the Wall Street Journal, certain senses of typical test words such as line are absent entirely.' When different corpora containing different sense inventories and very different levels of frequency for a given word and/or sense are used, it becomes futile to attempt to compare results. Test words themselves differ from study to study, including not only words whose assignment to clearly distinguishable senses varies considerably or which exhibit very different degrees of ambiguity (e.g., bank vs. line), but also words across different parts of speech and words that tend to appear more frequently in metaphoric, metonymic, and other nonliteral usages (e.g., bank vs. head). More seriously, the criteria for evaluating the correctness of sense assignment vary. Different studies employ different degrees of sense granularity (see Section 3.2 above), ranging from identification of homographs to fine sense distinctions. In addition, the means by which correct sense assignment is finally judged are typically unclear. Human judges must ultimately decide, but the lack of agreement among human judges is well documented: Amsler and White (1979) indicate that while there is reasonable consistency in sense assignment for a given expert on successive sense assignments (84%), agreement is significantly lower among experts. Ahlswede (1995) reports between 63.3% and 90.2% agreement among judges on his Ambiguity Questionnaire; when faced with on-line sense assignment in a large corpus, agreement among judges is far less, and in some cases worse than chance (see also Ahlswede [1992, 19931, Ahlswede and Lorand [19931). Jorgensen (1990) found the level of agreement in her experiment using data from the Brown Corpus to be about 68%. The difficulty of comparing results in WSD research has recently become a concern within the community, and efforts are underway to develop strategies for evaluation of WSD. Gale, Church, and Yarowsky (1992b) attempt to establish lower and upper bounds for evaluating the performance of WSD systems; their proposal for overcoming the problem of agreement among human judges in order to establish an upper bound provides a starting point, but it has not been widely discussed or implemented. A recent discussion at a workshop sponsored by the ACL Special Interest Group on the Lexicon (SIGLEX) on &quot;Evaluating Automatic Semantic Taggers&quot; (Resnik and Yarowsky [1997a]; see also Resnik and Yarowsky [1997b], Kilgarriff [19971) has sparked the formation of an evaluation effort for WSD (SENSEVAL), in the spirit of previous evaluation efforts such as the ARPA-sponsored Message Understanding Conferences (e.g., ARPA [1993]), and Text Retrieval Conferences (e.g. Harman [1993, 1995]). SENSEVAL will see its first results at a subsequent SIGLEX workshop to be held at Herstmonceux Castle, England in September, 1998. As noted above, WSD is not an end in itself but rather an &quot;intermediate task&quot; that contributes to an overall task such as information retrieval or machine translation. This opens the possibility of two types of evaluation for WSD work (using terminology borrowed from biology): in vitro evaluation, where WSD systems are tested independent of a given application, using specially constructed benchmarks; and evaluation in vivo, where, rather than being evaluated in isolation, results are evaluated in terms of their contribution to the overall performance of a system designed for a particular application, such as machine translation. 3.3.1 Evaluation In Vitro. In vitro evaluation, despite its artificiality, enables close examination of the problems plaguing a given task. In its most basic form, this type of evaluation (also called variously performance evaluation: Hirschman and Thompson [1996]; assessment: Bimbot, Chollet, and Paoloni [1994]; or declarative evaluation: Arnold, Sadler, and Humphreys [1993]) involves comparison of the output of a system for a given input, using measures such as precision and recall. SENSEVAL currently envisages this type of evaluation for WSD results. Alternatively, in vitro evaluation can focus on study of the behavior and performance of systems on a series of test suites representing the range of linguistic problems likely to arise in attempting WSD (diagnostic evaluation: Hirschman and Thompson [1996]; or typological evaluation: Arnold, Sadler, and Humphreys 1993). Considerably deeper understanding of the factors involved in the disambiguation task is required before appropriate test suites for typological evaluation of WSD results can be devised. Basic questions such as the role of part of speech in WSD, the treatment of metaphor, metonymy, and the like in evaluation, and how to deal with words of differing degrees and types of polysemy, must first be resolved. SENSEVAL will likely take us a step closer to this understanding; at the least, it will force consideration of what can be meaningfully regarded as an isolatable sense distinction and provide some measure of the distance between the performance of current systems and a predefined standard. The in vitro evaluation envisaged for SENSEVAL demands the creation of a manually sense-tagged reference corpus containing an agreed-upon set of sense distinctions. The difficulties of attaining sense agreement, even among experts, have already been outlined. Resnik and Yarowsky (1997b) have proposed that for WSD evaluation, it may be practical to retain only those sense distinctions that are lexicalized crosslinguistically. This proposal has the merit of being immediately usable, but in view of the types of problems cited in the previous section, systematic study of interlanguage relations will be required to determine its viability and generality. At present, the apparent best source of sense distinctions is assumed to be on-line resources such as LDOCE or WordNet, although the problems of utilizing such resources are well known, and their use does not address issues of more complex semantic tagging that goes beyond the typical distinctions made in dictionaries and thesauri. Resnik and Yarowsky (1997b) also point out that a binary evaluation (correct/incorrect) for WSD is not sufficient, and propose that errors be penalized according to a distance matrix among senses based on a hierarchical organization. For example, failure to identify homographs of bank (which would appear higher in the hierarchy) would be penalized more severely than failure to distinguish bank as an institution from bank as a building (which would appear lower in the hierarchy). However, despite the obvious appeal of this approach, it runs up against the same problem of the lack of an established, agreed-upon hierarchy of senses. Aware of this problem, Resnik and Yarowsky suggest creating the sense distance matrix based on results in experimental psychology such as Miller and Charles (1991) or Resnik (1995b). Even ignoring the cost of creating such a matrix, the psycholinguistic literature has made clear that these results are highly influenced by experimental conditions and the task imposed on the subjects (see, for example, Tabossi [1989, 1991], Rayner and Morris [1991]); in addition, it is not clear that psycholinguistic data can be of help in WSD aimed toward practical use in NLP systems. In general, WSD evaluation confronts difficulties of criteria that are similar to, but orders of magnitude greater than, those facing other tasks such as part-of-speech tagging, due to the elusive nature of semantic distinctions. It may be that at best we can hope to find practical solutions that will serve particular needs; this is considered more fully in the next section. 3.3.2 Evaluation In Vivo. Another approach to evaluation is to consider results insofar as they contribute to the overall performance in a particular application, such as machine translation, information retrieval, or speech recognition. This approach (also called adequacy evaluation: Hirschman and Thompson [1996]; or operational evaluation: Arnold, Sadler, and Humphreys [1993]), although it does not assure the general applicability of a method nor contribute to a detailed understanding of problems, does not demand agreement on sense distinctions or the establishment of a pretagged corpus. Only the final result is taken into consideration, subjected to evaluation appropriate to the task at hand. Methods for WSD have evolved largely independently of particular applications, especially in the recent past. It is interesting to note that few, if any, systems for machine translation have incorporated recent methods developed for WSD, despite the importance of WSD for MT noted by Weaver almost 50 years ago. The most obvious effort to incorporate WSD methods into larger applications is in the field of information retrieval, and the results are ambiguous: Krovetz and Croft (1992) report only a slight improvement in retrieval using WSD methods; Voorhees (1993) and Sanderson (1994) indicate that retrieval degrades if disambiguation is not sufficiently precise. SparckJones (forthcoming) questions the utility of any NLP technique for document retrieval. On the other hand, Schtitze and Pedersen (1995) show a marked improvement in retrieval (14.4%) using a method that combines search-by-word and search-by-sense. It remains to be seen to what extent WSD can improve results in particular applications. However, if meaning is largely a function of use, it may be that the only relevant evaluation of WSD results is achievable in the context of specific tasks. Work on automatic WSD has a history as long as automated language processing generally. Looking back, it is striking to note that most of the problems and the basic approaches to solving them were recognized at the outset. Since so much of the early work on WSD is reported in relatively obscure books and articles across several fields and disciplines, it is not surprising that recent authors are often unaware of it. What is surprising is that in the broad sense, relatively little progress seems to have been made in nearly 50 years. Even though much recent work cites results at the 90% level or better, these studies typically involve very few words, most often only nouns, and frequently concern only broad sense distinctions. In a sense, WSD work has come full circle, returning most recently to empirical methods and corpus-based analyses that characterize some of the earliest attempts to solve the problem. With sufficiently greater resources and enhanced statistical methods at their disposal, researchers in the 1990s have obviously improved on earlier results, but it appears that we may nearly have reached the limit of what can be achieved in the current framework. For this reason, it is especially timely to assess the state of WSD and consider, in the context of its entire history, the next directions of research. This paper is an attempt to provide that context, at least in part, by bringing WSD into the perspective of the past 50 years of work on the topic. While we are aware that much more could be added to what is presented here, we have made an attempt to cover at least the major areas of work and sketch the broad lines of development in the field.&quot; Of course, WSD is problematic in part because of the inherent difficulty of determining or even defining word sense, and this is not an issue that is likely to be solved in the near future. Nonetheless, it seems clear that current WSD research could benefit from a more comprehensive consideration of theories of meaning and work in the area of lexical semantics. One of the obvious stumbling blocks in much recent WSD work is the rather narrow view of sense that comes hand-in-hand with the attempt to use sense distinctions in everyday dictionaries, which cannot, and are not intended to, represent meaning in context. A different sort of view, one more consistent with current linguistic theory, is required; here, we see the recent work using generative lexicons as providing at least a point of departure. Another goal of this paper is to provide a starting point for the growing number of researchers working in various areas of computational linguistics who want to learn about WSD. There is renewed interest in WSD as it contributes to various applications, such as machine translation and document retrieval. WSD as &quot;intermediate task,&quot; while interesting in its own right, is difficult and perhaps ultimately impossible to assess in the abstract; incorporation of WSD methods into larger applications will therefore hopefully inform and enhance future work. Finally, if a lesson is to be learned from a review of the history of WSD, it is that research can be very myopic and, as a result, tends to revisit many of the same issues over time. This is especially true when work on a problem has been cross-disciplinary. There is some movement toward more merging of research from various areas, at least as far as language processing is con cerned, spurred by the practical problems of information access that we are facing as a result of rapid technological development. Hopefully, this will contribute to further progress on WSD.
Providing A Unified Account Of Definite Noun Phrases In Discourse discourse utterances that combine into of the discourse, namely, units discourse that are typically larger than a single sentence, but smaller than the complete discourse. However, the constituent structure is not determined solely by the linear sequence of utterances. It is common for two contiguous utterances to be members of different subconstituents of the discourse (as with breaks between phrases in the syntactic analysis of a sentence); likewise, it is common for two utterances that are not contiguous to be members of the same subconstituent. An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. That is, discourses have been shown to of coherence. coherence to the ways in which the larger segments of discourse relate to one another. It depends on such things as the function of a discourse, its subject matter, and rhetorical schema 1977, 1981; Reichman, 19811. coherence refers to the ways in which individual sentences bind together to form larger discourse segments. It depends on such things as the syntactic structure of an utterance, and the use of referring expressions 1Sidner, 19811. The two levels of discourse coherence correspond to two of focusing—global centering. Participants are said to be globally focused on a set of entities relevant to the overall discourse. These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811. In contrast. centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns (Sidner, 1979; Joshi and Weinstein, 19811. 44 The two levels of focusing/coherence have different effects on the processing of pronominal and nonpronominal definite noun phrases. Global coherence and focusing are major factors in the generation and interpretation of nonpronominal definite referring expressions.- Local coherence and centering have greater effect on the processing of pronominal expressions. In Section 5 we shall describe the rules governing the use of these kinds of expressions and shall explain why additional processing by the hearer (needed for drawing additional inferences) is involved when pronominal expressions are used to refer to globally focused entities or nonpronominal expressions are used to refer to centered entities. Many approaches to language interpretation have ignored these differences, depending instead on powerful inference mechanisms to identify the referents of referring expressions. Although such approaches may suffice, especially for well-formed texts, they are insufficient in general. In particular, such approaches will not work for generation. Here the relationships among focusing, coherence, and referring expressions are essential and must be explicitly provided for. Theories—and systems based on them--will generate unacceptable uses of referring expressions if they do not take these into 3. Centering and Anaphora theory, the centers of a sentence in a discourse serve to integrate that sentence into the discourse. Each S, has a single center, a set of centers, Cb(S) serves to link S to the preceding discourse, while Cf(S) provides a set of entities to which the succeeding discourse may be linked. To avoid confusion, the phrase 'the center' will he used to refer only to Cb(S). To clarify the notion of center, we will consider a number of discourses illustrating the various factors that combined in (abstractly) and in its identification in a discourse. In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse. We begin by showing that the center cannot be defined in syntactic terms alone. The interaction of semantics and centering is more complex and is discussed in Section 4. The following examples, drawn from Reinhart [19821, illustrate the point that the notion of center is not i.e., the syntax of a sentence S not determine which of its NPs (The differ in other respects also. Reichman [19811 and Grosz 110811 discuss some of these. attempts to incorporate focusing mechanisms in generation systems are described in [Appelt, 1981 and McKeown, 19821. can obviously affect the interpretation; for of this paper, it may be regarded as part of a for the use of this terminology discussed (la) Who did Max see yesterday? (lb) Max saw Rosa. (2a) Did anyone see Rosa yesterday? (2b) Max saw Rosa. (lb) and (2b) are identical, Cb(lb) Max and Cb(2b) is Rosa. This can be seen in part by noticing that saw Rosa' seems more natural than (Ib) *Max saw her' than (2b) (a fact consistent with the centering rule introduced in Section 5.) The subject NP is the center in one context, the object NP in the other. when the NP used to realize Cb(S) syntactically determined, the Cb(S) itself is not yet fully determined, for Cb(S) is typically not a linguistic entity (i.e., it is not a particular linguistic expression). Rosa, not 'Rosa' is the Cb(2b). Consider. the discourse: (3a) How is Rosa? (3b) Did anyone see her yesterday? saw her. Here, Cb(3c) is Rosa, but clearly would not be in other contexts where the expression 'her' still realized the backward-looking center of 'Max saw her.' This is seen most simply by considering the discourse that would result if &quot;How is Joan?' replaced (3a). In the discourse that resulted, Joan, not Rosa, would be the center of (3c). 4. Centering and Realization The interactions of semantic and pragmatic factors with centering and their effects on referring expressions are more complex than the preceding discussion suggests. In the examples given above, the NPs that realize Cb(S) also denote it, but this is not always the case: we used the term 'realize' in the above discussion advisedly. In this section, we consider two kinds of examples in which the center of a sentence is not simply the denotation of some noun phrase occurring in the sentence. First, we will examine several examples in which the choice of and interaction among different kinds of interpretations of definite noun phrases are affected by the local discourse context (i.e., centering). Second, the role of pragmatic factors in some problematic cases of referential uses of definite descriptions [Donnellan 19661 is discussed. 4.1. Realization and Value-Free and Value-Loaded Interpretations distinction between semantic denotation is necessary to treat the interaction between value-free and value-loaded interpretations [Barwise and Perry, 19821 of definite descriptions, as they occur in extended discourse. Consider, for example, the following sequence: (4a) The vice president of the United States is also president of the Senate. (4b) Historically, he is the president's key man in negotiations with Congress. to China, he handled tricky negotiations, so he prepared for this Cb(4b) and Cb(4b') are each realized by the anaphoric element 'he.' But (4b) expresses the same thing as 'Historically, the vice president of the United States is the president's key man in negotiations with Congress' (in which it is clear that no single individual vice president is being referred to) whereas (4b1 expresses the same thing as, 'As ambassador to China, the [person who is now] vice president of the United States handled many tricky negotiations,...' This can be accounted for by observing that 'the vice president of the United States' contributes both its value-free interpretation and its value-loading at the world type to Cf(4a). Cb(4b) is then the value-free interpretation and Cb(4b') is the valueloading, i.e., George Bush. In this example, both value-free and value-loaded interpretations are shown to stern from the same full definite noun phrase. It is also possible for the movement of the center from a value-free interpretation (for Cb(S)) to a value-loaded interpretation (for Cb of the next sentence)--or vice versa—to be accomplished solely with pronouns. That is, although (4b)-(4b1 is (at least for some readers) not a natural dialogue, similar sequences are possible. There appear to be strong constraints on the kinds of transitions that are allowed. In particular, if a given sentence forces either the value-free or value-loaded interpretation, then only that interpretation becomes possible in a subsequent sentence. However, if some sentence in a given context merely prefers one interpretation while allowing the other, then either one is possible in a subsequent sentence. For example, the sequence (6a) The vice president of the United States also president Senate. the president's key aan in negotiations vith Congress. in which 'he' may be interpreted as either value-free or (VL), may be followed by either of following As to China, he zany tricky negotiations. is required to be at least old. However, if we change (5b) to force the value-loaded interpretation, as in (5b&quot;), then only (5c) is possible. (Sb') Right now he is the president's key man in negotiations with Congress. Similarly, if (5b) is changed to force the value-free interpretation, as in (4b), then only (5c') is possible. If an intermediate sentence allows both interpretations but prefers one in a given context, then either is possible in the third sentence. A use with preference for a valueloaded interpretation followed by a use indicating the value-free interpretation is illustrated in the sequence: John thinks that the telephone is a toy. it every day. (11 preferred; ok) He doesn't realize that it is an invention changed the world. preference a value-free interpretation that is by value-loaded one is easiest to see in a dialogue situation; vice president of the United States is also president of the Senate. I he played some role in the House. (VF preferred; VL did, but that was before he was Realization and Use these examples, might appear that the concepts of value-free and value-loaded interpretation are identical to DonneIlan's 119661 attributive and referential uses of noun phrases. However, there is an important difference between these two distinctions. The importance to our theory is that the referential use of definite noun phrases introduces the need to take pragmatic factors (in particular speaker intention) into account, not just semantic factors. Donnellan [1966] describes the referential and uses of descriptions in the following way: 'A speaker who uses a definite description attributively in an assertion states something whoever or whatever is the so-and-so. speaker who uses a definite description referentially in an assertion, on the other hand, uses the description to enable his audience to pick out whom or what he is talking about and states something about that person or thing. In the first case the definite description might be said to occur essentially, for the speaker wishes to assert something about whatever or whoever fits that description; but in the referential use the definite description is merely one tool for doing a certain job--calling attention to a person or thing--and in general any other device for doing the same job, another description or a name, would do as well. In the attributive use, the attribute of being the so-and-so is all important, while it is not in the referential use.' The distinction Donnellan suggests can be formulated in terms of the different propositions a sentence S containing a definite description D may be used to express on different occasions of use. When D is used referentially, it contributes its denotation to the proposition expressed by 46 S; when it is used attributively, it contributes to the proposition expressed by S a semantic interpretation related to the descriptive content of D. The identity of this semantic interpretation is not something about which Donnellan is explicit. Distinct formal treatments of the semantics of definite descriptions in natural language would construe the appropriate interpretation differently. In semantic treatments based on possible worlds, the appropriate interpretation would be a (partial) function from possible worlds to objects; in the situation semantics expounded by Barwise and Perry, the appropriate interpretation is a (partial) function from resource to objects. As just described, the referential-attributive distinction appears to be exactly the distinction that Barwise and Perry formulate in terms of the value-loaded and valuefree interpretations of definite noun phrases. But this gloss omits an essential aspect of the referentialattributive distinction as elaborated by Donnellan. In view, a speaker may use referentially to refer to an object distinct from the semantic denotation of the description, and, moreover, to refer to an object even when the description has no semantic denotation. In one sense, this phenomenon arises within the framework of Barwise and Perry's treatment of descriptions. If we understand the semantic denotation of a description to be the unique object that satisfies the content of the description, if there is one, then Barwise and Perry would allow that there are referential uses of a description D that contribute objects other than the semantic denotation of D to the propositions expressed by uses of sentences in which D occurs. But this is only because Barwise and Perry allow that a description may be evaluated at a resource situation other than the complete situation in order to arrive at its denotation on a given occasion of use. Still, the denotation of the description relative to a given resource situation is the unique object in the situation that satisfies the description relative to that situation. The referential uses of descriptions that Donnellan gives of do seem to arise by evaluation of descriptions at alternative resource situations, but rather through the *referential intentions' of the speaker in his of the description. aspect of referential use is a rather a semantic phenomenon and is best analyzed in terms of the distinction between semantic reference and speaker's reference elaborated in Kripke [10771. the following discourses from Kripke [10771: 'any situation on which the speaker can focus attention is a potential candidate for a resource situation with to which the speaker may value load uses of descriptions. Such resource situations must contain a unique object satisfies description. husband is kind to her. NO. isn't. The can you're referring to isn't her husband. Her husband to He her isn't her husband. With (6a) and (7a), Kripke has in mind a case like the one discussed in Donnellan [19661, in which a speaker uses a description to refer to something other than the referent of that description, the unique thing that satisfies the description (if there is one). Kripke analyzes this case as an instance of the general phenomenon of a clash of intentions in language use. In the case at hand, the speaker has a general intention to use the description to refer to its semantic referent; his intention, distinct from general semantic intention, is to use it to refer to a particular individual. He incorrectly believes that these two intentions coincide this gives rise to a use of referring expression in which the speaker's reference reference are (The speaker's referent is presumably the woman's lover). From our point of view, the importance of the case resides in its showing that Cf(S) may include more than one entity, that is realized by a single NP in S. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). This can be seen by observing that both discourses seem equally appropriate and that the backward-looking centers of (6b) and (7b) are the husband and the lover, respectively, realized by their anaphoric elements. Hence, the forward-looking centers of a sentence may be related not semantically but to the that realize Hence, the importance of the referential/attributive distinction from our point of view is that it leads to cases in which the centers of a sentence may be pragmatically rather than semantically related to the noun phrases that realize them. 5. Center Movement and Center Realization-- Constraints En the foregoing sections we have discussed a number of examples to illustrate two essential points. First, the noun phrase that realizes the backward-looking center of an utterance in a discourse cannot be determined from the of the utterance alone. Second, the relation c noun phrases centers solely a semantic a pragmatic relation. This discussion has proceeded at a rather intuitive level, without explicit elaboration of the framework we regard as appropriate for dealing with centering and its role in explaining discourse phenomena. Before going on to describe constraints on the realization relation that are, of course, several alternative explanations; e.g., the may believe that the description is more likely than to be interpreted correctly by the hearer. Ferreting out the in a given situation requires of belief and the like. A discussion of issues is beyond the this paper. 67 explain certain phenomena in discourse, we should be somewhat more explicit about the notions of center and realization. We have said that each utterance S in a discourse has associated with it a backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). What manner of objects are these centers? They are the sort of objects that can serve as the semantic interpretations of singular That is, either they are objects in the world (e.g., planets, people, numbers) or they are functions from possible worlds (situations, etc.) to objects in the world that can be used to interpret definite descriptions. That is, whatever serves to interpret a definite noun phrase can be a center. For the sake of concreteness in many of the examples in the preceding discussion, we have relied on the situation semantics of Barwise and Perry. The theory we are developing does not depend on this particular semantical treatment of definite noun phrases, but it does require several of the distinctions that treatment provides. In particular, our theory requires a semantical treatment that accommodates the distinction between interpretations of definite noun phrases that contribute their content to the propositions expressed by sentences in which they occur and interpretations that contribute only their denotation—in other words, the distinction between value-free and value-loaded interpretations. As noted, a distinction of this sort can be effected within the framework of 'possible-worlds' approaches to the semantics of natural language. In addition, we see the need for interpretations of definite noun phrases to be dependent on their discourse context. Once again, this is a feature of interpretations that is accommodated in the relational approach to semantics advocated by Barwise and Perry, but it might be accommodated within other as Given that Cb(S), the center of sentence S in a discourse, is the interpretation of a definite noun phrase, how does it become related to S? In a typical example, S will contain a full definite noun phrase or pronoun that realizes the center. The realization relation is neither nor pragmatic. For example, realizes c in cases where a definite description and is interpretation, or an object related to it by a 'speaker's reference.' More when is pronoun, the principles that which c are such that realizes c from neither semantics nor pragmatics exclusively. They are principles that must be elicited from the study of itself. A tentative formulation of some principles is given below. it is typical that, when a center of S, S an that realizes c, is by no means necessary. In particular, for sentences containing noun treatment of our theory we will consider centers that are realized by constituents in other syntactic categories. 119831 discusses some of these issues and compares several of with Montague semantics. phrases that express functional relations (e.g., 'the door,' 'the owner') whose arguments are not exhibited explicitly (e.g., a house is the current center, but so far its door nor its owner has been it is the case that such argument can be backward-looking center of the sentence. We are studying such and expect to integrate that into our theory of discourse The basic rule that constrains the realization of the backward-looking center of an utterance is a constraint on the speaker, namely: the Cb the current utterance is the same as the of the previous utterance, a pronoun should be are two things to about this rule. First, it not preclude using for other entities as long as one is used for the center. Second, it is not a hard but rather principle, like a Gricean maxim, that violated. However, such violations lead at best to in which the is forced to draw additional inferences. simple example, consider the following sequence, assuming at the outset that John is the center of the discourse: (8a) He called up Mike yesterday. (he=John) (8b) He vas annoyed by John's call. Linguistic theories typically assign various linguistic phenomena to one of the categories, syntactic, semantic, or pragmatic, as if the phenomena in each category were relatively independent of those in the others. However, various phenomena in discourse do not seem to yield comfortably to any account that is strictly a syntactic or semantic or pragmatic one. This paper focuses on particular phenomena of this sort—the use of various referring expressions such as definite noun phrases and pronouns—and examines their interaction with mechanisms used to maintain discourse coherence. Even a casual survey of the literature on definite descriptions and referring expressions reveals not only defects in the individual accounts provided by theorists (from several different disciplines), but also deep confusions about the roles that syntactic, semantic, and pragmatic factors play in accounting for these phenomena. The research we have undertaken is an attempt to sort out some of these confusions and to create the basis for a theoretical framework that can account for a variety of discourse phenomena in which all three factors of language use interact. The major premise on which our research depends is that the concepts necessary for an adequate understanding of the phenomena in question are not exclusively either syntactic or semantic or pragmatic. The next section of this paper defines two levels of discourse coherence and describes their roles in accounting for the use of singular definite noun phrases. To illustrate the integration of factors in explaining the uses of referring expressions, their use on one of these levels, i.e., the local one, is discussed in Sections 3 and 4. This account requires introducing the notion of the centers of a sentence in a discourse, a notion that cannot be defined in terms of factors that are exclusively syntactic or semantic or pragmatic. In Section 5, the interactions of the two levels with these factors and their effects on the uses of referring expressions in discourse are discussed. A discourse comprises utterances that combine into subconstituents of the discourse, namely, units of discourse that are typically larger than a single sentence, but smaller than the complete discourse. However, the constituent structure is not determined solely by the linear sequence of utterances. It is common for two contiguous utterances to be members of different subconstituents of the discourse (as with breaks between phrases in the syntactic analysis of a sentence); likewise, it is common for two utterances that are not contiguous to be members of the same subconstituent. An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. That is, discourses have been shown to have two levels of coherence. Global coherence refers to the ways in which the larger segments of discourse relate to one another. It depends on such things as the function of a discourse, its subject matter, and rhetorical schema (Grosz, 1977, 1981; Reichman, 19811. Local coherence refers to the ways in which individual sentences bind together to form larger discourse segments. It depends on such things as the syntactic structure of an utterance, ellipsis, and the use of pronominal referring expressions 1Sidner, 19811. The two levels of discourse coherence correspond to two levels of focusing—global focusing and centering. Participants are said to be globally focused on a set of entities relevant to the overall discourse. These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811. In contrast. centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns (Sidner, 1979; Joshi and Weinstein, 19811. The two levels of focusing/coherence have different effects on the processing of pronominal and nonpronominal definite noun phrases. Global coherence and focusing are major factors in the generation and interpretation of nonpronominal definite referring expressions.- Local coherence and centering have greater effect on the processing of pronominal expressions. In Section 5 we shall describe the rules governing the use of these kinds of expressions and shall explain why additional processing by the hearer (needed for drawing additional inferences) is involved when pronominal expressions are used to refer to globally focused entities or nonpronominal expressions are used to refer to centered entities. Many approaches to language interpretation have ignored these differences, depending instead on powerful inference mechanisms to identify the referents of referring expressions. Although such approaches may suffice, especially for well-formed texts, they are insufficient in general. In particular, such approaches will not work for generation. Here the relationships among focusing, coherence, and referring expressions are essential and must be explicitly provided for. Theories—and systems based on them--will generate unacceptable uses of referring expressions if they do not take these relationships into account.3 In our theory, the centers of a sentence in a discourse serve to integrate that sentence into the discourse. Each sentence, S, has a single backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). Cb(S) serves to link S to the preceding discourse, while Cf(S) provides a set of entities to which the succeeding discourse may be linked. To avoid confusion, the phrase 'the center' will he used to refer only to Cb(S). To clarify the notion of center, we will consider a number of discourses illustrating the various factors that are combined in its definition (abstractly) and in its identification in a discourse. In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse. We begin by showing that the center cannot be defined in syntactic terms alone. The interaction of semantics and centering is more complex and is discussed in Section 4. The following examples, drawn from Reinhart [19821, illustrate the point that the notion of center is not syntactically definable,4 i.e., the syntax of a sentence S does not determine which of its NPs realizes Cb(S). (The reasons for the use of this terminology are discussed in Section 4.) Although (lb) and (2b) are identical, Cb(lb) is Max and Cb(2b) is Rosa. This can be seen in part by noticing that 'He saw Rosa' seems more natural than (Ib) and *Max saw her' than (2b) (a fact consistent with the centering rule introduced in Section 5.) The subject NP is the center in one context, the object NP in the other. Even when the NP used to realize Cb(S) can be syntactically determined, the Cb(S) itself is not yet fully determined, for Cb(S) is typically not a linguistic entity (i.e., it is not a particular linguistic expression). Rosa, not 'Rosa' is the Cb(2b). Consider. the discourse: Here, Cb(3c) is Rosa, but clearly would not be in other contexts where the expression 'her' still realized the backward-looking center of 'Max saw her.' This is seen most simply by considering the discourse that would result if &quot;How is Joan?' replaced (3a). In the discourse that resulted, Joan, not Rosa, would be the center of (3c). The interactions of semantic and pragmatic factors with centering and their effects on referring expressions are more complex than the preceding discussion suggests. In the examples given above, the NPs that realize Cb(S) also denote it, but this is not always the case: we used the term 'realize' in the above discussion advisedly. In this section, we consider two kinds of examples in which the center of a sentence is not simply the denotation of some noun phrase occurring in the sentence. First, we will examine several examples in which the choice of and interaction among different kinds of interpretations of definite noun phrases are affected by the local discourse context (i.e., centering). Second, the role of pragmatic factors in some problematic cases of referential uses of definite descriptions [Donnellan 19661 is discussed. The distinction between realization and semantic denotation is necessary to treat the interaction between value-free and value-loaded interpretations [Barwise and Perry, 19821 of definite descriptions, as they occur in extended discourse. Consider, for example, the following sequence: (4a) The vice president of the United States is also president of the Senate. (4b) Historically, he is the president's key man in negotiations with Congress. (4b') As Ambassador to China, he handled aany tricky negotiations, so he is well prepared for this job. Cb(4b) and Cb(4b') are each realized by the anaphoric element 'he.' But (4b) expresses the same thing as 'Historically, the vice president of the United States is the president's key man in negotiations with Congress' (in which it is clear that no single individual vice president is being referred to) whereas (4b1 expresses the same thing as, 'As ambassador to China, the [person who is now] vice president of the United States handled many tricky negotiations,...' This can be accounted for by observing that 'the vice president of the United States' contributes both its value-free interpretation and its value-loading at the world type to Cf(4a). Cb(4b) is then the value-free interpretation and Cb(4b') is the valueloading, i.e., George Bush. In this example, both value-free and value-loaded interpretations are shown to stern from the same full definite noun phrase. It is also possible for the movement of the center from a value-free interpretation (for Cb(S)) to a value-loaded interpretation (for Cb of the next sentence)--or vice versa—to be accomplished solely with pronouns. That is, although (4b)-(4b1 is (at least for some readers) not a natural dialogue, similar sequences are possible. There appear to be strong constraints on the kinds of transitions that are allowed. In particular, if a given sentence forces either the value-free or value-loaded interpretation, then only that interpretation becomes possible in a subsequent sentence. However, if some sentence in a given context merely prefers one interpretation while allowing the other, then either one is possible in a subsequent sentence. (6a) The vice president of the United States is also president of the Senate. (5b) He's the president's key aan in negotiations vith Congress. in which 'he' may be interpreted as either value-free (IT) or value-loaded (VL), may be followed by either of the following two sentences: (Sc) As ambassador to China, he handled zany tricky negotiations. (VL) (Sc') He is required to be at least 36 years old. (VF) However, if we change (5b) to force the value-loaded interpretation, as in (5b&quot;), then only (5c) is possible. (Sb') Right now he is the president's key man in negotiations with Congress. Similarly, if (5b) is changed to force the value-free interpretation, as in (4b), then only (5c') is possible. If an intermediate sentence allows both interpretations but prefers one in a given context, then either is possible in the third sentence. A use with preference for a valueloaded interpretation followed by a use indicating the value-free interpretation is illustrated in the sequence: John thinks that the telephone is a toy. He plays with it every day. (11 preferred; VT. ok) He doesn't realize that it is an invention that changed the world. (VF) The preference for a value-free interpretation that is followed by a value-loaded one is easiest to see in a dialogue situation; sl. The vice president of the United States is also president of the Senate. s2: I thought he played some important role in the House. (VF preferred; VL ok) SI: He did, but that was before he was VP. (W.) From these examples, it might appear that the concepts of value-free and value-loaded interpretation are identical to DonneIlan's 119661 attributive and referential uses of noun phrases. However, there is an important difference between these two distinctions. The importance to our theory is that the referential use of definite noun phrases introduces the need to take pragmatic factors (in particular speaker intention) into account, not just semantic factors. Donnellan [1966] describes the referential and attributive uses of definite descriptions in the following way: 'A speaker who uses a definite description attributively in an assertion states something about whoever or whatever is the so-and-so. A speaker who uses a definite description referentially in an assertion, on the other hand, uses the description to enable his audience to pick out whom or what he is talking about and states something about that person or thing. In the first case the definite description might be said to occur essentially, for the speaker wishes to assert something about whatever or whoever fits that description; but in the referential use the definite description is merely one tool for doing a certain job--calling attention to a person or thing--and in general any other device for doing the same job, another description or a name, would do as well. In the attributive use, the attribute of being the so-and-so is all important, while it is not in the referential use.' The distinction Donnellan suggests can be formulated in terms of the different propositions a sentence S containing a definite description D may be used to express on different occasions of use. When D is used referentially, it contributes its denotation to the proposition expressed by S; when it is used attributively, it contributes to the proposition expressed by S a semantic interpretation related to the descriptive content of D. The identity of this semantic interpretation is not something about which Donnellan is explicit. Distinct formal treatments of the semantics of definite descriptions in natural language would construe the appropriate interpretation differently. In semantic treatments based on possible worlds, the appropriate interpretation would be a (partial) function from possible worlds to objects; in the situation semantics expounded by Barwise and Perry, the appropriate interpretation is a (partial) function from resource situations5 to objects. As just described, the referential-attributive distinction appears to be exactly the distinction that Barwise and Perry formulate in terms of the value-loaded and valuefree interpretations of definite noun phrases. But this gloss omits an essential aspect of the referentialattributive distinction as elaborated by Donnellan. In DonneIlan's view, a speaker may use a description referentially to refer to an object distinct from the semantic denotation of the description, and, moreover, to refer to an object even when the description has no semantic denotation. In one sense, this phenomenon arises within the framework of Barwise and Perry's treatment of descriptions. If we understand the semantic denotation of a description to be the unique object that satisfies the content of the description, if there is one, then Barwise and Perry would allow that there are referential uses of a description D that contribute objects other than the semantic denotation of D to the propositions expressed by uses of sentences in which D occurs. But this is only because Barwise and Perry allow that a description may be evaluated at a resource situation other than the complete situation in order to arrive at its denotation on a given occasion of use. Still, the denotation of the description relative to a given resource situation is the unique object in the situation that satisfies the description relative to that situation. The referential uses of descriptions that Donnellan gives examples of do not seem to arise by evaluation of descriptions at alternative resource situations, but rather through the *referential intentions' of the speaker in his use of the description. This aspect of referential use is a pragmatic rather than a semantic phenomenon and is best analyzed in terms of the distinction between semantic reference and speaker's reference elaborated in Kripke [10771. Consider the following discourses drawn from Kripke [10771: 5Roughly, 'any situation on which the speaker can focus attention is a potential candidate for a resource situation with respect to which the speaker may value load his uses of definite descriptions. Such resource situations must contain a unique object which satisfies the description. (6a) Her husband is kind to her. (61)) NO. he isn't. The can you're referring to isn't her husband. (7a) Her husband is kind to her. (7b) He is kind to her but he isn't her husband. With (6a) and (7a), Kripke has in mind a case like the one discussed in Donnellan [19661, in which a speaker uses a description to refer to something other than the semantic referent of that description, i.e., the unique thing that satisfies the description (if there is one). Kripke analyzes this case as an instance of the general phenomenon of a clash of intentions in language use. In the case at hand, the speaker has a general intention to use the description to refer to its semantic referent; his specific intention, distinct from his general semantic intention, is to use it to refer to a particular individual. He incorrectly believes that these two intentions coincide and this gives rise to a use of the referring expression 'her husband&quot; in which the speaker's reference and the semantic reference are distinct.•6 (The speaker's referent is presumably the woman's lover). From our point of view, the importance of the case resides in its showing that Cf(S) may include more than one entity, that is realized by a single NP in S. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). This can be seen by observing that both discourses seem equally appropriate and that the backward-looking centers of (6b) and (7b) are the husband and the lover, respectively, realized by their anaphoric elements. Hence, the forward-looking centers of a sentence may be related not semantically but pragmatically to the NPs that realize them. Hence, the importance of the referential/attributive distinction from our point of view is that it leads to cases in which the centers of a sentence may be pragmatically rather than semantically related to the noun phrases that realize them. En the foregoing sections we have discussed a number of examples to illustrate two essential points. First, the noun phrase that realizes the backward-looking center of an utterance in a discourse cannot be determined from the syntax of the utterance alone. Second, the relation N realizes c between noun phrases N and centers c is neither solely a semantic nor solely a pragmatic relation. This discussion has proceeded at a rather intuitive level, without explicit elaboration of the framework we regard as appropriate for dealing with centering and its role in explaining discourse phenomena. Before going on to describe constraints on the realization relation that explain certain phenomena in discourse, we should be somewhat more explicit about the notions of center and realization. We have said that each utterance S in a discourse has associated with it a backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). What manner of objects are these centers? They are the sort of objects that can serve as the semantic interpretations of singular noun phrases.7 That is, either they are objects in the world (e.g., planets, people, numbers) or they are functions from possible worlds (situations, etc.) to objects in the world that can be used to interpret definite descriptions. That is, whatever serves to interpret a definite noun phrase can be a center. For the sake of concreteness in many of the examples in the preceding discussion, we have relied on the situation semantics of Barwise and Perry. The theory we are developing does not depend on this particular semantical treatment of definite noun phrases, but it does require several of the distinctions that treatment provides. In particular, our theory requires a semantical treatment that accommodates the distinction between interpretations of definite noun phrases that contribute their content to the propositions expressed by sentences in which they occur and interpretations that contribute only their denotation—in other words, the distinction between value-free and value-loaded interpretations. As noted, a distinction of this sort can be effected within the framework of 'possible-worlds' approaches to the semantics of natural language. In addition, we see the need for interpretations of definite noun phrases to be dependent on their discourse context. Once again, this is a feature of interpretations that is accommodated in the relational approach to semantics advocated by Barwise and Perry, but it might be accommodated within other approaches as wel1.8 Given that Cb(S), the center of sentence S in a discourse, is the interpretation of a definite noun phrase, how does it become related to S? In a typical example, S will contain a full definite noun phrase or pronoun that realizes the center. The realization relation is neither semantic nor pragmatic. For example, N realizes c may hold in cases where N is a definite description and e is its denotation, its value-free interpretation, or an object related to it by a 'speaker's reference.' More importantly, when N is a pronoun, the principles that govern which c are such that N realizes c derive from neither semantics nor pragmatics exclusively. They are principles that must be elicited from the study of discourse itself. A tentative formulation of some such principles is given below. Though it is typical that, when c is a center of S, S contains an N such that N realizes c, it is by no means necessary. In particular, for sentences containing noun In 3 hiller treatment of our theory we will consider centers that are realized by constituents in other syntactic categories. 8Israel 119831 discusses some of these issues and compares several properties of situation semantics with Montague semantics. phrases that express functional relations (e.g., 'the door,' 'the owner') whose arguments are not exhibited explicitly (e.g., a house is the current center, but so far neither its door nor its owner has been mentioned),9 it is sometimes the case that such an argument can be the backward-looking center of the sentence. We are currently studying such cases and expect to integrate that study into our theory of discourse phenomena. The basic rule that constrains the realization of the backward-looking center of an utterance is a constraint on the speaker, namely: If the Cb of the current utterance is the same as the Cb of the previous utterance, a pronoun should be used. There are two things to note about this rule. First, it does not preclude using pronouns for other entities as long as one is used for the center. Second, it is not a hard rule, but rather a principle, like a Gricean maxim, that can be violated. However, such violations lead at best to conditions in which the hearer is forced to draw additional inferences. As a simple example, consider the following sequence, assuming at the outset that John is the center of the discourse: (8a) He called up Mike yesterday. (he=John) (8b) He vas annoyed by John's call. (8b) is unacceptable, unless it is possible to consider the introduction of a second person named 'John.' However, intervening sentences that provide for a shift in center from John to Mike (e.g., 'He was studying for his driver's test') suffice to make (8b) completely acceptable. Sidner's discourse focus corresponds roughly to Cb(S), while her potential foci correspond approximately to Cf(S). However, she also introduces an actor focus to handle multiple pronouns in a single utterance. The basic centering rule not only allows us to handle the same examples more simply, but also appears to avoid one of the complications in Sidner's account. Example D4 from Sidner [1981) illustrates this problem: (9-1)I haven't seen Jeff for several days. (9-2)Carl thinks he's studying for his exams. (9-3)But I think he went to the Cape with Linda. On Sidner's account, Carl is the actor focus after (9-2) and Jeff is the discourse focus (Cb(9-2)). Because the actor focus is preferred as the referrent of pronominal expressions, Carl is the leading candidate for the entity referred to by he in (9-3). It is difficult to rule this case out without invoking fairly special rules. On our account, Jeff is Cb(9-2) and there is no problem. The addition of actor focus was made to handle multiple pronouns--for example, if (9-3) were replaced by The center rule allows such uses, without introducing a gGrosz 119771 refers to this as 'implicit focusing'; other examples are presented in Joshi and Weinstein 119811. second kind of focus (or center), by permitting entities other than Cb(S) to be pronominalized as long as Cb(S) is.io Two aspects of centering affect the kinds of inferences a hearer must draw in interpreting a definite description. First, the shifting of center from one entity to another requires recognition of this change. Most often such changes are affected by the use of full definite noun phrases, but in some instances a pronoun may be used. For example, Grosz [19771 presents several examples of pronouns being used to refer to objects mentioned many utterances back. Second, the hearer must process (interpret) the particular linguistic expression that realizes the center. Most previous attempts to account for the interaction of different kinds of referring expressions with centering and focusing (or 'topic') have conflated these two. For example, Joshi and Weinstein [19811 present a preliminary report on their research regarding the connection between the computational complexity of the inferences required to process a discourse and the coherence of that discourse as assessed by measures that invoke the centering phenomenon. However, several of the examples combine changes of expression and shifts in centering. Violations of the basic centering rule require the hearer to draw two different kinds of inferences. The kind required depends on whether a full definite noun phrase is used to express the center or whether a pronoun is used for a noncentered entity. We will consider each case separately. Several different functions may be served by the use of a full definite noun phrase to realize the currently centered entity. For instance, the full noun phrase may include some new and unshared information about the entity. In such cases, additional inferences arise from the need to determine that the center has not shifted and that the properties expressed hold for the centered entity. For example, in the following sequences the full definite noun phrases that are in boldface do more than merely refer. When the current center is not pronominalized (it may not be present in the sentence), the use of a pronoun to express an entity other than the current center, is strongly constrained. The particular cases that have been identified involve instances in which attention is being shifted back to a previously centered entity (e.g., Grosz, 1977; Reichman, 1978) or to one element of a set that is currently centered. In such cases, additional inferences 10Obviously, if Cb(S) is not expressed in the next sentence then this issue does not arise. are required to determine that the pronoun does not refer to the current center, as well as to identify the context back to which attention is shifting. These shifts, though indicated by linguistic expressions typically used for centering (pronouns), correspond to a shift in global focus. The main purpose of the paper was to sort out the confusion about the roles of syntactic, semantic, and pragmatic factors in the interpretation and generation of definite noun phrases in discourse. Specific mechanisms that account for the interactions among these factors were presented. Discourses were shown to be coherent at two different levels, i.e., with referring expressions used to identify entities that are centered locally and those focused upon more globally. The differences between references at the global and local levels were discussed, and the interaction of the syntactic role of a given noun phrase and its semantic interpretation with centering was described.
Unsupervised Learning of Narrative Schemas and their Participants We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. This paper describes a new approach to event semantics that jointly learns event relations and their participants from unlabeled corpora. The early years of natural language processing (NLP) took a “top-down” approach to language understanding, using representations like scripts (Schank and Abelson, 1977) (structured representations of events, their causal relationships, and their participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. This paper addresses two areas of work in event semantics, narrative event chains and semantic role labeling. We begin by highlighting areas in both that can mutually inform each other through a narrative schema model. Narrative Event Chains are partially ordered sets of events that all involve the same shared participant, the protagonist (Chambers and Jurafsky, 2008). A chain contains a set of verbs representing events, and for each verb, the grammatical role filled by the shared protagonist. An event is a verb together with its constellation of arguments. An event slot is a tuple of an event and a particular argument slot (grammatical relation), represented as a pair (v, d) where v is a verb and d E {subject, object, prep}. A chain is a tuple (L, O) where L is a set of event slots and O is a partial (temporal) ordering. We will write event slots in shorthand as (X pleads) or (pleads X) for (pleads, subject) and (pleads, object). Below is an example chain modeling criminal prosecution. In this example, the protagonist of the chain is the person being prosecuted and the other unspecified event slots remain unfilled and unconstrained. Chains in the Chambers and Jurafsky (2008) model are ordered; in this paper rather than address the ordering task we focus on event and argument induction, leaving ordering as future work. The Chambers and Jurafsky (2008) model learns chains completely unsupervised, (albeit after parsing and resolving coreference in the text) by counting pairs of verbs that share coreferring arguments within documents and computing the pointwise mutual information (PMI) between these verb-argument pairs. The algorithm creates chains by clustering event slots using their PMI scores, and we showed this use of co-referring arguments improves event relatedness. Our previous work, however, has two major limitations. First, the model did not express any information about the protagonist, such as its type or role. Role information (such as knowing whether a filler is a location, a person, a particular class of people, or even an inanimate object) could crucially inform learning and inference. Second, the model only represents one participant (the protagonist). Representing the other entities involved in all event slots in the narrative could potentially provide valuable information. We discuss both of these extensions next. The Chambers and Jurafsky (2008) narrative chains do not specify what type of argument fills the role of protagonist. Chain learning and clustering is based only on the frequency with which two verbs share arguments, ignoring any features of the arguments themselves. Take this example of an actual chain from an article in our training data. Given this chain of five events, we want to choose other events most likely to occur in this scenario. One of the top scoring event slots is (fly X). Narrative chains incorrectly favor (fly X) because it is observed during training with all five event slots, although not frequently with any one of them. An event slot like (charge X) is much more plausible, but is unfortunately scored lower by the model. Representing the types of the arguments can help solve this problem. Few types of arguments are shared between the chain and (fly X). However, (charge X) shares many arguments with (accuse X), (search X) and (suspect X) (e.g., criminal and suspect). Even more telling is that these arguments are jointly shared (the same or coreferent) across all three events. Chains represent coherent scenarios, not just a set of independent pairs, so we want to model argument overlap across all pairs. The second problem with narrative chains is that they make judgments only between protagonist arguments, one slot per event. All entities and slots in the space of events should be jointly considered when making event relatedness decisions. As an illustration, consider the verb arrest. Which verb is more related, convict or capture? A narrative chain might only look at the objects of these verbs and choose the one with the highest score, usually choosing convict. But in this case the subjects offer additional information; the subject of arrest (police) is different from that of convict (judge). A more informed decision prefers capture because both the objects (suspect) and subjects (police) are identical. This joint reasoning is absent from the narrative chain model. The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just described, Chambers and Jurafsky (2008) offers an unsupervised approach to event learning (goal 1), but lacks semantic role knowledge (goal 2). The following sections describe a model that addresses both goals. The next sections introduce typed narrative chains and chain merging, extensions that allow us to jointly learn argument roles with event structure. The first step in describing a narrative schema is to extend the definition of a narrative chain to include argument types. We now constrain the protagonist to be of a certain type or role. A Typed Narrative Chain is a partially ordered set of event slots that share an argument, but now the shared argument is a role defined by being a member of a set of types R. These types can be lexical units (such as observed head words), noun clusters, or other semantic representations. We use head words in the examples below, but we also evaluate with argument clustering by mapping head words to member clusters created with the CBC clustering algorithm (Pantel and Lin, 2002). We define a typed narrative chain as a tuple (L, P, O) with L and O the set of event slots and partial ordering as before. Let P be a set of argument types (head words) representing a single role. An example is given here: As mentioned above, narrative chains are learned by parsing the text, resolving coreference, and extracting chains of events that share participants. In our new model, argument types are learned simultaneously with narrative chains by finding salient words that represent coreferential arguments. We record counts of arguments that are observed with each pair of event slots, build the referential set for each word from its coreference chain, and then represent each observed argument by the most frequent head word in its referential set (ignoring pronouns and mapping entity mentions with person pronouns to a constant PERSON identifier). As an example, the following contains four worker mentions: But for a growing proportion of U.S. workers, the troubles really set in when they apply for unemployment benefits. Many workers find their benefits challenged. The four bolded terms are coreferential and (hopefully) identified by coreference. Our algorithm chooses the head word of each phrase and ignores the pronouns. It then chooses the most frequent head word as the most salient mention. In this example, the most salient term is workers. If any pair of event slots share arguments from this set, we count workers. In this example, the pair (X find) and (X apply) shares an argument (they and workers). The pair ((X find),(X apply)) is counted once for narrative chain induction, and ((X find), (X apply), workers) once for argument induction. Figure 1 shows the top occurring words across all event slot pairs in a criminal scenario chain. This chain will be part of a larger narrative schema, described in section 3.4. We now formalize event slot similarity with arguments. Narrative chains as defined in (Chambers and Jurafsky, 2008) score a new event slot (f, g) against a chain of size n by summing over the scores between all pairs: where C is a narrative chain, f is a verb with grammatical argument g, and sim(e, e') is the pointwise mutual information pmi(e, e'). Growing a chain by one adds the highest scoring event. We extend this function to include argument types by defining similarity in the context of a specific argument a: where A is a constant weighting factor and freq(b, b', a) is the corpus count of a filling the arguments of events b and b'. We then score the entire chain for a particular argument: sim((ei, di) , (ej, dj) , a) (3) Using this chain score, we finally extend chainsim to score a new event slot based on the argument that maximizes the entire chain’s score: The argument is now directly influencing event slot similarity scores. We will use this definition in the next section to build Narrative Schemas. Whereas a narrative chain is a set of event slots, a Narrative Schema is a set of typed narrative chains. A schema thus models all actors in a set of events. If (push X) is in one chain, (Y push) is in another. This allows us to model a document’s entire narrative, not just one main actor. A narrative schema is defined as a 2-tuple N = (E, C) with E a set of events (here defined as verbs) and C a set of typed chains over the event slots. We represent an event as a verb v and its grammatical argument positions D„ C_ {subject, object, prep}. Thus, each event slot (v, d) for all d E D„ belongs to a chain c E C in the schema. Further, each c must be unique for each slot of a single verb. Using the criminal prosecution domain as an example, a narrative schema in this domain is built as in figure 2. The three dotted boxes are graphical representations of the typed chains that are combined in this schema. The first represents the event slots in which the criminal is involved, the second the police, and the third is a court or judge. Although our representation uses a set of chains, it is equivalent to represent a schema as a constraint satisfaction problem between (e, d) event slots. The next section describes how to learn these schemas. Previous work on narrative chains focused on relatedness scores between pairs of verb arguments (event slots). The clustering step which built chains depended on these pairwise scores. Narrative schemas use a generalization of the entire verb with all of its arguments. A joint decision can be made such that a verb is added to a schema if both its subject and object are assigned to chains in the schema with high confidence. For instance, it may be the case that (Y pull over) scores well with the ‘police’ chain in figure 3. However, the object of (pull over A) is not present in any of the other chains. Police pull over cars, but this schema does not have a chain involving cars. In contrast, (Y search) scores well with the ‘police’ chain and (search X) scores well in the ‘defendant’ chain too. Thus, we want to favor search instead of pull over because the schema is already modeling both arguments. This intuition leads us to our event relatedness function for the entire narrative schema N, not just one chain. Instead of asking which event slot (v, d) is a best fit, we ask if v is best by considering all slots at once: where CN is the set of chains in our narrative N. If (v, d) does not have strong enough similarity with any chain, it creates a new one with base score Q. The Q parameter balances this decision of adding to an existing chain in N or creating a new one. We use equation 5 to build schemas from the set of events as opposed to the set of event slots that previous work on narrative chains used. In Chambers and Jurafsky (2008), narrative chains add the best (e, d) based on the following: where m is the number of seen event slots in the corpus and (vj, gj) is the jth such possible event slot. Schemas are now learned by adding events that maximize equation 5: where |v |is the number of observed verbs and vj is the jth such verb. Verbs are incrementally added to a narrative schema by strength of similarity. Figures 3 and 4 show two criminal schemas learned completely automatically from the NYT portion of the Gigaword Corpus (Graff, 2002). We parse the text into dependency graphs and resolve coreferences. The figures result from learning over the event slot counts. In addition, figure 5 shows six of the top 20 scoring narrative schemas learned by our system. We artificially required the clustering procedure to stop (and sometimes continue) at six events per schema. Six was chosen as the size to enable us to compare to FrameNet in the next section; the mean number of verbs in FrameNet frames is between five and six. A low Q was chosen to limit chain splitting. We built a new schema starting from each verb that occurs in more than 3000 and less than 50,000 documents in the NYT section. This amounted to approximately 1800 verbs from which we show the top 20. Not surprisingly, most of the top schemas concern business, politics, crime, or food. Most previous work on unsupervised semantic role labeling assumes that the set of possible automatically built from the verb ‘convict’. Each node shape is a chain in the schema. classes is very small (i.e, PropBank roles ARG0 and ARG1) and is known in advance. By contrast, our approach induces sets of entities that appear in the argument positions of verbs in a narrative schema. Our model thus does not assume the set of roles is known in advance, and it learns the roles at the same time as clustering verbs into frame-like schemas. The resulting sets of entities (such as {police, agent, authorities, government} or {court, judge, justice}) can be viewed as a kind of schema-specific semantic role. How can this unsupervised method of learning roles be evaluated? In Section 6 we evaluate the schemas together with their arguments in a cloze task. In this section we perform a more qualitative evalation by comparing our schema to FrameNet. FrameNet (Baker et al., 1998) is a database of frames, structures that characterize particular situations. A frame consists of a set of events (the verbs and nouns that describe them) and a set of frame-specific semantic roles called frame elements that can be arguments of the lexical units in the frame. FrameNet frames share commonalities with narrative schemas; both represent aspects of situations in the world, and both link semantically related words into frame-like sets in which each predicate draws its argument roles from a frame-specific set. They differ in that schemas focus on events in a narrative, while frames focus on events that share core participants. Nonetheless, the fact that FrameNet defines frame-specific argument roles suggests that comparing our schemas and roles to FrameNet would be elucidating. We took the 20 learned narrative schemas described in the previous section and used FrameNet to perform qualitative evaluations on three aspects of schema: verb groupings, linking structure (the mapping of each argument role to syntactic subject or object), and the roles themselves (the set of entities that constitutes the schema roles). Verb groupings To compare a schema’s event selection to a frame’s lexical units, we first map the top 20 schemas to the FrameNet frames that have the largest overlap with each schema’s six verbs. We were able to map 13 of our 20 narratives to FrameNet (for the remaining 7, no frame contained more than one of the six verbs). The remaining 13 schemas contained 6 verbs each for a total of 78 verbs. 26 of these verbs, however, did not occur in FrameNet, either at all, or with the correct sense. Of the remaining 52 verb mappings, 35 (67%) occurred in the closest FrameNet frame or in a frame one link away. 17 verbs (33%) deliberate deadlocked found convict acquit occurred in a different frame than the one chosen. We examined the 33% of verbs that occurred in a different frame. Most occurred in related frames, but did not have FrameNet links between them. For instance, one schema includes the causal verb trade with unaccusative verbs of change like rise and fall. FrameNet separates these classes of verbs into distinct frames, distinguishing motion frames from caused-motion frames. Even though trade and rise are in different FrameNet frames, they do in fact have the narrative relation that our system discovered. Of the 17 misaligned events, we judged all but one to be correct in a narrative sense. Thus although not exactly aligned with FrameNet’s notion of event clusters, our induction algorithm seems to do very well. Linking structure Next, we compare a schema’s linking structure, the grammatical relation chosen for each verb event. We thus decide, e.g., if the object of the verb arrest (arrest B) plays the same role as the object of detain (detain B), or if the subject of detain (B detain) would have been more appropriate. We evaluated the clustering decisions of the 13 schemas (78 verbs) that mapped to frames. For each chain in a schema, we identified the frame element that could correctly fill the most verb arguments in the chain. The remaining arguments were considered incorrect. Because we assumed all verbs to be transitive, there were 156 arguments (subjects and objects) in the 13 schema. Of these 156 arguments, 151 were correctly clustered together, achieving 96.8% accuracy. The schema in figure 5 with events detain, seize, arrest, etc. shows some of these errors. The object of all of these verbs is an animate theme, but confiscate B and raid B are incorrect; people cannot be confiscated/raided. They should have been split into their own chain within the schema. Argument Roles Finally, we evaluate the learned sets of entities that fill the argument slots. As with the above linking evaluation, we first identify the best frame element for each argument. For example, the events in the top left schema of figure 5 map to the Manufacturing frame. Argument B was identified as the Product frame element. We then evaluate the top 10 arguments in the argument set, judging whether each is a reasonable filler of the role. In our example, drug and product are correct Product arguments. An incorrect argument is test, as it was judged that a test is not a product. We evaluated all 20 schemas. The 13 mapped schemas used their assigned frames, and we created frame element definitions for the remaining 7 that were consistent with the syntactic positions. There were 400 possible arguments (20 schemas, 2 chains each), and 289 were judged correct for a precision of 72%. This number includes Person and Organization names as correct fillers. A more conservative metric removing these classes results in 259 (65%) correct. Most of the errors appear to be from parsing mistakes. Several resulted from confusing objects with adjuncts. Others misattached modifiers, such as including most as an argument. The cooking schema appears to have attached verbal arguments learned from instruction lists (wash, heat, boil). Two schemas require situations as arguments, but the dependency graphs chose as arguments the subjects of the embedded clauses, resulting in 20 incorrect arguments in these schema. The previous section compared our learned knowledge to current work in event and role semantics. We now provide a more formal evaluation against untyped narrative chains. The two main contributions of schema are (1) adding typed arguments and (2) considering joint chains in one model. We evaluate each using the narrative cloze test as in (Chambers and Jurafsky, 2008). The cloze task (Taylor, 1953) evaluates human understanding of lexical units by removing a random word from a sentence and asking the subject to guess what is missing. The narrative cloze is a variation on this idea that removes an event slot from a known narrative chain.Performance is measured by the position of the missing event slot in a system’s ranked guess list. This task is particularly attractive for narrative schemas (and chains) because it aligns with one of the original ideas behind Schankian scripts, namely that scripts help humans ‘fill in the blanks’ when language is underspecified. We count verb pairs and shared arguments over the NYT portion of the Gigaword Corpus (years 1994-2004), approximately one million articles. We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepositional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and argument head words. We use the OpenNLP1 coreference engine to resolve entity mentions. The test set is the same as in (Chambers and Jurafsky, 2008). 100 random news articles were selected from the 2001 NYT section of the Gigaword Corpus. Articles that did not contain a protagonist with five or more events were ignored, leaving a test set of 69 articles. We used a smaller development set of size 17 to tune parameters. The first evaluation compares untyped against typed narrative event chains. The typed model uses equation 4 for chain clustering. The dotted line ‘Chain’ and solid ‘Typed Chain’ in figure 6 shows the average ranked position over the test set. The untyped chains plateau and begin to worsen as the amount of training data increases, but the typed model is able to improve for some time after. We see a 6.9% gain at 2004 when both lines trend upwards. The second evaluation compares the performance of the narrative schema model against single narrative chains. We ignore argument types and use untyped chains in both (using equation 1 instead of 4). The dotted line ‘Chain’ and solid ‘Schema’ show performance results in figure 6. Narrative Schemas have better ranked scores in all data sizes and follow the previous experiment in improving results as more data is added even though untyped chains trend upward. We see a 3.3% gain at 2004. The final evaluation combines schemas with argument types to measure overall gain. We evaluated with both head words and CBC clusters as argument representations. Not only do typed chains and schemas outperform untyped chains, combining the two gives a further performance boost. Clustered arguments improve the results further, helping with sparse argument counts (‘Typed Schema’ in figure 6 uses CBC arguments). Overall, using all the data (by year 2004) shows a 10.1% improvement over untyped narrative chains. Our significant improvement in the cloze evaluation shows that even though narrative cloze does not evaluate argument types, jointly modeling the arguments with events improves event clustering. Likewise, the FrameNet comparison suggests that modeling related events helps argument learning. The tasks mutually inform each other. Our argument learning algorithm not only performs unsupervised induction of situation-specific role classes, but the resulting roles and linking structures may also offer the possibility of (unsupervised) FrameNet-style semantic role labeling. Finding the best argument representation is an important future direction. The performance of our noun clusters in figure 6 showed that while the other approaches leveled off, clusters continually improved with more data. The exact balance between lexical units, clusters, or more general (traditional) semantic roles remains to be solved, and may be application specific. We hope in the future to show that a range of NLU applications can benefit from the rich inferential structures that narrative schemas provide. This work is funded in part by NSF (IIS-0811974). We thank the reviewers and the Stanford NLP Group for helpful suggestions.
Name Tagging With Word Clusters And Discriminative Training We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material. At a recent meeting, we presented name-tagging technology to a potential user. The technology had performed well in formal evaluations, had been applied successfully by several research groups, and required only annotated training examples to configure for new name classes. Nevertheless, it did not meet the user's needs. To achieve reasonable performance, the HMM-based technology we presented required roughly 150,000 words of annotated examples, and over a million words to achieve peak accuracy. Given a typical annotation rate of 5,000 words per hour, we estimated that setting up a name finder for a new problem would take four person days of annotation work – a period we considered reasonable. However, this user's problems were too dynamic for that much setup time. To be useful, the system would have to be trainable in minutes or hours, not days or weeks. We left the meeting thinking about ways to reduce training requirements to no more than a few hours. It seemed that three existing ideas could be combined in a way that might reduce training requirements sufficiently to achieve the objective. First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999). The resulting clusters appeared to contain a great deal of implicit semantic information. This implicit information, we believed, could serve to augment a small amount of annotated data. Particularly promising were techniques for producing hierarchical clusters at various scales, from small and highly specific to large and more general. To benefit from such information, however, we would need an automatic learning mechanism that could effectively exploit it. Fortunately, a second line of recent research provided a potential solution. Recent work in discriminative methods (Lafferty et al., 2001; Sha and Pereira, 2003, Collins 2002) suggested a framework for exploiting large numbers of arbitrary input features. These methods seemed to have exactly the right characteristics for incorporating the statistically-correlated hierarchical word clusters we wished to exploit. Combining these two methods, we suspected, would be sufficient to drastically reduce the number of annotated examples required. However, we also hoped that a third technique, active learning (Cohn et al., 1996; McCallum and Nigam, 1998), would be particularly effective when used in conjunction with hierarchical word clusters. Specifically, active learning attempts to select examples for annotation by estimating the system's certainty about the answer, requesting a human judgment only for those cases where it is most uncertain. Unfortunately, the issue often comes down to whether a specific word has previously been observed in training: if the system has seen the word, it is certain, if not, it is uncertain. Word clusters at various scales, we hoped, would permit more subtle distinctions to influence the system's certainty, increasing the method’s effectiveness earlier in the process when fewer training examples have been annotated. We view clustering here as a method for estimating the probabilities of low frequency events, particularly events that are likely to go unobserved in a small annotated training corpus. For example, a clustering mechanism may choose to place AT&T in the same cluster as other company names based on contextual similarity. Then, even if the word AT&T was not previously annotated as a company, it may nonetheless be possible to infer that AT&T indeed is a company because it occupies a cluster that is populated mostly by other company names. Likewise, cluster membership can be used to exploit information from neighboring words. For example, if the word reported has previously been observed to follow person names, but the word announced has not yet been seen, it may be possible to guess that the word preceding announced is a person based on the fact that reported and announced occupy the same cluster. A practical obstacle to using clusters for this purpose is selecting an appropriate level of granularity: too small, and the clusters provide insufficient generalization; too large, and they inappropriately overgeneralize. Hierarchical clusters provide one way around the problem by avoiding commitment to any particular granularity in advance. However, the dominant trend during the past decade toward generative models has made integration of such hierarchical clusters difficult. Because the nested clusters surrounding each word are highly correlated, it is unreasonable to treat them as independent. Unfortunately, any treatment in a generative framework other than independent requires considerable ingenuity. Interestingly, before generative models began to dominate parsing, the Spatter parser (Magerman, 1995) achieved extremely promising results using a nongenerative statistical model. Of particular interest is the fact that Spatter used hierarchical word clusters for estimating its lexical attachment probabilities. However, the statistical decision trees underlying Spatter’s probability model never gained widespread acceptance, and indeed, our own limited experience with them yielded mixed results. In the past few years, researchers have begun to view generative models as instances of a broader class of linear (or log-linear) models, and have introduced discriminative methods (e.g. conditional random fields) to estimate the model parameters. These estimation methods do not impose the same strict independence conditions as generative models. Armed with modern discriminative training methods, it seemed reasonable to us to revisit hierarchical clustering. Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990). We implemented this algorithm twice as part of our work. The first implementation derived directly from the description given in the Brown paper. Then, in the hope of achieving greater efficiency, we reverseengineered the clustering software in Spatter. While the mathematical details differ slightly between the two algorithms, both aim to cluster together words so as to minimize the bigram language-model perplexity of the unsupervised corpus. In practice, we observed no significant differences in accuracy when using one or the other in our experiments. All experimental results given in this paper are with the Spatter clustering algorithm. The result of running the clustering algorithm is a binary tree, where each word occupies a single leaf node, and where each leaf node contains a single word. The root node defines a cluster containing the entire vocabulary. Interior nodes represent intermediate size clusters containing all of the words that they dominate. Thus, nodes higher in the tree correspond to larger word clusters, while lower nodes correspond to smaller clusters. A particular word can be assigned a binary string by following the traversal path from the root to its leaf, assigning a 0 for each left branch, and a 1 for each right branch. The following are example bit strings from the Spatter clustering algorithm: To implement discriminative training, we followed the averaged perceptron approach of (Collins, 2002). Our decision was based on three criteria. First, the method performed nearly as well as the currently best global discriminative model (Sha and Pereira, 2003), as evaluated on one of the few tasks for which there are any published results (noun phrase chunking). Second, convergence rates appeared favorable, which would facilitate multiple experiments. Finally, and most important, the method appeared far simpler to implement than any of the alternatives. algorithm exactly as described by Collins. However, we did not implement cross-validation to determine when to stop training. Instead, we simply iterated for 5 epochs in all cases, regardless of the training set size or number of features used. Furthermore, we did not implement features that occurred in no training instances, as was done in (Sha and Pereira, 2003). We suspect that these simplifications may have cost several tenths of a point in performance. A set of 16 tags was used to tag 8 name classes (the seven MUC classes plus the additional null class). Two tags were required per class to account for adjacent elements of the same type. For example, the string Betty Mary and Bobby Lou would be tagged as PERSON-START PERSON-START NULL-START PERSON-START PERSON-CONTINUE. Our model uses a total of 19 classes of features. The first seven of these correspond closely to features used in a typical HMM name tagger. The remaining twelve encode cluster membership. Clusters of various granularity are specified by prefixes of the bit strings. Short prefixes specify short paths from the root node and therefore large clusters. Long prefixes specify long paths and small clusters. We used 4 different prefix lengths: 8 bit, 12 bit, 16 bit, and 20 bit. Thus, the clusters decrease in size by about a factor of 16 at each level. The complete set of features is given in Table 2. We implemented the averaged perceptron training We used only a rudimentary confidence measure to perform active learning, introducing no additional features beyond those used in training and decoding. The confidence score we assign to a sentence is just the un-normalized difference in perceptron scores between the highest scoring theory and the second highest scoring alternative. To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word. We define the confidence at a word to be the difference between the summed forward and backward scores of the best and second best tags for that word. The confidence for the entire sentence is then just the minimum of the scores at each word position. We performed our experiments using the seven MUC-6 name categories: person, organization, location, date, time, percent, and monetary amount. For annotated data, we used text from Sections 02-23 of the Wall Street Journal Treebank corpus that had previously been annotated with the MUC name classes. Sections 02-21 were used as training material, and Section 23 was used as test (note that the syntactic trees were not used in any way). Scoring was performed using the MUC scorer. For unsupervised clustering data, we used the Wall Street Journal subset of the Continuous Speech Recognition (CSR-III) collection (LDC catalog # LDC95T6). This portion of the collection contains approximately 100 million words. Active learning experiments were performed by permitting the system to choose examples from among the pool of annotated data, rather than presenting the examples in their natural chronological order. This approach, previously used in [Boschee et al, 2002], permits simulation of human-in-the-loop experiments that are inexpensive to run and repeatable because they don’t actually involve a human annotator. However, because the pool of pre-annotated examples is limited, the results are most meaningful for small training sets. Once the system has selected the most useful examples from the pool, it is forced to choose among the remainder that it previously rejected as less useful. At the extreme where all available examples are used, our experimental framework prevents active learning from exhibiting any benefit whatsoever since the system is left no choice in selecting examples. Before considering the impact of word clustering on system performance, we first evaluate the discriminative tagger relative to the baseline HMM. For this experiment, we used all of the features described in Section 3 except word cluster features. The remaining features encode essentially the same information used in the HMM, although in a slightly different form. Results are shown in Figure 1. For very small and very large training sets, the systems perform about the same. Between these extremes, the discriminative tagger exhibits somewhat, though not distressingly, worse performance. We conjecture that lack of smoothing in the discriminative tagger may account for the difference. Second, we consider the impact of word clusters. Figure 2 compares performance of the discriminative tagger, now with cluster features included, to the baseline HMM. Immediately, with only 5,000 words of training, the discriminative model significantly outperforms the HMM. With 50,000 words of training, performance for the discriminative model exceeds 90F, a level not reached by the HMM until it has observed 150,000 words of training. Somewhat surprisingly, the clusters continue to provide some benefit even with 1,000,000 words of training. At this operating point, the discriminative tagger achieves an F-score of 96.08 compared to 94.72 for the HMM, a 25% reduction in error. Third, we consider the impact of active learning. Figure 3 shows (a) discriminative tagger performance without cluster features, (b) the same tagger using active learning, (c) the discriminative tagger with cluster features, and (d) the discriminative tagger with cluster features using active learning. Both with and without clusters, active learning exhibits a noticeable increase in learning rates. However, the increase in learning rate is significantly more pronounced when cluster features are introduced. We attribute this increase to better confidence measures provided by word clusters – the system is no longer restricted to whether or not it knows a word; it now can know something about the clusters to which a word belongs, even if it does not know the word. Finally, Figure 4 shows the impact of consolidating the gains from both cluster features and active learning compared to the baseline HMM. This final combination achieves an F-score of 90 with less than 20,000 words of training – a quantity that can be annotated in about 4 person hours – compared to 150,000 words for the HMM – a quantity requiring nearly 4 person days to annotate. At 1,000,000 word of training, the final combination continues to exhibit a 25% reduction in error over the baseline system (because of limitations in the experimental framework discussed earlier, active learning can provide no additional gain at this operating point). The work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Lin et al., 2003; Boschee et al, 2002; Collins and Singer, 1999; Yarowsky, 1995) that all focuses on reducing annotation requirements through a combination of (a) seed examples, (b) large unannotated corpora, and (c) training example selection. Moreover, our work is based largely on existing techniques for word clustering (Brown et al., 1990), discriminative training (Collins 2002), and active learning. The synthesis of these techniques, nevertheless, proved highly effective in achieving our primary objective of reducing the need for annotated data. Much work remains to be done. In an effort to move rapidly toward our primary objective, we investigated only one type of discriminative training (averaged perceptron), only one type of clustering (bigram mutual information), and only one simple confidence measure for active learning. It seems likely that some additional gains could be realized by alternative discriminative methods (e.g. conditional random fields estimated with conjugate-gradient training). Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance. On the application side, it would be interesting to apply the technique to other language problems. Applying it to parsing would yield a rare sense of closure, knitting together the word clustering of Magerman’s (1995) Spatter parser – arguably the first successful broadcoverage statistical parser – with structural elements of the now-dominant Collins (1997) style parsers. Because our combined method promises to require substantially less training data, it may also prove useful for so-called low-density languages, where limited resources – and even more limited numbers of native speakers – are available. For the moment, we find the initial results encouraging. We achieved a 25% reduction in error on a standard named-entity problem, compared to a state-of-the-art HMM. Our main objective, though, was not reducing error rates but rather reducing the amount of annotation required. At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.
Effects of Adjective Orientation and Gradability on Sentence Subjectivity Vas i le ios  Hatz ivass i log lou Depar tment  o1 Computer  Sc ience Co lumbia  Un ivers i l y New York,  NY  10027 vh@cs ,  co lumbia ,  edu Janyce  M.  Wiebe Depar tment  o f  Computer  Sc ience New Mex ico  State Un ivers i ty Las  Cruces ,  NM 88003 w iebe@cs ,  nmsu. edu Abstract Subjectivity is a pragmatic, sentence-level feature that has important implications for texl processing applica- lions such as information exlractiou and information ic- lricwd. We study tile elfeels of dymunic adjectives, se- mantically oriented adjectives, and gradable ad.ieclivcs on a simple subjectivity classiiicr, and establish lhat lhcy arc strong predictors of subjectivity. A novel train- able mclhod thai statistically combines two indicators of gradability is presented and ewlhlalcd, complementing exisling automatic Icchniques for assigning orientation labels. 1 I n t roduct ion In recent years, computalional tcchniqt,es for the deter- mination of &:deal semantic features have been proposed and ewdualed. Such features include sense, register, do- main spccilicity, pragmatic restrictions on usage, scnlan- lic markcdncss, and orientation, as well as automatically ictcnlifiecl links between words (e.g., semantic rclalcd- hess, syllollynly, antonylny, and tneronymy). Aulomal- ically learning features of this type from hugc corpora allows the construction or augmentation of lexicons, and the assignment of scmanlic htbcls lo words and phrases in running text. This information in turn can bc used to help dcterlninc addilional features at the It?teal, clause, sentence, or document level. Tiffs paper explores lira benelits that some lexical fea- tures of adjectives offer lor the prediction of a contexlual sentence-level feature, suOjectivity. Subjectivity in nat- ural language re[crs to aspects of language used to ex- press opinions and ewfluations. The computatiomtl task addressed here is to distinguish sentences used to present opinions and other tbrms of subjectivity (suOjective sen- tences, e.g., "At several different layers, its a fascinating title") from sentences used to objectively present factual information (objective sentences, e.g., "Bell industries Inc. increased its quarterly to 10 cents from 7 cents a share"). Much research in discourse processing has focused on task-oriented and insmmtional dialogs. The task ad- dressed here comes to the fore in other genres, especially news reporting and lnternet lorums, in which opinions of various agents are expressed and where subjectivity judgements couht help in recognizing inllammatory rues- sages ("llanles) and mining online sources for product reviews. ()thor (asks for whicll subjectivity recognition is potentially very useful include infornmtion extraction and information retrieval. Assigning sub.icctivity labels to documents or portions of documents is an example of non-topical characteri?ation f information. Current in- formation extraction and rolricval lechnology focuses al- most exclusively on lhe subject matter of the documcnls. Yet, additiomtl components of a document inllucncc its relevance to imrlicuhu ? users or tasks, including, for ex- alnple, the evidential slatus el: lhc material presented, and attitudes adopted in fawn" or against a lmrticular person, event, or posilion (e.g., articles on a presidenlial cam- paign wrillen to promote a specific candidate). In sum- marization, subjectivity judgmcnls could be included in documcllt proiilcs to augment aulomatically produced docunacnt summaries, and to hel l) the user make rele- vance judgments when using a search engine. ()thor work on sub.iectivity (Wicbc et al., 1999; Bruce and Wicbc, 2000) has established a positive and statisti- cally signilicant correlation with the presence of adiec- lives. ?incc the mere presence of one or iDoic adjectives is useful for prcdicling (hat a scntcrtce is subjective we investigate ill this paper (lie cflccts of additional cxical scmanlic lcalurcs of adjectives that can be automatically learned from corpora. We consider two such l%atures: se- mantic orientation, which represents an ewdualivc har- acterization of a words deviation from the norm for its semantic group (e.g., beauti/ul is positively oriented, as opposed to ugly); and gradability, which characterizes a words ability to express a property in wlrying degrees. In lira remainder of this paper, we [irst address adjec- tive orientation in Section 2, summarizing a previously published method for automatically separating oriented adjectives into positive and negative classes. Then, Sec- tion 3 presents a novel method for learning gradablc ad- jectives using a largo corpus and a statistical feature com- bination naodel. In Section 4, we review earlier exper- iments on testing subjectivity using wuious fcatt, res as predictors, and then present comparative analyses of the effects that orientation and gradability have on our abil- ity to Inedict sentence subjectivity from adjectives. Wc show that both give us higher-quality features for recog- nizing st@icctive sentences, and conclude by discussing future extensions to Ibis work. 299 Ct Number of Number of Average nnmber Ratio o1 average adjectives in links in of links for Accuracy test set (IAc~l) test set (IL~I) each adjective group frequencies 730 2,568 7.04 78.08% 1.8699 516 2,159 8.37 82.56% 1.9235 369 1,742 9.44 87.26% 1.3486 236 1,238 10.49 92.37% 1.4040 Table 1: Evaluation o1 the adjective orientation classification and labeling methods (from (Hatzivassiloglou and McK- eown, 1997)). 2 Semantic Orientation The semantic orientation or polarity of a word indicates the direction the word deviates flom the norm for its se- mantic group or lexicalfield (Lehrer, 1974). It is an eval- uative characteristic (Battistella, 1990) of the meaning of the word which restricts its usage to appropriate prag- matic contexts. Words that encode a desirable state (e.g., beautiful, unbiased) have a positive orientation, while words that represent undesirable states have a negative orientation. Within tile particular syntactic lass o1 ad- jectives, orientation can be expressed as the ability of an adjective to ascribe in general a positive or negative qual- ity to the modified item, making it better or worse than a similar unmodilied item. Most antonymous adjectives can be contrasted on the basis of orientation (e.g., beautil)d-ugly); similarly, nearly synonymous terms are often distinguished by dill fcrent orientations (e.g., simple-siml)listic). While ori- entation applies to many adjectives, there are also those that have no orientation, typically as members of groups of complementary, qualitative terms (Lyons, 1977) (e.g., domestic, medical, or red). Since orientation is inher- ently connected with cwduative judgements, it appears to be a promising feature for predicting subjectivity. Hatzivassiloglou and McKeown (1997) presented a method for autonmtically assigning a + or - orientation label to adjectives known to have some semantic orien- tation. Their method is based on information extracted fiom conjunctions between adjectives in a large corpus I because orientation constrains the use of the words in specific contexts (e.g., compare corrupt and brutal with *corrupt but brutal), observed conjunctions of adjectives can be exploited to inler whether the conjoined words are of the same or different orientation. Using a shallow parser on a 21 million word corpus of Wall Street Jour- nal articles, Hatzivassiloglou and McKeown developed and trained a log-linear statistical model that predicts whether any two adjectives have the same orientation with 82% accuracy. The predicted links o1 same or dil L ferent orientation are automatically assigned a strength value (essentially, a confidence stimate) by tile model, and induce a graph that can be partitioned with a clus- tering algorithm into components so that all words in the same component belong to the same orientation class. Once the classes have been determined, flequency infor- mation is used to assign positive or negative labels to each class (there are slightly fewer positive terms, but with a significantly higher rate of occurrence than nega- tive terms). Hatzivassiloglou and McKeown applied their method to 1,336 (657 positive and 679 negative) adjectives which were all the oriented adjectives appearing in the corpus 20 times or more. Orientation labels were assigned to these adjectives by hand. I Subsequent validation of the initial selection and label assignment steps with indepen- dent human judges showed an agreement of 89% tor tile first step and 97% for the second step, establishing that orientation is a fairly objective semantic property. Be- cause the accuracy ol the method depends on the den- sity of conjunctions per adjective, Hatzivassiloglou and MeKeown tested separately their algorithm for adjectives appearing in at least 2, 3, 4, or 5 conjunctions in the co l pus; their results are shown in Table I. In this paper, we use the model labels assigned by hand by Hatzivassiloglou and McKeown, and tile labels automatically obtained by their method and reported in (Hatzivassiloglou and McKeown, 1997) with the follow- ing extension: An adjective that appears in k conjunc- tions will receive (possibly different) labels when ana- lyzed together with all adjectives appearing in at least 2, 3 . k conjunctions; since performance generally in- creases with the number of conjunctions per adjective, we select as the orientation label the one assigned by the experi,nent t,sing the highest applicable conjunctions threshold. Overall, we have labels for 730 adjectives 2, with a prediction accuracy of 81.51%. 3 Gradability Gradability (or grading) (Sapir, 1944; Lyons, 1977, p. 27 I) is the semantic property that enables a word to par- ticipate in comparative constructs and to accept mod- ifying expressions that act as intensitiers or diminish- ers. Gradable adjectives express properties in varying degrees ot strength, relative to a norm either explicitly ISome adjectives with unclem; mnbiguous, or conlexl,-dependenl orientation were excluded. 2Those appearing in the corpus in two conjunctions or inore, since some conjunction data nlust be left out to hain the link prediction algo- rithm. 300 cold Unmodilied by grading words Moditied by grading words civil Unmodilied by grading words Modified by grading words Uninllected 392 20 1,296 1 Inllected for degree 18 0 0 0 litble 2: Extracted wdues of gradability indicators, i.e., frequencies of the word with or without he specitied intlection or moditication, for two adjectives, one gradable (cold) and one primarily non-gradable (civil). The frequencies were compt, ted liom the 1987 Wall Street Journal corpus. mentioned or implicitly supplied by the modilied noun (for example, asmall planet is usually much larger thart a large house; cf. the distinction between absolute and tel- alive adjectives made by Katz (1972, p. 254)). This rel- ativism in the interpetation of gradable words indicates that gradability is likely to be a good predictor ?71 subjec- tivity. 3.1 Indicators ofgradability Most gradable words appear at least several times in a large corpt, s either in forms inflected for degree (i.e., comparative and superlative), or in tile context of grading modilicrs such as veo,. However, non-gradable words may also occasionally appear in such contexts or forms under exceptional circumstances. For example, ve O, dead can be used tk)r emphasis, and re&let am~ re&let (as in "her lhce became redder and redder") can be used to indicate a progression of coloring, qb distinguish be- tween truly gradablc adjectives and non-gradable adjec- tives in these exceptional contexts, we have developed a trainable log-linear statistical model that lakes into ac- count tile number of times an ad.iective has been observed in a form or context indicating gradability relative to the number of limes it has been seen in non-gradable con- texts. We use a shallow parser to retrieve from a large corpus tagged for part-of-speech with Churchs PARTS tagger (Church, 1988) all adjectives and their modifiers. Al- though the most common use of an adverb modifying an adjective is to function as an intensilier or diminisher (Quirk et al., 1985, p. 445), adverbs can also add to tile semantic ontent of the adjectival phrase instead of pro- viding a grading effect (e.g., immediately available, po- litically vuhmrable), or function as cmphasizers, adding to the force o1 tile base adjective and not lo its degree (e.g., virtually impossible; compare *re O, impossible). Therefore, we compiled by hand a list of 73 adverbs and noun phrases (such as a little, exceedingly, somewhat, and veo) that are fiequently used as grading moditicrs. The number of times each adjective appears mod ilied by a term form this list becomes a first indicator of gradabil- ity. To detect inflected forms o1 adjectives (which, in 15> glish, always indicate gradability st, bject to the excel> tions discussed earlier), we have implemented an auto- matic lnorphology analysis component. This program recognizes several irregular forms (e.g., good-better- best) and strips tile grading suffixes -er and -est Dora regularly inllected adjectives, producing a list of candi- date base forms that if inflected would yield tilt origi- nal adjective (e.g., bigger produces three potential forms, big, bigg, and bigge). The frequency of these candi- date base words is checked against ile corpus, and tile form with signilicantly higher frequency is selected. To guard against cases of base adjective forms that end in -er or-est (e.g., sih,er), the original word is also included alllong tile candidates. The total number of times this procedure is successfully applied for each adjective be- comes a second indicator of gradability. 3.2 l )etermlnlng radabil l ty The presence or absence of each of the above two indica- tors results in a 2 x 2 frequency table IBr each adjective; examples for one gradable and one non-gradable adjec- tive are given in "lhble 2. "lb convert lhese four numbers to a single decision on tile gradability of tile ad.iective, we use a log-linear model. Ix)g-linear models (Nantnef and l)ufly, 1989) construct a linear combination (weighted sum) of the predictor wlriables 1~, i=1 and relate it to the actual response H. (in this case, 0 for non-gmdable and 1 for gradable) via the so-called logis- tic trcm,sJbrmation, 1~- I -t- eJ Maximum likelihood estimates for the coefficients fli are obtained from training samples for which the correct response H, is known, using the iterative reweighted non- linear least squares algorithm (Bates and Watts, 1988). This statistical model is particularly suited for model- ing variables with a "yes"-"no" (binary) value, because, unlike linear models, it captures the dependency of IFs variance on its mean (Santner and Dully, 1989). We normalize the counts for the two indicators of g,adability, and the cot, at otjoint occurrences of both in- tleetion and modilication by grading moditiers, by divid- ing with the total frequency of the adjective in the corpus. In this manner, we obtain three real-valued predictors 301 Classitied as gradable: acceptable accurate afraid aware busy careful cautious el~eap creative critical dangerous different disappointing equal fair fanfiliar far favorable formal free frequent good grand inadequate intense interesting legitimate likely positive professional reasonable rich short-term significant slow solid sophisticated sound speculative thin tight tough uucertain widespread worth Classilied as non-gradable: additional alleged alternative annual antitrust automatic ertain criminal cumulative daily deputy domestic ldcrly false linaneial first-quarter full hefty illegal institutional internal egislative long-distance military min imum monthly moral national official one-time other outstanding present prior prospective punitive regional scientific secondary sexual subsidiary taxable three-nmnth three-year total tremendous two-year unfifir unsolicited upper vohmtary white wholesale world-wide wrong Figure 1: Automatically obtained classification of a sample of 100 adjectives as gradable or not. Correct decisions (according to the COBU1LD-based reference model) are indicated in bold. ,  3 for the log-linear model. We also con- sider a modilied model, where any adjective for which any occurrence of simultaneous inflection and modilica- tion has been detected is automatically labeled gradable; the remaining two predictors are used to classify the ad- jectives that do not fullill this condition. This modilica- tion is motivated by the fact that observing an adjective in such a context offers a very high likelihood o1 grad- ability. 3.3 Experimental results We extracted from the 1987 Wall Street Journal corpus (21 million words) all adjectives with a frequency o1 300 or more; this produced a collection of 496 words. Grad- ability labels specifying whether each word is gradable or not were manually assigned, using tim designations of the Collins COBUILD (Collins Birmingham Univer- sity International Language Database) dictionary (Sin- clair, 1987). COBUILD marks each sense of each adjec- tive with one of the labels QUALIT, CLASSIF, or COLOR, corresponding to gradable, non-gradable, and color ad- jectives. In cases where COBUILD supplies conflicting labels for different senses of a word, we either omitted that word or, if a sense were predominant, gave it the label of that sense. In some cases, the word did not appear in COBUILD; these typically were descriptive compounds peci[ic to the domain (e.g., anti-takeover, over-the-coullter) and were in most cases marked as non- gradable adjectives. Overall, 453 of tile 496 adjeclives (91.33%) were assigned gradability labels by hand, while the remaining 53 words were discarded because they were misclassitied as adjectives by the part-ol:speech tagger (e.g., such) or because they coukt not be assigned a unique gradability label in accordance with COBUILD. Out of these words, 235 (51.88%) were manually classi- lied as gradable adjectives, and 218 (48.12%) were clas- silied as non-gradablc adjectives. Following the methodology of the preceding subsec- tion, we recovered the inflection and modilication indica- tors for these 453 adjectives, and trained both the unmod- ified and modilied log-linear models rcpcatedly, using a randomly selected subset ol 300 adjectives for training and 100 adjectives for testing. The entire cycle of se- lecting random test and training sets, fitting the models coefficients, making predictions, and evaluating the pre- dicted gradability labels is repeated 100 times, to ensure that the ewtluation is not affected by a lucky (or unlucky) partition of the data between training and test sets. This procedure yields over the 453 adjectives gradability clas- sifications with an average precision o1 93.55% and av- erage recall o1 82.24% (in terms of the gradable words reported or recovered, respectively). The overall accu- racy of the predicted gradability labels is 87.97%. These results were obtained with the modified log-linear model, which slightly ot, tperformed the model that uses all three predictors (in that case, we obtained an average precision of 93.86%, average recall ol 81.70%, and average over- all accuracy o1 87.70%). Figure I lists the gradability labels that were automatically assigned to one of the 100 random test sets ttsing the moditied prediction algorithm. We also assigned automatically labels to the entire set of 453 adjectives, using 4-fold cross-validation (repeatedly training on three-fourths of tim 453 adjectives and test- ing on the rest). This resulted in precision of 94.15%, recall of 82.13%, and accuracy of 88.08% for the entire adjective set. 4 Subjectivity The main motivation for the present paper is to examine the effect that information about an adjectives semantic orientation and gradability has on its probability of oc- curring in a subjective sentence (and hence on its quality as a subjectivity predictor). We tirst review related work on subjectivity recognition and then present our results. 4.1 Previous work on subjectivity recognition In work by Wiebc, Bruce, and OHara (Wiebe ct al., 1999; Bruce and Wicbe, 2000), a corpus of 1,001 sen- tences 3 of the Wall Street Journal TreeBank Corpus 3Conlpoutld sentences were manually segmented into their con- juncts, and each conjtmct treated as a scparale sentence. 302 (Marcus et al., 1993) was nlanually annotated with sub- jeciivity chlssifications. Specifically, each sentence was assigned a subjective or objective classitication, accord- ing to concensus lags derived by a slalistical analysis of lhe chisses assigned by three human judges (see (Wiebe et al., 1999) for further infornmtion). The total nulnber of subjective sentences in lhe data is 486, and the total number of objeclive sentences i 515. Bruce and Wiebe (2000) performed a statistical anal- ysis of the assigned classitications, linding lhat ac(iec- tivcs are statistically signilicantly and positively corre- lated with subjective sentences in the corpus on the basis (, . The proba- of the log-likelihood ratio test statistic -,2 bility of a sentence being subjective, simply given din! there is at least one adjective in lhe sentellee, is 56%, even though there are more objective than subjective sen- lences in the corpus. In addition, Bruce and Wicbe iden- tiffed a type of adjective that is indicative of subjective sentences: those Quirk et al. (1985) term dynamic, which "denote qualities that a,e thoughl to be subjecl to con- trol by the possessor" (p. 434). IZxamples are "kind" and "careful". Bruce and Wiebe nianually applied synlactic tests to identify dynamic adjectives in hall of the corpus nlentioned above. We inclutle such adjectives in the anal- ysis below, to assess whether additional lexical seinantic features associated with subjectivity hel I ) improve pro- dictability. (1999) developed an automatic system to perform st, bjectivily lagging. In 10-fold cross valida- lion experiments applied to the corpus described above, a probabilislic lassilier oblaincd an average accuracy on subjectivity lagging of 72.17%, nlorc Ihan 20 perccnlage poinls higher than the baseline accuracy obtained by al- ways choosing tile nlore frcquent class. A binary feature is included for each of lhe lbllowing: lhe presence in lhe sentence of a plollotln, an adjective, a cardinal number, a modal other fllan will, and an adverb other than #lot. They also inchlded a binary feature representing whether or not the sentence begins a new lxuagraph, l:inally, a feature was included representing co-occurrence of word tokens and punciuation marks with tile sul~jective and ob- jective classilicfition. An analysis of the system showed that the adjective [cature was imporlant to realizing the inlprovolncnts over lllO baseline accuracy, in this ])apci, we use lhe performance of the simple adjcclive fealtue as a baseline, and identify higher quality adjeclive features based on gradability and orienlalion. 4.2 Or ientat ion and gradabi l i ty  as subjectivity predictors: Results We measure the precision of a simple prediction method for subjectivity: a sonlence is classilicd as subjcclivc il at least one nlonlbor of a set of adjectives N occurs in 1he sontonco, alld objeclive otherwise. By wirying 1tlo sot (e.g., all adjeclives, only gradable adjectives, only nega- tively orienied adjectives, etc.) we call assess the t, seful- heSS of ihe additional knowledge for predicting subjec- livity. For the present study, we use tile set of all adjectives automatically identified in tile corpt, s by Wiebc et al. (1999) (Section 4.1 ); the set of dynamic adjectives Ill,{Inu- ally identified by Bruce and Wiebe (2000) (Section 4.1); tile set of scnmntic orientation labels assigned by Hatzi- vassiloglou and McKeown (1997), both manually and automatically with our extension described in Section 2; and the set of gradability labels, both manually and att- tomatically assigned according to the revised log-linear model of Section 3. We calculate restllts (shown in hi- ble 3) for each of lhese sets of all adjectives, dynamic, oriented and gradable adjectives, as well as for unions and intersections of lhose sets. Nole fliat these four sets have been extracted lrom comparable but different cor- pora (different years of the Wall Street Journal), therefore sometimes adjectives in one corpus may not be present in the other corpus, reducing the size of intersection sets. Also, for gradability, we worked with a sample set of 100 adjectives rather than all possible adjectives we could automatically calcuhtte gladabiliiy vahles for, since our goal in the present work is to measure correlations be- tween these sets and sul~jeciivity, rather than building a system for predicling subjectivity for as many ac[iectives as possible. In Table 3, the second cohmm identifies 8, the set of ac[iective types in question. The third cohimn gives the number of subjective sentences that contain one or more instances of members of S, and the fourth colunul gives lhe same ligure for ol~jective sentences. Therefore these two cohinuls together specify lhe coverage of tlm subjectivity indicator examined. The lifth cohimn gives 111c onditional probability that a sentence is subjective. givell that (tile of iilorc illstatices of ti/enlbcl+S of +5; ap- pears. This is a precishm inetrie that assesses feature quality: if inslances of <"7 appear, how likely is the son- tence to be subjective? The last two colunuls contrast the observed conditional probability with the a priori prob- ability of subjective sentellees (i.e., chalice; sixth col- ulnn) and with the probability assigned by the baseline all-adjectives model (i.e., the lirst row in the table; sev- enth colunm). The nlost striking aspect of these results is lhat all sets involviug dynamic adiectives positive or negative po- larity, or gradability are better predictors of sul~jective sentenccs than the class of adjectives as a whole, lqve of the sets are at least 25 points better (LI4, LI6, L21, L23, and L24); four others are at least 20 points better (L2, L9, L13, and 1,15); and live others are at least 15 points better (L4, LI I, 1,18, L20, and 1,22). In most of these cases, the difference between these predictors and all adjectives i  statistically signiticant 4 fit the 5% level or less; ahnost all of these predictors offer statistically sig- nificantly better than even odds in predicting subjectivity correctly. In nlany cases where statistical signilicance Iwe applied achi-square l st Oll the 2 x 2 cross-classificalion able (Fleiss, 1981). Adjeclive Set S # Subj Sents with (s G ,5) + Dyn Adjs fq S of L5. # Obj Sents l(Subj Sent I Significance with (s G ,5) + (~ e S) +) Against maiority Against all adjs All Adjectives 403 321 0.56 0.0041 N/A Dynamic Adjectives 92 32 0.74 1.1989 ? 1.0 - r  1..6369 - 10 -4 Pol+, man 138 87 0.61 0.0007 0.1546 Pol- ,  man 79 37 0.67 0.0001 0.0158 Pol+ U Pol- ,  man 197 114 0.63 6.91.91 ? 10 -~ 0.0260 Grad, man 193 115 0.63 1.9633 ? 10 -~ 0.0440 Not Grad, man 172 147 0.54 0.1084 0.6496 to1+, auto 121 79 0.60 0.0026 0.2537 Pol- ,  auto 61 21 0.74 1.1635 ? 10 -~ 0.0017 PoI+ U Io1--, auto 170 95 0.64 8.5888 - 10 -~ 0.0202 Grad, auto 30 14 0.68 0.0166 0.1418 Not Grad, auto 63 51 0.55 0.2079 0.9363 51 19 0.73 0.0001 0.0081 8.0397.10 -~ Dyn Adjs 71 S of L6. 39 8 0.83 l)yn Adjs 71 S of L I0. 50 19 0.72 0.0002 0.0103 Dyn Adjs 71 S ofLl  I 7 2 0.78 0.1582 0.3220 Grad 71 Pol+, man 90 58 0.61 0.0070 0.2891 Grad 71 Pol-,  man 35 I6 0.69 0.0080 0.09711 Grad 71 (Pol+ U Pol-), man 119 71 0.63 0.0005 0.1000 Grad fl Pol+, auto 13 6 0.68 0.1376 0.3833 Grad n Pol-,  auto 2 0 1.00 0.4556 0.5838 Grad 71 (Pol+ U Pol-), auto 15 6 0.71 0.0636 0.2255 l)yn Adjs N S o1 L22. 4 0 1.00 0.1203 0.2019 I)yn Adjs (1 ,_"; of L19. 24 5 0.83 0.0006 0.0070 Key: (s G ,5)+: one or more instances of members ofS. Ib/+: positive polarity, l b l - :  negalive polarity. GtzM: gradable. Matt: manually identilied. Auto: automalically identified. Table 3: Subjectivity prediction results. 4.3671.. 10 -4 could not established this is due to small counts, caused by the small size of the set of adjectives automatically labeled for gradability. It is also important to note that, in most cases, tile automatically-classified adjectives are comparable or better predictors of subjective sentences than the manually-assigned ones. Comparing tile automatically generated classes with the manually identilied ones, the positive polarity set decreases by 1 percentage point (L3 and L8), while the negative polarity set increases by 7 points (L4 and L9), and the gradable sot increases by 5 percentage points (L6 and LI 1). Among the intersection sets, in two cases the results are lower for tile computer- generated sets (Ll 3/LI 5 and L 14/L 16), but in tile other 4 eases, the results are higher (LI 7/L20, L 18/L21, L19/L2, L24/L23). Finally, the table shows that, in most cases, pro- dictability improves or at worst remains essentially tile same as additional lexical features are considered. For tile set of dynamic adjectives, the predictability is 74% (L2), and improves in 4 of the 6 cases in which it is in- tersected with other sets (LI4, L l6, L23, and L24). For the other two (L 13 and LI 5), predictability is only 1 or 2 points lower (not statistically significant). For the man- ually assigned polarity and gradability sets, in one case predictability is lower (L17 < L6), but in the other cases it remains the same or improves. The results are even better for the automatically assigned polarity and grad- ability sets: predictability improves when both features are considered in all but one case, when predictability remains the same (L20 > L8; L21 > L9; L22 > LI0; and LI 1 _< L20, L21, and L22). 5 Conclusion and Future Work This paper presents an analysis of different adjective fea- tures for predicting subjectivity, showing that tlmy are more precise than those previously used for this task. Wc establish that lexical semantic features uch as seman- tic orientation and gradability determine in large part the subjectivity status of sentences in which they appear. We also present an automatic meflmd for extracting radabil- ity values reliably, complementing earlier work on se- mantic orientation and dynamic adjectives. In addition to finding more precise features for auto- marie subjectivity recognition, this kind of analysis could help efforts to encode subjective features in ontologies such as those described in (Knight and Luk, 1994; Ma- hesh and Nirenburg, 1995; Hovy, 1998). These on- tologies are useful for many NLP tasks, such as ma- chine translation, word-sense disambiguation, and gen- eration. Some subjective features are included in exist- ing ontologies (for example, Mikrokosmos (Mahesh and 304 Nirenburg, 1995) includes atlitude slots). Our corpus- based methods could help in idenlifying more or exlend- ing their coverage. To be able to use automatic subjectivily recognition in texl-processing applications, good ch,cs o1 sub.iccliv- ity mttst be found. The features developed in lhis paper are not only good clues of subjectivity, lhey can be Men- tilied automatically from corpora (see (Hatzivassiloglou and McKeown, 1997), and Section 3 in the present pa- per). In fact, the results in "Iable 3 show that the pre- dictability of the automatically determined gradability and polarity sets is better than or at least comparable to the predictability of the manually determined sets. Thus, tile oriented and gradable adjectives in the particular ap- plication genre can be idenlified fo," use in subjectivity recognition. Ou, efforts in this paper are largely exploratory, aim- ing to establish correlations among tim wlrious features examined. In related work, we have begun to incorporale the features developed herc into systems for recognizing flames and mining reviews in lnternel forums, extend- ing subjectivity judgments froth the sentence to the doc- ument level. In addition, we are seeking ways lo extend the orientation and gradability methods o that individual word occurrences, rather than word lypes, are character- ized as oriented or gradable. We also pla n l{7 incorpo- rate the new features presented here in machine learning models for tile prediction of subjectivity (e.g., (Wiebe ct al., 1999)) and lest lheir interaclions wilh olhcr proposed features. Acknowledglnents This research was SUl~ported in part by the National Sci- ence Foundation under grant number IIS-9817434, and by |he Of lice of Nawtl Research under grant number N00014-95-1-0776. Any opinions, tindings, or recom- mendations a,e those of tile authors, and do not neces- sarily rellect the views of the above agencies. References I)ouglas M. Bates and 1)onald G. Watts. NoMi~> ear Regression Analysis and its Applicatiolls. Wiley, New York. Edwin L. Battistella. Markedness: 7he Evahiative Siq~etwtructure qfLanguage. State University of New York Press, Albany, New York. Rebecca Bruce and ,lanyce Wiebe. Recognizing subjectivity: A case study of rllanual tagging. Natural Language E, gineering, 6(2). Kenneth W. Church. A stochastic paris p,ogranl and noun phrase parser for unrestricted text. In Pro- ceedings of the Second Co,ference o, Applied Natu- ral Language Processing (ANLP-88), pages 136-143, Austin, Texas, February. Association for Computa- tional Linguistics. Statistical Methods for Rates and lmportions. Wiley, New York, 2rid edition. Vasileios Hatzivassiloglou and Kathlcen R. McKeown. Predicting tile semantic orientation of adjec- tives. In Pmeeedi,gs o[ the 35th Annual Meeting q/ the ACL and the 8th Col!/erence o/ rite Europeall Ch(q)ter of the ACL, pages 174-181, Madrid, Spain, July. Association re," Computational Linguistics. Combining and slandardizing large-scale practical ontologies for machine lransla- lion and other uses. In Proceedings of the 1st Interna- tional Conference on Language Resources and Evaht- alien (LREC), Granada, Spain. Jerrold J. Kalz. Sema,tic Theory. Harper and Row, New York. Kevin Knight and Steve K. Luk. Building a large- scale knowledge base liw machine hanslation. Ill Pro- ceedi,gs o[" the 12th Natio,al Co,ference o, Artifi- cial l,telli,q,e, ce (AAAI-94), w)lume 1, pages 773-778, Sealtlc, Washinglon, July-Augt, st. American Associ- ation for Artificial Intelligence. Adrienne Lehrer. Sema, tic l,}elds and Lexical Structztre. North Holland, Amster&tm and New York. Sema,tics, volume 1. Cambridge University Press, Cambridge, England. K. Mahesh and S. Nirenburg. A siluated ontol- ogy for practical NLP. In Pmceedi,gs of the Work- shol~ oil Basic Ontological Issues in Knowledge Shar- ing, 14th lntenmtio,al.loi, t Co,ference oil Artificial Intelligence (LICAI-95), MontrEal, Canada, Augusl. Milchell E /Vlarcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large aunotaled cor- pus of Fmglish: the Penn Treebank. Coml;tttatioltal Lin,~?uistics, 19(2):313-330, June. I~tandolph Quirk, Sidney Grecnbaum, Geoffrey l,eech, alld Jall Svartvik. A Complvhe,sive Grammar elthe English l.cmguage. Longman, London and New York. Sanlner and Diane E. l)uffy. The Statis- tical Analysis of Discrete Data. Springer-Verlag, New York. ()n grading: A study ill semantics. l~hilosol;hy qfScie,ce, 2:93-116. Reprinted in (Sapir, 1949). Selected Wiqtings i, Language, Culture and Personality. University of California Press, Be,keley, California. Edited by David G. Mat> delbat, m. John M. Sinclair (editor in chiet). Collins COBU1LD English Language Dictionary. Collins, London. J. Wiebe, R. Bruce, and T. OHara. Develop- ment and use of a gold standard ata set for subjec- tivity classilieations. In Proceedings of tile 37th An- total Meeting of the Association for Computational Li,guistics (ACL-99), pages 246-253, Universily of Maryhmd, June.
Minimized Models for Unsupervised Part-of-Speech Tagging We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging. The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods. In this paper, we develop new methods for unsupervised part-of-speech tagging. We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type. The goal is to tag each word token so as to maximize accuracy against a gold tag sequence. Whether this is a realistic problem set-up is arguable, but an interesting collection of methods and results has accumulated around it, and these can be clearly compared with one another. We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which a gold tag sequence is available. There are 5,878 word types in this test set. We use the standard tag dictionary, consisting of 57,388 word/tag pairs derived from the entire Penn Treebank.1 8,910 dictionary entries are relevant to the 5,878 word types in the test set. Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data. There are 45 distinct grammatical tags. In this set-up, there are no unknown words. Figure 1 shows prior results for this problem. While the methods are quite different, they all make use of two common model elements. One is a probabilistic n-gram tag model P(ti|ti−n+1...ti−1), which we call the grammar. The other is a probabilistic word-given-tag model P(wi|ti), which we call the dictionary. The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication. They improve this to 83.9% by employing a fully Bayesian approach which integrates over all possible parameter values, rather than estimating a single distribution. They further improve this to 86.8% by using priors that favor sparse distributions. Smith and Eisner (2005) employ a contrastive estimation technique, in which they automatically generate negative examples and use CRF training. In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes. They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank. Goldberg et al. (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence “V V” is disallowed). Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for “the” from six to just one). In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments. The literature omits one other baseline, which is EM with a 2-gram tag model. Here we obtain 81.7% accuracy, which is better than the 3-gram model. It seems that EM with a 3-gram tag model runs amok with its freedom. For the rest of this paper, we will limit ourselves to a 2-gram tag model. We analyze the tag sequence output produced by EM and try to see where EM goes wrong. The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. The Bayesian methods overcome this effect by using priors which favor sparser distributions. But it is not easy to model such priors into EM learning. As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.). We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence). The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus. We see how the rare tag labels (like FW, SYM, etc.) are abused by EM. As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output. We also look at things more globally. We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence. We call this the observed grammar size, and it is 915. That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence. Here, we show a sample word sequence and the corresponding IP network generated for that sequence. can compare it with the gold tagging’s observed grammar size, which is 760. So we can safely say that EM is learning a grammar that is too big, still abusing its freedom. Bayesian sparse priors aim to create small models. We take a different tack in the paper and directly ask: What is the smallest model that explains the text? Our approach is related to minimum description length (MDL). We formulate our question precisely by asking which tag sequence (of the 106425 available) has the smallest observed grammar size. The answer is 459. That is, there exists a tag sequence that contains 459 distinct tag bigrams, and no other tag sequence contains fewer. We obtain this answer by formulating the problem in an integer programming (IP) framework. Figure 2 illustrates this with a small sample word sequence. We create a network of possible taggings, and we assign a binary variable to each link in the network. We create constraints to ensure that those link variables receiving a value of 1 form a left-to-right path through the tagging network, and that all other link variables receive a value of 0. We accomplish this by requiring the sum of the links entering each node to equal to the sum of the links leaving each node. We also create variables for every possible tag bigram and word/tag dictionary entry. We constrain link variable assignments to respect those grammar and dictionary variables. For example, we do not allow a link variable to “activate” unless the corresponding grammar variable is also “activated”. Finally, we add an objective function that minimizes the number of grammar variables that are assigned a value of 1. Figure 3 shows the IP solution for the example word sequence from Figure 2. Of course, a small grammar size does not necessarily correlate with higher tagging accuracy. For the small toy example shown in Figure 3, the correct tagging is “PRO AUX V . PRO V” (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead. For solving the integer program, we use CPLEX software (a commercial IP solver package). Alternatively, there are other programs such as lp solve, which are free and publicly available for use. Once we create an integer program for the full test corpus, and pass it to CPLEX, the solver returns an sponding grammar sizes for the sample word sequence from Figure 2 using the given dictionary and grammar. The IP solver finds the smallest grammar set that can explain the given word sequence. In this example, there exist two solutions that each contain only 4 tag pair entries, and IP returns one of them. objective function value of 459.3 CPLEX also returns a tag sequence via assignments to the link variables. However, there are actually 104378 tag sequences compatible with the 459-sized grammar, and our IP solver just selects one at random. We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%. We also note that CPLEX takes 320 seconds to return the optimal solution for the integer program corresponding to this particular test data (24,115 tokens with the 45-tag set). It might be interesting to see how the performance of the IP method (in terms of time complexity) is affected when scaling up to larger data and bigger tagsets. We leave this as part of future work. But we do note that it is possible to obtain less than optimal solutions faster by interrupting the CPLEX solver. Our IP formulation can find us a small model, but it does not attempt to fit the model to the data. Fortunately, we can use EM for that. We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP. Starting with uniform probabilities, EM finds a tagging that is 84.5% accurate, substantially better than the 81.7% originally obtained with the fully-connected grammar. So we see a benefit to our explicit small-model approach. While EM does not find the most accurate 3Note that the grammar identified by IP is not uniquely minimal. For the same word sequence, there exist other minimal grammars having the same size (459 entries). In our experiments, we choose the first solution returned by CPLEX. sequence consistent with the IP grammar (90.3%), it finds a relatively good one. The IP+EM tagging (with 84.5% accuracy) has some interesting properties. First, the dictionary we observe from the tagging is of higher quality (with fewer spurious tagging assignments) than the one we observe from the original EM tagging. Figure 4 shows some examples. We also measure the quality of the two observed grammars/dictionaries by computing their precision and recall against the grammar/dictionary we observe in the gold tagging.4 We find that precision of the observed grammar increases from 0.73 (EM) to 0.94 (IP+EM). In addition to removing many bad tag bigrams from the grammar, IP minimization also removes some of the good ones, leading to lower recall (EM = 0.87, IP+EM = 0.57). In the case of the observed dictionary, using a smaller grammar model does not affect the precision (EM = 0.91, IP+EM = 0.89) or recall (EM = 0.89, IP+EM = 0.89). During EM training, the smaller grammar with fewer bad tag bigrams helps to restrict the dictionary model from making too many bad choices that EM made earlier. Here are a few examples of bad dictionary entries that get removed when we use the minimized grammar for EM training: in FW a SYM of RP In RBR During EM training, the minimized grammar helps to eliminate many incorrect entries (i.e., zero out model parameters) from the dictionary, thereby yielding an improved dictionary model. So using the minimized grammar (which has higher precision) helps to improve the quality of the chosen dictionary (examples shown in Figure 4). This in turn helps improve the tagging accuracy from 81.7% to 84.5%. It is clear that the IP-constrained grammar is a better choice to run EM on than the full grammar. Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training. In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar). Next, we proceed to recover some good tag bigrams and expand the grammar in a restricted fashion by making use of the higher-quality dictionary produced by the IP+EM method. We now run EM again on the full grammar (all possible tag bigrams) in combination with this good dictionary (containing fewer entries than the full dictionary). Unlike the original training with full grammar, where EM could choose any tag bigram, now the choice of grammar entries is constrained by the good dictionary model that we provide EM with. This allows EM to recover some of the good tag pairs, and results in a good grammardictionary combination that yields better tagging performance. With these improvements in mind, we embark on an alternating scheme to find better models and taggings. We run EM for multiple passes, and in each pass we alternately constrain either the grammar model or the dictionary model. The procedure is simple and proceeds as follows: We notice significant gains in tagging performance when applying this technique. The tagging accuracy increases at each step and finally settles at a high of 91.6%, which outperforms the existing state-of-the-art systems for the 45-tag set. The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al. (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. Figure 5 shows the tagging performance achieved at each step. We found that it is the elimination of incorrect entries from the dictionary (and grammar) and not necessarily the initialization weights from previous EM training, that results in the tagging improvements. Initializing the last trained dictionary or grammar at each step with uniform weights also yields the same tagging improvements as shown in Figure 5. We find that the observed grammar also improves, growing from 459 entries to 603 entries, with precision increasing from 0.94 to 0.96, and recall increasing from 0.57 to 0.76. The figure also shows the model’s internal grammar and dictionary sizes. Figure 6 and 7 show how the precision/recall of the observed grammar and dictionary varies for different models from Figure 5. In the case of the observed grammar (Figure 6), precision increases at each step, whereas recall drops initially (owing to the grammar minimization) but then picks up again. The precision/recall of the observed dictionary on the other hand, is not affected by much. Multiple random restarts for EM, while not often emphasized in the literature, are key in this domain. Recall that our original EM tagging with a fully-connected 2-gram tag model was 81.7% accurate. When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy. Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8). As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models. Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al. (2008). But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data. Because EM is efficient, we can extend our word-sequence trainModel 1 Model 2 Model 3 Model 4 Model 5 ing data from the 24,115-token set to the entire Penn Treebank (973k tokens). We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%. This is our final result on the 45-tagset, and we note that it is higher than previously reported results. Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008). Their systems were shown to obtain considerable improvements in accuracy when using a 17-tagset (a coarsergrained version of the tag labels from the Penn Treebank) instead of the 45-tagset. When tagging the same standard test corpus with the smaller 17-tagset, our method is able to achieve a substantially high accuracy of 96.8%, which is the best result reported so far on this task. The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case (Goldberg et al., 2008). The first row in the table compares tagging results when using a full dictionary (i.e., a lexicon containing entries for 49,206 word types). The InitEM-HMM system from Goldberg et al. (2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008). In comparison, the Bayesian HMM (BHMM) model from Goldwater et al. (2007) and the CE+spl model (Contrastive Estimation with a spelling model) from Smith and Eisner (2005) report lower accuracies (87.3% and 88.7%, respectively). Our system (IP+EM) which uses integer programming and EM, gets the highest accuracy (96.8%). The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank. The IP+EM models used in the 17-tagset experiments reported here were not trained on the entire Penn Treebank, but instead used a smaller section containing 77,963 tokens for estimating model parameters. We also include the accuracies for our IP+EM model when using only the 24,115 token test corpus for EM estimation (shown within parenthesis in second column of the table in Figure 9). We find that our performance does not degrade when the parameter estimation is done using less data, and our model still achieves a high accuracy of 96.8%. The literature also includes results reported in a different setting for the tagging problem. In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types. In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities). Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries. We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word. We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary. Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the tag given a particular suffix (e.g., P(VBG I ing) = 0.97, P(N I ing) = 0.0001, ...). Next, for every unknown word “w”, the trained P(tag I suffix) model is used to predict the top 3 tag possibilities for “w” (using only its suffix information), and subsequently this word along with its 3 tags are added as a new entry to the lexicon. We do this for every unknown word, and eventually we have a dictionary containing entries for all the words. Once the completed lexicon (containing both correct entries for words in the lexicon and the predicted entries for unknown words) is available, we follow the same methodology from Sections 3 and 4 using integer programming to minimize the size of the grammar and then applying EM to estimate parameter values. Figure 9 shows comparative results for the 17tagset case when the dictionary is incomplete. The second and third rows in the table shows tagging accuracies for different systems when a cutoff of 2 (i.e., all word types that occur with frequency counts < 2 in the test corpus are removed) and a cutoff of 3 (i.e., all word types occurring with frequency counts < 3 in the test corpus are removed) is applied to the dictionary. This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary. As the results in Figure 9 illustrate, the IP+EM method clearly does better than all the other systems except for the LDA+AC model. The LDA+AC model from Toutanova and Johnson (2008) has a strong ambiguity class component and uses more features to handle the unknown words better, and this contributes to the slightly higher performance in the incomplete dictionary cases, when compared to the IP+EM model. The method proposed in this paper is simple— once an integer program is produced, there are solvers available which directly give us the solution. In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task. While some previous methods introduced for the same task have achieved big tagging improvements using additional linguistic knowledge or manual supervision, our models are not provided with any additional information. Figure 10 illustrates for the 45-tag set some of the common mistakes that our best tagging model (92.3%) makes. In some cases, the model actually gets a reasonable tagging but is penalized perhaps unfairly. For example, “to” is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO. The model also gets penalized for tagging the word “U.S.” as an adjective (JJ), which might be considered valid in some cases such as “the U.S. State Department”. In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS). Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998). In MDL, there is a single objective function to (1) maximize the likelihood of observing the data, and at the same time (2) minimize the length of the model description (which depends on the model size). However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results. In the past, only a few approaches utilizing MDL have been shown to work for natural language applications. These approaches employ heuristic search methods with MDL for the task of unsupervised learning of morphology of natural languages (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2005). The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques. We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007). We presented a novel method for attacking dictionary-based unsupervised part-of-speech tagging. Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset. The method works by explicitly minimizing the grammar size using integer programming, and then using EM to estimate parameter values. The entire process is fully automated and yields better performance than any existing state-of-the-art system, even though our models were not provided with any additional linguistic knowledge (for example, explicit syntactic constraints to avoid certain tag combinations such as “V V”, etc.). However, it is easy to model some of these linguistic constraints (both at the local and global levels) directly using integer programming, and this may result in further improvements and lead to new possibilities for future research. For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.
A Polynomial-Time Algorithm For Statistical Machine Translation Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code. 6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance. The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce Chinese sentences. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each Chinese sentence e that is to be translated, the system must attempt to find the English sentence e* such that: In the IBM model, the search for the optimal e* is performed using a best-first heuristic &quot;stack search&quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the robustness that is obtained by using very flexible language and translation models. The language model allows sentences of arbitrary order and the translation model allows arbitrary word-order permutation. The models employ no structural constraints, relying instead on probability parameters to assign low probabilities to implausible sentences. This exhaustive space, together with massive number of parameters, permits greater modeling accuracy. But while accuracy is enhanced, translation efficiency suffers due to the lack of structure in the hypothesis space. The translation channel is characterized by two sets of parameters: translation and alignment probabilities.' The translation probabilities describe lexical substitution, while alignment probabilities describe word-order permutation. The key problem is that the formulation of alignment probabilities a(il j, V, T) permits the Chinese word in position j of a length-T sentence to map to any position i of a length-V English sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. Note there are no explicit linguistic grammars in the IBM channel model. Useful methods do exist for incorporating constraints fed in from other preprocessing modules, and some of these modules do employ linguistic grammars. For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995). If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by a preprocessing stage, a modified version of the A*based algorithm can follow the brackets to guide the search heuristically. This strategy appears to produces moderate improvements in search speed and slightly better translations. Such linguistic-preprocessing techniques could 'Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &quot;Model 2&quot;; search costs for the more complex models are correspondingly higher. also be used with the new model described below, but the issue is independent of our focus here. In this paper we address the underlying assumptions of core channel model itself which does not directly use linguistic structure. A slightly different model is employed for a word alignment application by Dagan et al. (Dagan, Church, and Gale, 1993). Instead of alignment probabilities, offset probabilities o(k) are employed, where k is essentially the positional distance between the English words aligned to two adjacent Chinese words: wherepr e v is the position of the immediately preceding Chinese word and N is a constant that normalizes for average sentence lengths in different languages. The motivation is that words that are close to each other in the Chinese sentence should tend to be close in the English sentence as well. The size of the parameter set is greatly reduced from the x x (TI x I VI parameters of the alignment probabilities, down to a small set of jkl parameters. However, the search space remains the same. The A*-style stack-decoding approach is in some ways a carryover from the speech recognition architectures that inspired the channel translation model. It has proven highly effective for speech recognition in both accuracy and speed, where the search space contains no order variation since the acoustic and text streams can be assumed to be linearly aligned. But in contrast, for translation models the stack search alone does not adequately compensate for the combinatorially more complex space that results from permitting arbitrary order variations. Indeed, the stack-decoding approach remains impractically slow for translation, and has not achieved the same kind of speed as for speech recognition. The model we describe in this paper, like Dagan et al. 's model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus generates two strings, one on the Chinese stream and one on the English stream. Thus, the tree: An additional mechanism accommodates a conservative degree of word-order variation between the two languages. With each production of the grammar is associated either a straight orientation or an inverted orientation, respectively denoted as follows: In the case of a production with straight orientation, the right-hand-side symbols are visited leftto-right for both the Chinese and English streams. But for a production with inverted orientation, the 2 Readers of the papers cited above should note that we have switched the roles of English and Chinese here, which helps simplify the presentation of the new translation algorithm. right-hand-side symbols are visited left-to-right for Chinese and right-to-left for English. Thus, the tree: In the special case of BTGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a syntactic frame can be accommodated.3 Syntactic frames generally contain four or fewer subconstituents. Figure 2 shows that for the case of four subconstituents, BTGs permit 22 out of the 24 possible alignments. The only prohibited arrangements are &quot;inside-out&quot; transformations (Wu, 1995b), which we have been unable to find any examples of in our corpus. Moreover, extremely distorted alignments can be handled by BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than ITG is used since, as we discussed earlier, pure channel translation models operate without explicit grammars, providing no constituent categories around which a more sophisticated ITG could be structured. But the structural constraints of the BTG can improve search efficiency, even without differentiated constituent categories. Just as in the baseline system, we rely on the language and translation models to take up the slack in place of an explicit grammar. In this approach, an 0(T7) algorithm similar to the one described later can be constructed to replace A* search. However we do not feel it is worth preserving offset (or alignment or distortion) parameters simply for the sake of preserving the original translation channel model. These parameterizations were only intended to crudely model word-order variation. Instead, the BTG itself can be used directly to probabilistically rank alternative alignments, as described next. The second possibility is to use a stochastic bracketing transduction grammar (SBTG) in the channel model, replacing the translation model altogether. In a SBTG, a probability is associated with each production. Thus for the normal-form BTG, we have: The translation lexicon is encoded in productions of the third kind. The latter two kinds of productions allow words of either Chinese or English to go unmatched. The SBTG assigns a probability Pr(c, e, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(e) and integrating out Pr(q) to give Pr(cle) in Equation (2). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation To complete the picture we add a bigram model get—let = g(e.i 1e3_ Pr(e). 1) for the English language model Offset, alignment, or distortion parameters are entirely eliminated. A large part of the implicit function of such parameters—to prevent alignments where too many frame arguments become separated—is rendered unnecessary by the BTG's structural constraints, which prohibit many such configurations altogether. Another part of the parameters' purpose is subsumed by the SBTG's probabilities all and a0, which can be set to prefer straight or inverted orientation depending on the language pair. As in the original models, the language model heavily influences the remaining ordering decisions. Matters are complicated by the presence of the bigram model in the objective function (which wordalignment models, as opposed to translation models, do not need to deal with). As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967). But unlike the word-alignment model, to accommodate the bigram model we introduce indexes in the recurrence not only on subtrees over the source Chinese string, but also on the delimiting words of the target English substrings. Another feature of the algorithm is that segmentation of the Chinese input sentence is performed in parallel with the translation search. Conventional architectures for Chinese NLP generally attempt to identify word boundaries as a preprocessing stage.5 Whenever the segmentation preprocessor prematurely commits to an inappropriate segmentation, difficulties are created for later stages. This problem is particularly acute for translation, since the decision as to whether to regard a sequence as a single unit depends on whether its components can be translated compositionally. This in turn often depends on what the target language is. In other words, the Chinese cannot be appropriately segmented except with respect to the target language of translation—a task-driven definition of correct segmentation. The algorithm is given below. A few remarks about the notation used: cs,.t denotes the subsequence of Chinese tokens cs+1, cs+2, , ct. We use E(s..t) to denote the set of English words that are translations the Chinese word created by taking all tokens in c, t together. E(s, t) denotes the set of English words that are translations of any of the Chinese words anywhere within e, ..t. Note also that we assume the explicit sentence-start and sentenceend tokens co = <s> and cT+1 = <Is>, which makes the algorithm description more parsimonious. Finally, the argmax operator is generalized to vector notation to accomodate multiple indices. = 0 if etyz > Olslyz and 4yz > 451)tyz where of the parse tree to qo =_- (-1,T— 1, <s>, </ s>). The remaining descendants in the optimal parse tree are then given recursively for any q = (s,t,y, z) by: a probabilistic optimization problem. But perhaps most importantly, our goal is to constrain as tightly as possible the space of possible transduction relationships between two languages with fixed wordorder, making no other language-specific assumptions; we are thus driven to seek a kind of languageuniversal property. In contrast, the ID/LP work was directed at parsing a single language with free word-order. As a consequence, it would be necessary to enumerate a specific set of linear-precedence (LP) relations for the language, and moreover the immediate-dominance (ID) productions would typically be more complex than binary-branching. This significantly increases time complexity, compared to our BTG model. Although it is not mentioned in their paper, the time complexity for ID/LP parsing rises exponentially with the length of production right-hand-sides, due to the number of permutations. ITGs avoid this with their restriction to inversions, rather than permutations, and BTGs further minimize the grammar size. We have also confirmed empirically that our models would not be feasible under general permutations. Assume the number of translations per word is bounded by some constant. Then the maximum size of E(s,t) is proportional to t — s. The asymptotic time complexity for the translation algorithm is thus bounded by 0(T7). Note that in practice, actual performance is improved by the sparseness of the translation matrix. An interesting connection has been suggested to direct parsing for ID/LP grammars (Shieber, 1984), in which word-order variations would be accommodated by the parser, and related ideas for generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not significantly compromised by substituting our efficient optimization algorithm. It is not our purpose here to argue that accuracy can be increased with our model. No morphological processing has been used to correct the output, and until now we have only been testing with a bigram model trained on extremely limited samples. A coarse evaluation of (Xiang gang de an ding fan rong shi WO men sheng hu6 fang shi de zhi Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. gu'o, wa xian zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code. We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance.
An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an unsupervised stochastic model – the only resource we use is a morphological analyzer – which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language. We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with affix morphology. Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text, according to the word context. In this work, we investigate morphological disambiguation in Modern Hebrew. We explore unsupervised learning method, which is more challenging than the supervised case. The main motivation for this approach is that despite the development of annotated corpora in Hebrew', there is still not enough data available for supervised training. The other reason, is that unsupervised methods can handle the dynamic nature of Modern Hebrew, as it evolves over time. In the case of English, because morphology is simpler, morphological disambiguation is generally covered under the task of part-of-speech tagging. The main morphological variations are embedded in the tag name (for example, Ns and Np for noun singular or plural). The tagging accuracy of supervised stochastic taggers is around 96%97% (Manning and Schutze, 1999, 10.6.1). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised word-based HMM, trained on a corpus of 42,186 sentences (about 1M words), over a tag set of 159 different tags. Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With good initial conditions, such as good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27% and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2000 sentences of the training corpus are manually tagged. Modern Hebrew is characterized by rich morphology, with a high level of ambiguity. On average, in our corpus, the number of possible analyses per word reached 2.4 (in contrast to 1.4 for English). In Hebrew, several morphemes combine into a single word in both agglutinative and fusional ways. This results in a potentially high number of tags for each word. In contrast to English tag sets whose sizes range from 48 to 195, the number of tags for Hebrew, based on all combinations of the morphological attributes (part-of-speech, gender, number, person, tense, status, and the affixes' properties2), can grow theoretically to about 300,000 tags. In practice, we found only 1,934 tags in a corpus of news stories we gathered, which contains about 6M words. The large size of such a tag set (about 10 times larger than the most comprehensive English tag set) is problematic in term of data sparseness. Each morphological combination appears rarely, and more samples are required in order to learn the probabilistic model. In this paper, we hypothesize that the large set of morphological features of Hebrew words, should be modeled by a compact morpheme model, based on the segmented words (into prefix, baseform, and suffix). Our main result is that best performance is obtained when learning segmentation and morpheme tagging in one step, which is made possible by an appropriate text representation. Several works have dealt with Hebrew tagging in the past decade. In Hebrew, morphological analysis requires complex processing according to the rules of Hebrew word formation. The task of a morphological analyzer is to produce all possible analyses for a given word. Recent analyzers provide good performance and documentation of this process (Yona and Wintner, 2005; Segal, 2000). Morphological analyzers rely on a dictionary, and their performance is, therefore, impacted by the occurrence of unknown words. The task of a morphological disambiguation system is to pick the most likely analysis produced by an analyzer in the context of a full sentence. Levinger et al. (1995) developed a context-free method in order to acquire the morpho-lexical probabilities, from an untagged corpus. Their method handles the data sparseness problem by using a set of similar words for each word, built according to a set of rules. The rules produce variations of the morphological properties of the word analyses. Their tests indicate an accuracy of about 88% for context-free analysis selection based on the approximated analysis distribution. In tests we reproduced on a larger data set (30K tagged words), the accuracy is only 78.2%. In order to improve the results, the authors recommend merging their method together with other morphological disambiguation methods – which is the approach we pursue in this work. Levinger's morphological disambiguation system (Levinger, 1992) combines the above approximated probabilities with an expert system, based on a manual set of 16 syntactic constraints . In the first phase, the expert system is applied, dis24–86). ambiguating 35% of the ambiguous words with an accuracy of 99.6%. In order to increase the applicability of the disambiguation, approximated probabilities are used for words that were not disambiguated in the first stage. Finally, the expert system is used again over the new probabilities that were set in the previous stage. Levinger reports an accuracy of about 94% for disambiguation of 85% of the words in the text (overall 80% disambiguation). The system was also applied to prune out the least likely analyses in a corpus but without, necessarily, selecting a single analysis for each word. For this task, an accuracy of 94% was reported while reducing 92% of the ambiguous analyses. Carmel and Maarek (1999) use the fact that on average 45% of the Hebrew words are unambiguous, to rank analyses, based on the number of disambiguated occurrences in the text, normalized by the total number of occurrences for each word. Their application – indexing for an information retrieval system – does not require all of the morphological attributes but only the lemma and the PoS of each word. As a result, for this case, 75% of the words remain with one analysis with 95% accuracy, 20% with two analyses and 5% with three analyses. Segal (2000) built a transformation-based tagger in the spirit of Brill (1995). In the first phase, the analyses of each word are ranked according to the frequencies of the possible lemmas and tags in a training corpus of about 5,000 words. Selection of the highest ranked analysis for each word gives an accuracy of 83% of the test text – which consists of about 1,000 words. In the second stage, a transformation learning algorithm is applied (in contrast to Brill, the observed transformations are not applied, but used for re-estimation of the word couples probabilities). After this stage, the accuracy is about 93%. The last stage uses a bottomup parser over a hand-crafted grammar with 150 rules, in order to select the analysis which causes the parsing to be more accurate. Segal reports an accuracy of 95%. Testing his system over a larger test corpus, gives poorer results: Lembersky (2001) reports an accuracy of about 85%. Bar-Haim et al. (2005) developed a word segmenter and PoS tagger for Hebrew. In their architecture, words are first segmented into morphemes, and then, as a second stage, these morphemes are tagged with PoS. The method proceeds in two sequential steps: segmentation into morphemes, then tagging over morphemes. The segmentation is based on an HMM and trained over a set of 30K annotated words. The segmentation step reaches an accuracy of 96.74%. PoS tagging, based on unsupervised estimation which combines a small annotated corpus with an untagged corpus of 340K words by using smoothing technique, gives an accuracy of 90.51%. As noted earlier, there is as yet no large scale Hebrew annotated corpus. We are in the process of developing such a corpus, and we have developed tagging guidelines (Elhadad et al., 2005) to define a comprehensive tag set, and assist human taggers achieve high agreement. The results discussed above should be taken as rough approximations of the real performance of the systems, until they can be re-evaluated on such a large scale corpus with a standard tag set. Arabic is a language with morphology quite similar to Hebrew. Theoretically, there might be 330,000 possible morphological tags, but in practice, Habash and Rambow (2005) extracted 2,200 different tags from their corpus, with an average number of 2 possible tags per word. As reported by Habash and Rambow, the first work on Arabic tagging which used a corpus for training and evaluation was the work of Diab et al. (2004). Habash and Rambow were the first to use a morphological analyzer as part of their tagger. They developed a supervised morphological disambiguator, based on training corpora of two sets of 120K words, which combines several classifiers of individual morphological features. The accuracy of their analyzer is 94.8% – 96.2% (depending on the test corpus). An unsupervised HMM model for dialectal Arabic (which is harder to be tagged than written Arabic), with accurracy of 69.83%, was presented by Duh and Kirchhoff (2005). Their supervised model, trained on a manually annotated corpus, reached an accuracy of 92.53%. Arabic morphology seems to be similar to Hebrew morphology, in term of complexity and data sparseness, but comparison of the performances of the baseline tagger used by Habash and Rambow – which selects the most frequent tag for a given word in the training corpus – for Hebrew and Arabic, shows some intriguing differences: 92.53% for Arabic and 71.85% for Hebrew. Furthermore, as mentioned above, even the use of a sophisticated context-free tagger, based on (Levinger et al., 1995), gives low accuracy of 78.2%. This might imply that, despite the similarities, morphological disambiguation in Hebrew might be harder than in Arabic. It could also mean that the tag set used for the Arabic corpora has not been adapted to the specific nature of Arabic morphology (a comment also made in (Habash and Rambow, 2005)). We propose an unsupervised morpheme-based HMM to address the data sparseness problem. In contrast to Bar-Haim et al., our model combines segmentation and morphological disambiguation, in parallel. The only resource we use in this work is a morphological analyzer. The analyzer itself can be generated from a word list and a morphological generation module, such as the HSpell wordlist (Har'el and Kenigsberg, 2004). The lexical items of word-based models are the words of the language. The implication of this decision is that both lexical and syntagmatic relations of the model, are based on a word-oriented tagset. With such a tagset, it must be possible to tag any word of the language with at least one tag. Let us consider, for instance, the Hebrew phrase bclm hn`im3, which contains two words. The word bclm has several possible morpheme segmentations and analyses as described in Table 1. In wordbased HMM, we consider such a phrase to be generated by a Markov process, based on the wordoriented tagset of N = 1934 tags/states and about M = 175K word types. Line W of Table 2 describes the size of a first-order word-based HMM, built over our corpus. In this model, we found 834 entries for the II vector (which models the distribution of tags in first position in sentences) out of possibly N = 1934, about 250K entries for the A matrix (which models the transition probabilities from tag to tag) out of possibly N2 ;::Li 3.7M, and about 300K entries for the B matrix (which models the emission probabilities from tag to word) out of possibly M · N ;::Li 350M. For the case of a secondorder HMM, the size of the A2 matrix (which models the transition probabilities from two tags to the third one), grows to about 7M entries, where the size of the B2 matrix (which models the emission probabilities from two tags to a word) is about 5M. Despite the sparseness of these matrices, the number of their entries is still high, since we model the whole set of features of the complex word forms. Let us assume, that the right segmentation for the sentence is provided to us – for example: b clm hn`im – as is the case for English text. In such a way, the observation is composed of morphemes, generated by a Markov process, based on a morpheme-based tagset. The size of such a tagset for Hebrew is about 200, where the size of the II,A,B,A2 and B2 matrices is reduced to 145, 16K, 140K, 700K, and 1.7M correspondingly, as described in line M of Table 2 – a reduction of 90% when compared with the size of a word-based model. The problem in this approach, is that ”someone” along the way, agglutinates the morphemes of each word leaving the observed morphemes uncertain. For example, the word bclm can be segmented in four different ways in Table 1, as indicated by the placement of the '-' in the Segmentation column, while the word hn`im can be segmented in two different ways. In the next section, we adapt the parameter estimation and the searching algorithms for such uncertain output observation. In contrast to standard HMM, the output observations of the above morpheme-based HMM are ambiguous. We adapted Baum-Welch (Baum, 1972) and Viterbi (Manning and Schutze, 1999, 9.3.2) algorithms for such uncertain observation. We first formalize the output representation and then describe the algorithms. Output Representation The learning and searching algorithms of HMM are based on the output sequence of the underlying Markov process. For the case of a morpheme-based model, the output sequence is uncertain – we don’t see the emitted morphemes but the words they form. If, for instance, the Markov process emitted the morphemes b clm h nim, we would see two words (bclm hn`im) instead. In order to handle the output ambiguity, we use static knowledge of how morphemes are combined into a word, such as the four known combinations of the word bclm, the two possible combinations of the word hn`im, and their possible tags within the original words. Based on this information, we encode the sentence into a structure that represents all the possible “readings” of the sentence, according to the possible morpheme combinations of the words, and their possible tags. The representation consists of a set of vectors, each vector containing the possible morphemes and their tags for each specific “time” (sequential position within the morpheme expansion of the words of the sentence). A morpheme is represented by a tuple (symbol, state, prev, next), where symbol denotes a morpheme, state is one possible tag for this morpheme, prev and next are sets of indexes, denoting the indexes of the morphemes (of the previous and the next vectors) that precede and follow the current morpheme in the overall lattice, representing the sentence. Fig. 2 describes the representation of the sentence bclm hn`im. An emission is denoted in this figure by its symbol, its state index, directed edges from its previous emissions, and directed edges to its next emissions. In order to meet the condition of Baum-Eagon inequality (Baum, 1972) that the polynomial P(O|µ) – which represents the probability of an observed sequence O given a model µ – be homogeneous, we must add a sequence of special EOS (end of sentence) symbols at the end of each path up to the last vector, so that all the paths reach the same length. The above text representation can be used to model multi-word expressions (MWEs). Consider the Hebrew sentence: hw' `wrk dyn gdwl, which can be interpreted as composed of 3 units (he lawyer great / he is a great lawyer) or as 4 units (he edits law big / he is editing an important legal decision). In order to select the correct interpretation, we must determine whether `wrk dyn is an MWE. This is another case of uncertain output observation, which can be represented by our text encoding, as done in Fig. 1. This representation seems to be expensive in term of the number of emissions per sentence. However, we observe in our data that most of the words have only one or two possible segmentations, and most of the segmentations consist of at most one affix. In practice, we found the average number of emissions per sentence in our corpus (where each symbol is counted as the number of its predecessor emissions) to be 455, where the average number of words per sentence is about 18. That is, the cost of operating over an ambiguous sentence representation increases the size of the sentence (from 18 to 455), but on the other hand, it reduces the probabilistic model by a factor of 10 (as discussed above). Morphological disambiguation over such a sequence of vectors of uncertain morphemes is similar to words extraction in automatic speech recognition (ASR)(Jurafsky and Martin, 2000, chp. 5,7). The states of the ASR model are phones, where each observation is a vector of spectral features. Given a sequence of observations for a sentence, the encoding – based on the lattice formed by the phones distribution of the observations, and the language model – searches for the set of words, made of phones, which maximizes the acoustic likelihood and the language model probabilities. In a similar manner, the supervised training of a speech recognizer combines a training corpus of speech wave files, together with word-transcription, and language model probabilities, in order to learn the phones model. There are two main differences between the typical ASR model and ours: (1) an ASR decoder deals with one aspect - segmentation of the observations into a set of words, where this segmentation can be modeled at several levels: subphones, phones and words. These levels can be trained individually (such as training a language model from a written corpus, and training the phones model for each word type, given transcripted wave file), and then combined together (in a hierarchical model). Morphological disambiguation over uncertain morphemes, on the other hand, deals with both morpheme segmentation and the tagging of each morpheme with its morphological features. Modeling morpheme segmentation, within a given word, without its morphology features would be insufficient. (2) The supervised resources of ASR are not available for morphological disambiguation: we don’t have a model of morphological features sequences (equivalent to the language model of ASR) nor a tagged corpus (equivalent to the transcripted wave files of ASR). These two differences require a design which combines the two dimensions of the problem, in order to support unsupervised learning (and searching) of morpheme sequences and their morphological features, simultaneously. Parameter Estimation We present a variation of the Baum-Welch algorithm (Baum, 1972) which operates over the lattice representation we have defined above. The algorithm starts with a probabilistic model µ (which can be chosen randomly or obtained from good initial conditions), and at each iteration, a new model µ� is derived in order to better explain the given output observations. For a given sentence, we define T as the number of words in the sentence, and T as the number of vectors of the output representation O = {ot},1 < t < T, where each item in the output is denoted by olt = (sym, state, prev, next),1 < t < T,1 < l < |ot|. We define a(t, l) as the probability to reach olt at time t, and 0(t, l) as the probability to end the sequence from olt. Fig. 3 describes the expectation and the maximization steps of the learning algorithm for a first-order HMM. The algorithm works in O(T) time complexity, where T is the total number of symbols in the output sequence encoding, where each symbol is counted as the size of its prev set. Searching for best state sequence The searching algorithm gets an observation sequence O and a probabilistic model µ, and looks for the best state sequence that generates the observation. We define S(t, l) as the probability of the best state sequence that leads to emission olt, and�(t, l) as the index of the emission at time t−1 that precedes olt in the best state sequence that leads to it. Fig. 4 describes the adaptation of the Viterbi (Manning and Schutze, 1999, 9.3.2) algorithm to our text representation for first-order HMM, which works in O(T) time. We ran a series of experiments on a Hebrew corpus to compare various approaches to the full morphological disambiguation and PoS tagging tasks. The training corpus is obtained from various newspaper sources and is characterized by the following statistics: 6M word occurrences, 178,580 distinct words, 64,541 distinct lemmas. Overall, the ambiguity level is 2.4 (average number of analyses per word). We tested the results on a test corpus, manually annotated by 2 taggers according to the guidelines we published and checked for agreement. The test corpus contains about 30K words. We compared two unsupervised models over this data set: Word model [W], and Morpheme model [M]. We also tested two different sets of initial conditions. Uniform distribution [Uniform]: For each word, each analysis provided by the analyzer is estimated with an equal likelihood. Context Free approximation [CF]: We applied the CF algorithm of Levinger et al. (1995) to estimate the likelihood of each analysis. Table 3 reports the results of full morphological disambiguation. For each morpheme and word models, three types of models were tested: [1] First-order HMM, [2-] Partial second-order HMM only state transitions were modeled (excluding B2 matrix), [2] Second-order HMM (including the B2 matrix). Analysis If we consider the tagger which selects the most probable morphological analysis for each word in the text, according to Levinger et al. (1995) approximations, with accuracy of 78.2%, as the baseline tagger, four steps of error reduction can be identified. (1) Contextual information: The simplest first-order word-based HMM with uniform initial conditions, achieves error reduction of 17.5% (78.2 – 82.01). (2) Initial conditions: Error reductions in the range: 11.5% – 37.8% (82.01 – 84.08 for word model 1, and 81.53 – 88.5 for morhpeme model 2-) were achieved by initializing the various models with context-free approximations. While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English - about 70% (79 – 94). The key difference (beside the unclear characteristic of Elworthy initial condition - since he made use of an annotated corpus) is the much higher quality of the uniform distribution for Hebrew. (3) Model order: The partial second-order HMM [2-] produced the best results for both word (85.75%) and morpheme (88.5%) models over the initial condition. The full second-order HMM [2] didn’t upgrade the accuracy of the partial second-order, but achieved the best results for the uniform distribution morpheme model. This is because the context-free approximation does not take into account the tag of the previous word, which is part of model 2. We believe that initializing the morpheme model over a small set of annotated corpus will set much stronger initial condition for this model. (4) Model type: The main result of this paper is the error reduction of the morpheme model with respect to the word model: about 19.3% (85.75 – 88.5). In addition, we apply the above models for the simpler task of segmentation and PoS tagging, as reported in Table 4. The task requires picking the correct morphemes of each word with their correct PoS (excluding all other morphological features). The best result for this task is obtained with the morpheme model 2: 92.32%. For this simpler task, the improvement brought by the morpheme model over the word model is less significant, but still consists of a 5% error reduction. Unknown words account for a significant chunk of the errors. Table 5 shows the distribution of errors contributed by unknown words (words that cannot be analyzed by the morphological analyzer). 7.5% of the words in the test corpus are unknown: 4% are not recognized at all by the morphological analyzer (marked as [None] in the table), and for 3.5%, the set of analyses proposed by the analyzer does not contain the correct analysis [Missing]. We extended the lexicon to include missing and none lexemes of the closed sets. In addition, we modified the analyzer to extract all possible segmentations of unknown words, with all the possible tags for the segmented affixes, where the remaining unknown baseforms are tagged as UK. The model was trained over this set. In the next phase, the corpus was automatically tagged, according to the trained model, in order to form a tag distribution for each unknown word, according to its context and its form. Finally, the tag for each unknown word were selected according to its tag distribution. This strategy accounts for about half of the 7.5% unknown words. Table 6 shows the confusion matrix for known words (5% and up). The key confusions can be attributed to linguistic properties of Modern Hebrew: most Hebrew proper names are also nouns (and they are not marked by capitalization) – which explains the PN/N confusion. The verb/noun and verb/adjective confusions are explained by the nature of the participle form in Hebrew (beinoni) – participles behave syntactically almost in an identical manner as nouns. In this work, we have introduced a new text encoding method that captures rules of word formation in a language with affixational morphology such as Hebrew. This text encoding method allows us to learn in parallel segmentation and tagging rules in an unsupervised manner, despite the high ambiguity level of the morphological data (average number of 2.4 analyses per word). Reported results on a large scale corpus (6M words) with fully unsupervised learning are 92.32% for PoS tagging and 88.5% for full morphological disambiguation. In this work, we used the backoff smoothing method, suggested by Thede and Harper (1999), with an extension of additive smoothing (Chen, 1996, 2.2.1) for the lexical probabilities (B and B2 matrices). To complete this study, we are currently investigating several smoothing techniques (Chen, 1996), in order to check whether the morpheme model is critical for the data sparseness problem, or whether it can be handled with smoothing over a word model. We are currently investigating two major methods to improve our results: first, we have started gathering a larger corpus of manually tagged text and plan to perform semi-supervised learning on a corpus of 100K manually tagged words. Second, we plan to improve the unknown word model, such as integrating it with named entity recognition system (Ben-Mordechai, 2005).
The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination. The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms. Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Com binatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentencewhich are then manipulated by the parser. Supertag ging was introduced for LTAG as a way of increasingparsing efficiency by reducing the number of struc tures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003).Supertagging accuracy is relatively high for man ually constructed LTAGs (Bangalore and Joshi,1999). However, for LTAGs extracted automati cally from the Penn Treebank, performance is much lower (Chen et al, 1999; Chen et al, 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al, 2000). In this paper we demonstratethat CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an auto matically extracted grammar, but also offers several practical advantages. Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse. Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number. The supertagger is also crucial for increasing thespeed of the parser. We show that spectacular in creases in speed can be obtained, without affectingaccuracy or coverage, by tightly integrating the su pertagger with the CCG grammar and parser. To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis. We also demonstrate how extra constraints on the category combinations, and the application of beam search using the parsing model, can further increase parsing speed.This is the first work we are aware of to succes fully integrate a supertagger with a full parser which uses a lexicalised grammar automatically extractedfrom the Penn Treebank. We also report signifi cantly higher parsing speeds on newspaper text than any previously reported for a full wide-coverage parser. Our results confirm that wide-coverage CCG parsing is feasible for many large-scale NLP tasks. Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories to gether using CCG?s combinatory rules.1 The first stage can be accomplished by simply assigning to each word all categories from the word?s entry in the lexicon (Hockenmaier, 2003). 1See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. The WSJ is a publication that I enjoy reading NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP Figure 1: Example sentence with CCG lexical categories frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00 cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat 1 1 225 0 0 12 (0.03%) 12 (0.6%) 10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%) Table 1: Statistics for the lexical category setAn alternative is to use a statistical tagging approach to assign one or more categories. A statisti cal model can be used to determine the most likelycategories given the word?s context. The advantage of this supertagging approach is that the number of categories assigned to each word can be re duced, with a correspondingly massive reduction in the number of derivations. Bangalore and Joshi (1999) use a standard Markov model tagger to assign LTAG elementarytrees to words. Here we use the Maximum En tropy models described in Curran and Clark (2003). An advantage of the Maximum Entropy approachis that it is easy to encode a wide range of poten tially useful information as features; for example,Clark (2002) has shown that POS tags provide use ful information for supertagging. The next section describes the set of lexical categories used by our supertagger and parser. 2.1 The Lexical Category Set. The set of lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of CCG normal-form deriva tions derived semi-automatically from the PennTreebank. Following Clark (2002), we apply a fre quency cutoff to the training set, only using thosecategories which appear at least 10 times in sections 2-21. Figure 1 gives an example sentence su pertagged with the correct CCG lexical categories. Table 1 gives the number of different category types and shows the coverage on training (seen) anddevelopment (unseen) data (section 00 from CCGbank). The table also gives statistics for the com plete set containing every lexical category type inCCGbank.2 These figures show that using a fre quency cutoff can significantly reduce the size of the category set with only a small loss in coverage. 2The numbers differ slightly from those reported in Clark (2002) since a newer version of CCGbank is being used here. Clark (2002) compares the size of grammarsextracted from CCGbank with automatically extracted LTAGs. The grammars of Chen and Vijay Shanker (2000) contain between 2,000 and 9,000 tree frames, depending on the parameters used inthe extraction process, significantly more elemen tary structures than the number of lexical categories derived from CCGbank. We hypothesise this is a key factor in the higher accuracy for supertaggingusing a CCG grammar compared with an automati cally extracted LTAG. 2.2 The Tagging Model. The supertagger uses probabilities p(y|x) where y is a lexical category and x is a context. The conditional probabilities have the following log-linear form: p(y|x) = 1 Z(x)e ? i ?i fi(y,x) (1) where fi is a feature, ?i is the corresponding weight, and Z(x) is a normalisation constant. The context is a 5-word window surrounding the target word. Features are defined for each word in the window and for the POS tag of each word. Curran and Clark(2003) describes the model and explains how Gen eralised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. Theper-word accuracy is between 91 and 92% on un seen data in CCGbank; however, Clark (2002) shows this is not high enough for integration into a parser since the large number of incorrect categories results in a significant loss in coverage. Clark (2002) shows how the models in (1) can be used to define a multi-tagger which can assign more than one category to a word. For each word inthe sentence, the multi-tagger assigns all those cat ? CATS/ ACC SENT ACC SENT WORD ACC (POS) ACC 0.1 1.4 97.0 62.6 96.4 57.4 0.075 1.5 97.4 65.9 96.8 60.6 0.05 1.7 97.8 70.2 97.3 64.4 0.01 2.9 98.5 78.4 98.2 74.2 0.01k=100 3.5 98.9 83.6 98.6 78.9 0 21.9 99.1 84.8 99.0 83.0 Table 2: Supertagger accuracy on section 00 egories whose probability according to (1) is within some factor, ?, of the highest probability category for the word. We follow Clark (2002) in ignoring the featuresbased on the previously assigned categories; there fore every tagging decision is local and the Viterbi algorithm is not required. This simple approach has the advantage of being very efficient, and we findthat it is accurate enough to enable highly accu rate parsing. However, a method which used theforward-backward algorithm to sum over all possi ble sequences, or some other method which took into account category sequence information, may well improve the results. For words seen at least k times in the trainingdata, the tagger can only assign categories appear ing in the word?s entry in the tag dictionary. Eachentry in the tag dictionary is a list of all the cate gories seen with that word in the training data. For words seen less than k times, we use an alternative dictionary based on the word?s POS tag: the tagger can only assign categories that have been seen with the POS tag in the training data. A value of k = 20was used in this work, and sections 2-21 of CCG bank were used as training data.Table 2 gives the per-word accuracy (acc) on sec tion 00 for various values of ?, together with the average number of categories per word. The sent acc column gives the precentage of sentences whose words are all supertagged correctly. The figures for ? = 0.01k=100 correspond to a value of 100 for thetag dictionary parameter k. The set of categories as signed to a word is considered correct if it contains the correct category. The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran andClark (2003) tagger. The drop in accuracy is ex pected given the importance of POS tags as features. The figures for ? = 0 are obtained by assigning all categories to a word from the word?s entry in the tag dictionary. For words which appear less than 20 times in the training data, the dictionary based on the word?s POS tag is used. The table demonstrates the significant reduction in the average number of categories that can be achieved through the use of a supertagger. To give one example, the number of categories in the tag dictionary?s entry for the wordis is 45 (only considering categories which have appeared at least 10 times in the training data). However, in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., the supertag ger correctly assigns 1 category to is for ? = 0.1, and 3 categories for ? = 0.01. The parser is described in detail in Clark and Curran (2004). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG. In this paper weuse the normal-form model, which defines proba bilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence. Featuresare defined in terms of the local trees in the derivation, including lexical head information and word word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. The feature set we use is from the best performing normal-form model in Clark and Curran (2004). For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm. The dependency relations are de fined in terms of the argument slots of CCG lexical categories. Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. 3.1 Model Estimation. In Clark and Curran (2004) we describe a discrim inative method for estimating the parameters of a log-linear parsing model. The estimation method maximises the following objective function: L?(?) = L(?) ?G(?) (2) = log m ? j=1 P?(d j|S j) ? n ? i=1 ?2i 2?2The data consists of sentences S 1, . . . , S m, to gether with gold standard normal-form derivations, d1, . . . , dm. L(?) is the log-likelihood of model ?, and G(?) is a Gaussian prior term used to avoid overfitting (n is the number of features; ?i is the weight for feature fi; and ? is a parameter of theGaussian). The objective function is optimised using L-BFGS (Nocedal and Wright, 1999), an iterative algorithm from the numerical optimisation lit erature.The algorithm requires the gradient of the objective function, and the value of the objective function, at each iteration. Calculation of these val ues requires all derivations for each sentence in the training data. In Clark and Curran (2004) wedescribe efficient methods for performing the cal culations using packed charts. However, a very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in Clark and Curran (2003) we report a memory usage of 30 GB. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster. The need for large high-performance computing resources is a disadvantage of our earlier approach.In the next section we show how use of the supertag ger, combined with normal-form constraints on thederivations, can significantly reduce the memory re quirements for the model estimation. Since the training data contains the correct lexicalcategories, we ensure the correct category is as signed to each word when generating the packed charts for model estimation. Whilst training theparser, the supertagger can be thought of as supply ing a number of plausible but incorrect categoriesfor each word; these, together with the correct cat egories, determine the parts of the parse space that are used in the estimation process. We would like to keep the packed charts as small as possible, but not lose accuracy in the resulting parser. Section 4.2discusses the use of various settings on the supertag ger. The next section describes how normal-form constraints can further reduce the derivation space. 4.1 Normal-Form Constraints. As well as the supertagger, we use two additional strategies for reducing the derivation space. Thefirst, following Hockenmaier (2003), is to only al low categories to combine if the combination hasbeen seen in sections 2-21 of CCGbank. For exam ple, NP/NP could combine with NP/NP accordingto CCG?s combinatory rules (by forward composi tion), but since this particular combination does not appear in CCGbank the parser does not allow it.The second strategy is to use Eisner?s normal form constraints (Eisner, 1996). The constraints SUPERTAGGING/PARSING USAGE CONSTRAINTS DISK MEMORY ? = 0.01 ? 0.05 ? 0.1 17 GB 31 GB CCGbank constraints 13 GB 23 GB Eisner constraints 9 GB 16 GB ? = 0.05 ? 0.1 2 GB 4 GB Table 3: Space requirements for model training dataprevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application. Eis ner only deals with a grammar without type-raising,and so the constraints do not guarantee a normal form parse when using a grammar extracted from CCGbank. However, the constraints are still useful in restricting the derivation space. As far as we are aware, this is the first demonstration of the utility of such constraints for a wide-coverage CCG parser. 4.2 Results (Space Requirements). Table 3 shows the effect of different supertagger set tings, and the normal-form constraints, on the size of the packed charts used for model estimation. The disk usage is the space taken on disk by the charts,and the memory usage is the space taken in memory during the estimation process. The training sen tences are parsed using a number of nodes from a 64-node Beowulf cluster.3 The time taken to parse the training sentences depends on the supertagging and parsing constraints, and the number of nodes used, but is typically around 30 minutes. The first row of the table corresponds to using the least restrictive ? value of 0.01, and reverting to ? = 0.05, and finally ? = 0.1, if the chart size exceeds some threshold. The threshold was set at300,000 nodes in the chart. Packed charts are created for approximately 94% of the sentences in sec tions 2-21 of CCGbank. The coverage is not 100%because, for some sentences, the parser cannot pro vide an analysis, and some charts exceed the node limit even at the ? = 0.1 level. This strategy was used in our earlier work (Clark and Curran, 2003) and, as the table shows, results in very large charts.Note that, even with this relaxed setting on the su pertagger, the number of categories assigned to each word is only around 3 on average. This suggests that it is only through use of the supertagger that we are able to estimate a log-linear parsing model on all of the training data at all, since without it the memory 3The figures in the table are estimates based on a sample of the nodes in the cluster. requirements would be far too great, even for the entire 64-node cluster.4 The second row shows the reduction in size if the parser is only allowed to combine categorieswhich have combined in the training data. This sig nificantly reduces the number of categories created using the composition rules, and also prevents thecreation of unlikely categories using rule combina tions not seen in CCGbank. The results show thatthe memory and disk usage are reduced by approx imately 25% using these constraints. The third row shows a further reduction in size when using the Eisner normal-form constraints. Even with the CCGbank rule constraints, theparser still builds many non-normal-form derivations, since CCGbank does contain cases of compo sition and type-raising. (These are used to analysesome coordination and extraction cases, for example.) The combination of the two types of normal form constraints reduces the memory requirements by 48% over the original approach. In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies. The final row corresponds to a more restrictive setting on the supertagger, in which a value of ? = 0.05 is used initially and ? = 0.1 is used if thenode limit is exceeded. The two types of normal form constraints are also used. In Clark and Curran(2004) we show that using this more restrictive set ting has a small negative impact on the accuracy of the resulting parser (about 0.6 F-score over labelled dependencies). However, the memory requirement for training the model is now only 4 GB, a reduction of 87% compared with the original approach. The previous section showed how to combine the supertagger and parser for the purpose of creating training data, assuming the correct category for each word is known. In this section we describe our approach to tightly integrating the supertagger and parser for parsing unseen data. Our previous approach to parsing unseen data (Clark et al, 2002; Clark and Curran, 2003) wasto use the least restrictive setting of the supertagger which still allows a reasonable compromise be tween speed and accuracy. Our philosophy was to give the parser the greatest possibility of finding thecorrect parse, by giving it as many categories as pos sible, while still retaining reasonable efficiency.4Another possible solution would be to use sampling meth ods, e.g. Osborne (2000). SUPERTAGGING/PARSING TIME SENTS WORDS CONSTRAINTS SEC /SEC /SEC ? = 0.01? 0.1 3 523 0.7 16 CCGbank constraints 1 181 2.0 46 Eisner constraints 995 2.4 55 ? = 0.1? 0.01k=100 608 3.9 90 CCGbank constraints 124 19.4 440 Eisner constraints 100 24.0 546 Parser beam 67 35.8 814 94% coverage 49 49.0 1 114 Parser beam 46 52.2 1 186 Oracle 18 133.4 3 031 Table 4: Parse times for section 23 The problem with this approach is that, for some sentences, the number of categories in the chart still gets extremely large and so parsing is unacceptably slow. Hence we applied a limit to the number of categories in the chart, as in the previous section,and reverted to a more restrictive setting of the su pertagger if the limit was exceeded. We first used a value of ? = 0.01, and then reverted to ? = 0.05, and finally ? = 0.1. In this paper we take the opposite approach: westart with a very restrictive setting of the supertag ger, and only assign more categories if the parser cannot find an analysis spanning the sentence. In this way the parser interacts much more closely with the supertagger. In effect, the parser is using the grammar to decide if the categories provided by thesupertagger are acceptable, and if not the parser re quests more categories. The parser uses the 5 levels given in Table 2, starting with ? = 0.1 and moving through the levels to ? = 0.01k=100 . The advantage of this approach is that parsing speeds are much higher. We also show that our new approach slightly increases parsing accuracy over the previous method. This suggests that, given our current parsing model, it is better to rely largely on the supertagger to provide the correct categoriesrather than use the parsing model to select the cor rect categories from a very large derivation space. 5.1 Results (Parse Times). The results in this section are all using the best per forming normal-form model in Clark and Curran (2004), which corresponds to row 3 in Table 3. All experiments were run on a 2.8 GHZ Intel Xeon P4 with 2 GB RAM. Table 4 gives parse times for the 2,401 sentences in section 23 of CCGbank. The final two columns give the number of sentences, and the number of ? CATS/ 0.1 FIRST 0.01 FIRST WORD PARSES % PARSES % 0.1 1.4 1689 88.4 0 0.0 0.075 1.5 43 2.3 7 0.4 0.05 1.7 51 2.7 39 2.0 0.01 2.9 79 4.1 1816 95.1 0.01k=100 3.5 33 1.7 33 1.7 NO SPAN 15 0.8 15 0.8 Table 5: Supertagger ? levels used on section 00words, parsed per second. For all of the figures re ported on section 23, unless stated otherwise, the parser is able to provide an analysis for 98.5% of the sentences. The parse times and speeds include the failed sentences, but do not include the time takenby the supertagger; however, the supertagger is ex tremely efficient, and takes less than 6 seconds to supertag section 23, most of which consists of load time for the Maximum Entropy model. The first three rows correspond to our strategy ofearlier work by starting with the least restrictive set ting of the supertagger. The first value of ? is 0.01; if the parser cannot find a spanning analysis, this ischanged to ? = 0.01k=100; if the node limit is ex ceeded (for these experiments set at 1,000,000), ? is changed to 0.05. If the node limit is still exceeded, ? is changed to 0.075, and finally 0.1. The second row has the CCGbank rule restriction applied, and the third row the Eisner normal-form restrictions.The next three rows correspond to our new strat egy of starting with the least restrictive setting of thesupertagger (? = 0.1), and moving through the set tings if the parser cannot find a spanning analysis. The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%. The new strategy also has a spectacular impact on the speed compared with the old strategy, reducing the times by 83% without the normal-form constraints and 90% with the constraints. The 94% coverage row corresponds to using only the first two supertagging levels; the parser ignores the sentence if it cannot get an analysis at the ? = 0.05 level. The percentage of sentences without an analysis is now 6%, but the parser is extremely fast,processing almost 50 sentences a second. This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for ex ample, for which large amounts of data are required. The oracle row shows the parser speed when it is provided with only the correct lexical categories.The parser is extremely fast, and in Clark and Cur ran (2004) we show that the F-score for labelled dependencies is almost 98%. This demonstratesthe large amount of information in the lexical categories, and the potential for improving parser ac curacy and efficiency by improving the supertagger. Finally, the first parser beam row corresponds to the parser using a beam search to further reduce thederivation space. The beam search works by prun ing categories from the chart: a category can only be part of a derivation if its beam score is within some factor, ?, of the highest scoring category forthat cell in the chart. Here we simply use the ex ponential of the inside score of a category as the beam score; the inside score for a category c is the sum over all sub-derivations dominated by c of the weights of the features in those sub-derivations (see Clark and Curran (2004).5The value of ? that we use here reduces the accu racy of the parser on section 00 by a small amount (0.3% labelled F-score), but has a significant impacton parser speed, reducing the parse times by a fur ther 33%. The final parser beam row combines thebeam search with the fast, reduced coverage config uration of the parser, producing speeds of over 50 sentences per second. Table 5 gives the percentage of sentences which are parsed at each supertagger level, for both the new and old parsing strategies. The results show that, for the old approach, most of the sentences areparsed using the least restrictive setting of the supertagger (? = 0.01); conversely, for the new ap proach, most of the sentences are parsed using the most restrictive setting (? = 0.1). As well as investigating parser efficiency, we have also evaluated the accuracy of the parser onsection 00 of CCGbank, using both parsing strate gies together with the normal-form constraints. Thenew strategy increases the F-score over labelled de pendencies by approximately 0.5%, leading to the figures reported in Clark and Curran (2004). 5.2 Comparison with Other Work. The only other work we are aware of to investigate the impact of supertagging on parsing efficiency is the work of Sarkar et al (2000) for LTAG. Sarkar etal. did find that LTAG supertagging increased pars ing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse. The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours.5Multiplying by an estimate of the outside score may im prove the efficacy of the beam. Kaplan et al (2004) report high parsing speedsfor a deep parsing system which uses an LFG gram mar: 1.9 sentences per second for 560 sentencesfrom section 23 of the Penn Treebank. They also re port speeds for the publicly available Collins parser (Collins, 1999): 2.8 sentences per second for the same set. The best speeds we have reported for the CCG parser are an order of magnitude faster. This paper has shown that by tightly integrating a supertagger with a CCG parser, very fast parse times can be achieved for Penn Treebank WSJ text. As far as we are aware, the times reported here are an orderof magnitude faster than any reported for compara ble systems using linguistically motivated grammar formalisms. The techniques we have presented inthis paper increase the speed of the parser by a factor of 77. This makes this parser suitable for large scale NLP tasks.The results also suggest that further improvements can be obtained by improving the supertagger, which should be possible given the simple tag ging approach currently being used.The novel parsing strategy of allowing the grammar to decide if the supertagging is likely to be cor rect suggests a number of interesting possibilities.In particular, we would like to investigate only re pairing those areas of the chart that are most likely to contain errors, rather than parsing the sentence from scratch using a new set of lexical categories. This could further increase parsing effficiency. Acknowledgements We would like to thank Julia Hockenmaier, whosework creating the CCGbank made this research possible, and Mark Steedman for his advice and guid ance. This research was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move toward fine-grained information extraction (IE), in particular biomolecular event extraction (Kim et al., 2009; Ananiadou et al., 2010). The series is complementary to BioCreative (Hirschman et al., 2007); while BioCreative emphasizes the short-term applicability of introduced IE methods for tasks such as database curation, BioNLP-ST places more emphasis on the measurability of the state-of-the-art and traceability of challenges in extraction through an approach more closely tied to text. These goals were pursued in the first event, BioNLP-ST 2009 (Kim et al., 2009), through high quality benchmark data provided for system development and detailed evaluation performed to identify remaining problems hindering extraction performance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). As the second event of the series, BioNLP-ST 2011 preserves the general design and goals of the previous event, but adds a new focus on variability to address a limitation of BioNLP-ST 2009: the benchmark data sets were based on the Genia corpus (Kim et al., 2008), restricting the community-wide effort to resources developed by a single group for a small subdomain of molecular biology. BioNLPST 2011 is organized as a joint effort of several groups preparing various tasks and resources, in which variability is pursued in three primary directions: text types, event types, and subject domains. Consequently, generalization of fine grained bio-IE in these directions is emphasized as the main theme of the second event. This paper summarizes the entire BioNLP-ST 2011, covering the relationships between tasks and similar broad issues. Each task is presented in detail in separate overview papers and extraction systems in papers by participants. BioNLP-ST 2011 includes four main tracks (with five tasks) representing fine-grained bio-IE. The GE task (Kim et al., 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al., 2008). The data represents a focused domain of molecular biology: transcription factors in human blood cells. The purpose of the GE task is two-fold: to measure the progress of the community since the last event, and to evaluate generalization of the technology to full papers. For the second purpose, the provided data is composed of two collections: the abstract collection, identical to the BioNLP-ST 2009 data, and the new full paper collection. Progress on the task is measured through the unchanged task definition and the abstract collection, while generalization to full papers is measured on the full paper collection. In this way, the GE task is intended to connect the entire event to the previous one. The EPI task (Ohta et al., 2011) focuses on IE for protein and DNA modifications, with particular emphasis on events of epigenetics interest. While the basic task setup and entity definitions follow those of the GE task, EPI extends on the extraction targets by defining 14 new event types relevant to task topics, including major protein modification types and their reverse reactions. For capturing the ways in which different entities participate in these events, the task extends the GE argument roles with two new roles specific to the domain, Sidechain and Contextgene. The task design and setup are oriented toward the needs of pathway extraction and curation for domain databases (Wu et al., 2003; Ongenaert et al., 2008) and are informed by previous studies on extraction of the target events (Ohta et al., 2010b; Ohta et al., 2010c). The ID task (Pyysalo et al., 2011a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publications. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to PROTEIN four new entity types, including CHEMICAL and ORGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (Pyysalo et al., 2010; Ananiadou et al., 2011), including the support of systems such as PATRIC (http://patricbrc.org). The bacteria track consists of two tasks, BB and BI. 2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al., 2011) is to extract the habitats of bacteria mentioned in textbooklevel texts written for non-experts. The texts are Web pages about the state of the art knowledge about bacterial species. BB targets general relations, Localization and PartOf, and is challenging in that texts contain more coreferences than usual, habitat references are not necessarily named entities, and, unlike in other BioNLP-ST 2011 tasks, all entities need to be recognized by participants. BB is the first task to target phenotypic information and, as habitats are yet to be normalized by the field community, presents an opportunity for the BioNLP community to contribute to the standardization effort. The BI task (Jourde et al., 2011) is devoted to the extraction of bacterial molecular interactions and regulations from publication abstracts. Mainly focused on gene transcriptional regulation in Bacillus subtilis, the BI corpus is provided to participants with rich semantic annotation derived from a recently proposed ontology (Manine et al., 2009) defining ten entity types such as gene, protein and derivatives as well as DNA sites/motifs. Their interactions are described through ten relation types. The BI corpus consists of the sentences of the LLL corpus (N´edellec, 2005), provided with manually checked linguistic annotations. The main tasks are characterized in Table 1. From the text type perspective, BioNLP-ST 2011 generalizes from abstracts in 2009 to full papers (GE and ID) and web pages (BB). It also includes data collections for a variety of specific subject domains (GE, ID, BB an BI) and a task (EPI) whose scope is not defined through a domain but rather event types. In terms of the target event types, ID targets a superset of GE events and EPI extends on the representation for PHOSPHORYLATION events of GE. The two bacteria track tasks represent an independent perspective relatively far from other tasks in terms of their target information. BioNLP-ST 2011 includes three supporting tasks designed to assist in primary the extraction tasks. Other supporting resources made available to participants are presented in (Stenetorp et al., 2011). The CO task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections between event triggers and protein references is a major part of event extraction, it becomes much harder if one is replaced with a coreferencing expression. The CO task seeks to address this problem. The data sets for the task were produced based on MedCO annotation (Su et al., 2008) and other Genia resources (Tateisi et al., 2005; Kim et al., 2008). The REL task (Pyysalo et al., 2011b) involves the recognition of two binary part-of relations between entities: PROTEIN-COMPONENT and SUBUNITCOMPLEX. The task is motivated by specific challenges: the identification of the components of proteins in text is relevant e.g. to the recognition of Site arguments (cf. GE, EPI and ID tasks), and relations between proteins and their complexes relevant to any task involving them. REL setup is informed by recent semantic relation tasks (Hendrickx et al., 2010). The task data, consisting of new annotations for GE data, extends a previously introduced resource (Pyysalo et al., 2009; Ohta et al., 2010a). The REN task (Jourde et al., 2011) objective is to extract renaming pairs of Bacillus subtilis gene/protein names from PubMed abstracts, motivated by discrepancies between nomenclature databases that interfere with search and complicate normalization. REN relations partially overlap several concepts: explicit renaming mentions, synonymy, and renaming deduced from biological proof. While the task is related to synonymy relation extraction (Yu and Agichtein, 2003), it has a novel definition of renaming, one name permanently replacing the other. Table 2 shows the task schedule, split into two phases to allow the use of supporting task results in addressing the main tasks. In recognition of their higher complexity, a longer development period was arranged for the main tasks (3 months vs 7 weeks). BioNLP-ST 2011 received 46 submissions from 24 teams (Table 3). While seven teams participated in multiple tasks, only one team, UTurku, submitted final results to all the tasks. The remaining 17 teams participated in only single tasks. Disappointingly, only two teams (UTurku, and ConcordU) performed both supporting and main tasks, and neither used supporting task analyses for the main tasks. Detailed evaluation results and analyses are presented in individual task papers, but interesting observations can be obtained also by comparisons over the tasks. Table 4 summarizes best results for various criteria (Note that the results shown for e.g. GEa, GEf and GEp may be from different teams). The community has made a significant improvement in the repeated GE task, with an over 10% reduction in error from ’09 to GEa. Three teams achieved better results than M10, the best previously reported individual result on the ’09 data. This indicates a beneficial role from focused efforts like BioNLP-ST. The GEf and ID results show that generalization to full papers is feasible, with very modest loss in performance compared to abstracts (GEa). The results for PHOSPHORYLATION events in GE and EPI are comparable (GEp vs EPIp), with the small drop for the EPI result, suggesting that the removal of the GE domain specificity does not compromise extraction performance. EPIc results indicate some challenges in generalization to similar event types, and EPIf suggest substantial further challenges in additional argument extraction. The complexity of ID is comparable to GE, also reflected to their final results, which further indicate successful generalization to a new subject domain as well as to new argument (entity) types. The BB task is in part comparable to GEl and involves a representation similar to REL, with lower results likely in part because BB requires entity recognition. The BI task is comparable to LLL Challenge, though BI involves more entity and event types. The BI result is 20 points above the LLL best result, indicating a substantial progress of the community in five years. Meeting with wide participation from the community, BioNLP-ST 2011 produced a wealth of valuable resources for the advancement of fine-grained IE in biology and biomedicine, and demonstrated that event extraction methods can successfully generalize to new text types, event types, and domains. However, the goal to observe the capacity of supporting tasks to assist the main tasks was not met. The entire shared task period was very long, more than 6 months, and the complexity of the task was high, which could be an excessive burden for participants, limiting the application of novel resources. There have been ongoing efforts since BioNLP-ST 2009 to develop IE systems based on the task resources, and we hope to see continued efforts also following BioNLP-ST 2011, especially exploring the use of supporting task resources for main tasks.
Word-Sense Disambiguation Using Statistical Methods We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent. An alluring aspect of the statistical approach to machine translation rejuvenated by Brown et al. [Brown et at., 1988, Brown et al., 1990] is the systematic framework it provides for attacking the problem of lexical disambiguation. For example, the system they describe translates the French sentence Je vais prendre la decision as I will make the decision, correctly interpreting prendre as make. The statistical translation model, which supplies English translations of French words, prefers the more common translation take, but the trigram language model recognizes that the three-word sequence make the decision is much more probable than take the decision.. The system is not always so successful. It incorrectly renders Je vais prendre ma propre decision as / will take my own decision. The language model does not realize that take my own decision is improbable because take and decision no longer fall within a single trigram. Errors such as this are common because the statistical models only capture local phenomena; if the context necessary to determine a translation falls outside the scope of the models, the word is likely to be translated incorrectly. However, if the relevant context is encoded locally, the word should be translated correctly. We can achieve this within the traditional paradigm of analysis, transfer, and synthesis by incorporating into the analysis phase a sense-disambiguation component that assigns sense labels to French words. If prendre is labeled with one sense in the context of decision but with a different sense in other contexts, then the translation model will learn from training data that the first sense usually translates to make, whereas the other sense usually translates to take. Previous efforts at algorithmic disambiguation of word senses [Lesk, 1986, White, 1988, He and •Veronis, 1990] have concentrated on information that can be extracted from electronic dictionaries, and focus, therefore, on senses as determined by those dictionaries. Here, in contrast, we present a procedure for constructing a sense-disambiguation component that labels words so as to elucidate their translations in another language. We are conThe proposal will not now be implemented Les propositions ne seront pas mises en application maintenant cerned about senses as they occur in a dictionary only to the extent that those senses are translated differently. The French noun interet, for example, is translated into German as either Zins or Interesse according to its sense, but both of these senses are translated into English as interest, and so we make no attempt to distinguish them. Following Brown et al. [Brown et al., 1994 we choose as the translation of a French sentence F that sentence E for which Pr (EIF) is greatest. By Bayes' rule, Since the denominator does not depend on E, the sentence for which Pr (EIF) is greatest is also the sentence for which the product Pr (E) Pr (FIE) is greatest. The first factor in this product is a statistical characterization of the English language and the second factor is a statistical characterization of the process by which English sentences are translated into French. We can compute neither factors precisely. Rather, in statistical translation, we employ models from which we can obtain estimates of these values. We call the model from which we compute Pr (E) the language model and that from which we compute Pr (FIE) the translation model. The translation model used by Brown et al. [Brown et al., 1990] incorporates the concept of an alignment in which each word in E acts independently to produce some of the words in F. If we denote a typical alignment by A, then we can write the probability of F given E as a sum over all possible alignments: Although the number of possible alignments is a very rapidly growing function of the lengths of the French and English sentences, only a tiny fraction of the alignments contributes substantially to the sum, and of these few, one makes the greatest contribution. We call this most probable alignment the Viterbi alignment between E and F. The identity of the Viterbi alignment for a pair of sentences depends on the details of the translation model, but once the model is known, probable alignments can be discovered algorithmically [Brown et al., 1991]. Brown et al. [Brown et al., 1990], show an example of such au automatically derived alignment in their Figure 3. (For the reader's convenience, we have reproduced that figure here as Figure 1.) In a Viterbi alignment, a French word that is connected by a line to an English word is said to be aligned with that English word. Thus, in Figure 1, Les is aligned with The, propositions with proposal, and so on. We call a pair of aligned words obtained in this wa.y a connection. From the Viterbi alignments for 1,002,165 pairs of short French and English sentences from the Canadian Hansard data [Brown et al., 1990], we have extracted a set of 12,028,485 connections. Let p(e, f) be the probability that a connection chosen at random from this set will connect the English word e to the French word f. Because each French word gives rise to exactly one connection, the right marginal of this distribution is identical to the distribution of French words in these sentences. The left marginal, however, is not the same as the distribution of English words: English words that tend to produce several French words at a time are overrepresented while those that tend to produce no French words are underrepresented. Using p(e, f) we can compute the mutual information between a French word and its English mate in a connection. In this section, we discuss a method for labelling a word with a sense that depends on the context in which it appears in such a way as to increase the mutual information between the members of a connection. In the sentence Je vais prendre ma propre decision, the French verb prendre should be translated as make because the object of prendre is decision. If we replace decision by voiture, then prendre should be translated as take to yield I will take my own car. In these examples, one can imagine assigning a sense to prendre by asking whether the first noun to the right of prendre is decision OT voiture. We say that the noun to the right is the informant for prendre. In Il doute que les notres gagnent, which. means He doubts that we will win, the French word il should be translated as he. On the other hand, in Il faut que les notres gagnent, which means it is necessary that we win, il should be translated as it. Here, we can determine which sense to assign to il by asking about the identity of the first verb to its right. Even though we cannot hope to determine the translation of il from this informant unambiguously, we can hope to obtain a significant amount of information about the translation. As a -final example, consider the English word is. In the sentence I think it is a problem, it is best to translate is as est as in Je pense que c'est un ,probleme. However, this is certainly not true in the sentence I think there is a problem, which translates as Je pense qu'll y a un probleme. Here we can reduce the entropy of the distribution of the translation of is by asking if the word to the left is there. If so, then is is less likely to be translated as est than if not. Motivated by examples like these, we investigated a simple method of assigning two senses to a word w by asking a single binary question about one word of the context in which w appears. One does not know beforehand whether the informant will be the first noun to the right, the first verb to the right, or some other word. in the context of w. However, one can construct a question for each of a number of candidate informant sites, and then choose the most informative question. Given a potential informant such as the first 1101111 to the right, we can construct a question that has high mutual information with the translation of w by using the flip-flop algorithm devised by Nadas, Nahamoo, Picheny, and Powell [Nadas et al., 1991]. To understand their algorithm, first imagine that w is a French word and that English words which are possible translations of w have been divided into two classes. Consider the problem of constructing a binary question about the potential informant that provides maximal information about these two English word classes. If the French vocabulary is of size V, then there are 2&quot; possible questions. However, using the splitting theorem of Breiman, Friedman, 01shen, and Stone [Breiman et aL, 1984], it is possible to find the most informative of these 2v questions in time which is linear in V. The flip-flop algorithm begins by making an initial assignment of the English translations into two classes, and then uses the splitting theorem to find the best question about the potential informant. This question divides the French vocabulary into two sets. One can then use the splitting theorem to find a division of the English translations of w into two sets which has maximal mutual information with the French sets. In the flip-flop algorithm, one alternates between splitting the French vocabulary into two sets and the English translations of w into two sets. After each such split, the mutual information between the French and English sets is at least as great as before the split. Since the mutual information is bounded by one bit, the process converges to a partition of the French vocabulary that has high mutual information with the translation of w. We used the flip-flop algorithm in a pilot experiment in which we assigned two senses to each of the 500 most common English words and two senses to each of the 200 most common French words. For a French word, we considered questions about seven informants: the word to the left, the word to the right, the first noun to the left, the first noun to the right, the first verb to the left, the first verb to the right, and the tense of either the current word, if it is a verb, or of the first verb to the left of the current word. For an English word, we only considered questions about the the word to the left and the word two to the left. We restricted the English questions to the previous two words so that we could easily use them in our translation system which produces an English sentence from left to right. When a. potential informant did not exist, because, say there wa,s no noun to the left of some word in a particular sentence, we used the special word, TERM...WORD. To find the nouns and verbs in our French sentences, we used the tagging algorithm described by Merialdo [Merialdo, 1990]. Figure 2 shows the question that was constructed for the verb prendre. The noun to the right yielded the most information, .381 bits, about the English translation of prendre. The box in the top of the figure shows the words which most frequently occupy that site, that is, the nouns which appear to the right of prendre with a probability greater than one part in fifty. An instance of prendre is assigned the first or second sense depending on whether the first noun to the right appears in the lefthand or the right-hand column. So, for example, if the noun to the right of prendre is decision, parole, or connaissance, then prendre is assigned the second sense. The box at the bottom of the figure shows the most probable translations of each of the two senses. Notice that the English verb to_make is three times as likely when prendre has the second sense as when it has the first sense. People make decisions, speeches, and acquaintances, they do not take them. Figure 3 shows our results for the verb vouloir. Here, the best informant is the tense of vouloir. The first sense is three times more likely than the second sense to translate as to_want, but twelve times less likely to translate as to like. In polite English, one says I would like so and so more commonly than I would want so and so. The question in Figure 4 reduces the entropy of the translation of the French preposition depuis by .738 bits. When depuis is followed by an article, it translates with probability .772 to since, and otherwise only with probability .016. Finally, consider the English word cent. In our text, it is either a denomination of currency, in which case it is usually preceded by a number and translated as c., or it is the second half of per cent, in which case it is preceded by per and translated along with per as Z. The results in Figure 5 show that the algorithm has discovered this, and in so doing has reduced the entropy of the translation of cent by .378 bits. to a word, and thus can extract no more than one bit of information about the translation of that, word. Since the entropy of the translation of a. common word can be as high as five bits, there is reason to hope that using more senses will further improve the performance of our system. Our method asks a. single question about a single word of context. We can think of this as the first question in a decision tree which can be extended to additional levels [Lucassen, 1983, Lucassen and Mercer, 1984, Breiman et al., 1984, Bahl et a/., 1989]. We are working on these and other improvements and hope to report better results in the future. Pleased with these results, we incorporated sense-assignment questions for the 500 most common English words and 200 most common French words into our translation system. This system is an enhanced version of the one described by Brown et al. [Brown et al., 1990] in that it uses a trigram language model, and has a French vocabulary of 57,802 words, a.nd an English vocabulary of 40,809 words. We translated 100 randomly selected Hansard sentences each of which is 10 words or less in length. We judged 45 of the resultant translations as acceptable as compared with 37 acceptable translations produced by the same system running without sense-disambiguation questions.
Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical matranslation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. Statistical approaches to machine translation, pioneered by (Brown et al., 1993), achieved impressive performance by leveraging large amounts of parallel corpora. Such approaches, which are essentially stochastic string-to-string transducers, do not explicitly model natural language syntax or semantics. In reality, pure statistical systems sometimes suffer from ungrammatical outputs, which are understandable at the phrasal level but sometimes hard to comprehend as a coherent sentence. In recent years, syntax-based statistical machine translation, which aims at applying statistical models to structural data, has begun to emerge. With the research advances in natural language parsing, especially the broad-coverage parsers trained from treebanks, for example (Collins, 1999), the utilization of structural analysis of different languages has been made possible. Ideally, by combining the natural language syntax and machine learning methods, a broad-coverage and linguistically wellmotivated statistical MT system can be constructed. However, structural divergences between languages (Dorr, 1994),which are due to either systematic differences between languages or loose translations in real corpora,pose a major challenge to syntax-based statistical MT. As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) represents each production in parallel dependency trees as a finite-state transducer. Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches, though different in formalism, model the two languages using tree-based transduction rules or a synchronous grammar, possibly probabilistic, and using multi-lemma elementary structures as atomic units. The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process. However, few of the above mentioned formalisms have large scale implementations. And to the best of our knowledge, the advantages of syntax based statistical MT systems over pure statistical MT systems have yet to be empirically verified. We believe difficulties in inducing a synchronous grammar or a set of tree transduction rules from large scale parallel corpora are caused by: Hajic et al. (2002) limited non-isomorphism by n-to-m matching of nodes in the two trees. However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. In a different approach, Hwa et al. (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees. This motivated us to look for a more efficient and effective way to induce a synchronous grammar from parallel corpora and to build an MT system that performs competitively with the pure statistical MT systems. We chose to build the synchronous grammar on the parallel dependency structures of the sentences. The synchronous grammar is induced by hierarchical tree partitioning operations. The rest of this paper describes the system details as follows: Sections 2 and 3 describe the motivation behind the usage of dependency structures and how a version of synchronous dependency grammar is learned. This grammar is used as the primary translation knowledge source for our system. Section 4 defines the tree-to-tree transducer and the graphical model for the stochastic tree-to-tree transduction process and introduces a polynomial time decoding algorithm for the transducer. We evaluate our system in section 5 with the NIST/Bleu automatic MT evaluation software and the results are discussed in Section 6. According to Fox (2002), dependency representations have the best inter-lingual phrasal cohesion properties. The percentage for head crossings is 12.62% and that of modifier crossings is 9.22%. Furthermore, a grammar based on dependency structures has the advantage of being simple in formalism yet having CFG equivalent formal generative capacity (Ding and Palmer, 2004b). Dependency structures are inherently lexicalized as each node is one word. In comparison, phrasal structures (treebank style trees) have two node types: terminals store the lexical items and non-terminals store word order and phrasal scopes. Ding and Palmer (2004b) described one version of synchronous grammar: Synchronous Dependency Insertion Grammars. A Dependency Insertion Grammars (DIG) is a generative grammar formalism that captures word order phenomena within the dependency representation. In the scenario of two languages, the two sentences in the source and target languages can be modeled as being generated from a synchronous derivation process. A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990)). Apart from other details, a DIG can be viewed as a tree substitution grammar defined on dependency trees (as opposed to phrasal structure trees). The basic units of the grammar are elementary trees (ET), which are sub-sentential dependency structures containing one or more lexical items. The synchronous version, SDIG, assumes that the isomorphism of the two syntactic structures is at the ET level, rather than at the word level, hence allowing non-isomorphic tree to tree mapping. We illustrate how the SDIG works using the following pseudo-translation example: Almost any tree-transduction operations defined on a single node will fail to generate the target sentence from the source sentence without using insertion/deletion operations. However, if we view each dependency tree as an assembly of indivisible sub-sentential elementary trees (ETs), we can find a proper way to transduce the input tree to the output tree. An ET is a single “symbol” in a transducer’s language. As shown in Figure 2, each circle stands for an ET and thick arrows denote the transduction of each ET as a single symbol. As the start to our syntax-based SMT system, the SDIG must be learned from the parallel corpora. One straightforward way to induce a generative grammar is using EM style estimation on the generative process. Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004). However, a synchronous derivation process cannot handle two types of cross-language mappings: crossing-dependencies (parent-descendent switch) and broken dependencies (descendent appears elsewhere), which are illustrated below: In the above graph, the two sides are English and the foreign dependency trees. Each node in a tree stands for a lemma in a dependency tree. The arrows denote aligned nodes and those resulting inconsistent dependencies are marked with a “*”. Fox (2002) collected the statistics mainly on French and English data: in dependency representations, the percentage of head crossings per chance (case [b] in the graph) is 12.62%. Using the statistics on cross-lingual dependency consistencies from a small word to word aligned Chinese-English parallel corpus1, we found that the percentage of crossing-dependencies (case [b]) between Chinese and English is 4.7% while that of broken dependencies (case [c]) is 59.3%. The large number of broken dependencies presents a major challenge for grammar induction based on a top-down style EM learning process. Such broken and crossing dependencies can be modeled by SDIG if they appear inside a pair of elementary trees. However, if they appear between the elementary trees, they are not compatible with the isomorphism assumption on which SDIG is based. Nevertheless, the hope is that the fact that the training corpus contains a significant percentage of dependency inconsistencies does not mean that during decoding the target language sentence cannot be written in a dependency consistent way. (Ding and Palmer, 2004a) gave a polynomial time solution for learning parallel sub-sentential dependency structures from non-isomorphic dependency trees. Our approach, while similar to (Ding and Palmer, 2004a) in that we also iteratively partition the parallel dependency trees based on a heuristic function, departs (Ding and Palmer, 2004a) in three ways: (1) we base the hierarchical tree partitioning operations on the categories of the dependency trees; (2) the statistics of the resultant tree pairs from the partitioning operation are collected at each iteration rather than at the end of the algorithm; (3) we do not re-train the word to word probabilities at each iteration. Our grammar induction algorithm is sketched below: C = CategorySequence[i] be the current syntactic category set. For each tree pair in the corpus, do { a) For the tentative synchronous partitioning operation, use a heuristic function to select the BEST word pair (ei*, f j*) , where both ei*, f j* are NOT “chosen”, Category(ei*) E C and Category(f j*) E C. b) If (ei*, fj*) is found in (a), mark ei*, fj* as “chosen” and go back to (a), else go to (c). At each iteration, one specific set of categories of nodes is handled. The category sequence we used in the grammar induction is: We first process top NP chunks because they are the most stable between languages. Interestingly, NPs are also used as anchor points to learn monolingual paraphrases (Ibrahim et al., 2003). The phrasal structure categories can be extracted from automatic parsers using methods in (Xia, 2001). An illustration is given below (Chinese in pinyin form). The placement of the dependency arcs reflects the relative word order between a parent node and all its immediate children. The collected ETs are put into square boxes and the partitioning operations taken are marked with dotted arrows. Similar to (Ding and Palmer, 2004a), we also use a heuristic function in Step 1(a) of the algorithm to rank all the word pairs for the tentative tree partitioning operation. The heuristic function is based on a set of heuristics, most of which are similar to those in (Ding and Palmer, 2004a). For a word pair (ei, fj)for the tentative partitioning operation, we briefly describe the heuristics: the foreign words in the current tree. The above heuristics are a set of real valued numbers. We use a Maximum Entropy model to interpolate the heuristics in a log-linear fashion, which is different from the error minimization training in (Ding and Palmer, 2004a). where y = (0,1) as labeled in the training data whether the two words are mapped with each other. The MaxEnt model is trained using the same word level aligned parallel corpus as the one in Section 3.1. Although the training corpus isn’t large, the fact that we only have a handful of parameters to fit eased the problem. It is worth noting that the set of derived parallel dependency Elementary Trees is not a full-fledged SDIG yet. Many features in the SDIG formalism such as arguments, head percolation, etc. are not yet filled. We nevertheless use this derived grammar as a Mini-SDIG, assuming the unfilled features as empty by default. A full-fledged SDIG remains a goal for future research. As discussed before (see Figure 1 and 2), the architecture of our syntax based statistical MT system is illustrated in Figure 5. Note that this is a nondeterministic process. The input sentence is first parsed using an automatic parser and a dependency tree is derived. The rest of the pipeline can be viewed as a stochastic tree transducer. The MT decoding starts first by decomposing the input dependency tree in to elementary trees. Several different results of the decomposition are possible. Each decomposition is indeed a derivation process on the foreign side of SDIG. Then the elementary trees go through a transfer phase and target ETs are combined together into the output. The stochastic tree-to-tree transducer we propose models MT as a probabilistic optimization process. Let f be the input sentence (foreign language), and e be the output sentence (English). We have lation model” (TM) and the “language model” (LM). Assuming the decomposition of the foreign tree is given, our approach, which is based on ETs, uses the graphical model shown in Figure 6. In the model, the left side is the input dependency tree (foreign language) and the right side is the output dependency tree (English). Each circle stands for an ET. The solid lines denote the syntactical dependencies while the dashed arrows denote the statistical dependencies. Let T( x) be the dependency tree constructed from sentence x . A tree-decomposition function D( t) is defined on a dependency tree t , and outputs a certain ET derivation tree of t , which is generated by decomposing t into ETs. Given t , there could be multiple decompositions. Conditioned on decomposition D , we can rewrite (2) as: By definition, the ET derivation trees of the input and output trees should be isomorphic: D(T(f )) ≅ D(T(e)) . Let Tran(u) be a set of possible translations for the ET u . We have: For any ET v in a given ET derivation tree d , let Root(d) be the root ET of d , and let Parent(v) denote the parent ET of v . We have: an HMM. While HMM is defined on a sequence our model is defined on the derivation tree of ETs. In reality, the learned parallel ETs are unlikely to cover all the structures that we may encounter in decoding. As a unified approach, we augment the SDIG by adding all the possible word pairs (fj, ei) as a parallel ET pair and using the IBM Model 1 (Brown et al., 1993) word to word translation probability as the ET translation probability. For efficiency reasons, we use maximum approximation for (3). Instead of summing over all the possible decompositions, we only search for the best decomposition as follows: where, letting root(v) denote the root word of v , P(v  |Parent(v)) = P(root(v)  |root (Parent(v))) (6) The prior probability of tree decomposition is An analogy between our model and a Hidden Markov Model (Figure 7) may be helpful. In Eq. (4), P(u  |v) is analogous to the emission probably P(oi  |si) in an HMM. In Eq. (5), P(v  |Parent(v)) is analogous to the transition probability P(si  |si−1) in So bringing equations (4) to (9) together, the best translation would maximize: Observing the similarity between our model and a HMM, our dynamic programming decoding algorithm is in spirit similar to the Viterbi algorithm except that instead of being sequential the decoding is done on trees in a top down fashion. As to the relative orders of the ETs, we currently choose not to reorder the children ETs given the parent ET because: (1) the permutation of the ETs is computationally expensive (2) it is possible that we can resort to simple linguistic treatments on the output dependency tree to order the ETs. Currently, all the ETs are attached to each other at their root nodes. In our implementation, the different decompositions of the input dependency tree are stored in a shared forest structure, utilizing the dynamic programming property of the tree structures explicitly. Suppose the input sentence has n words and the shared forest representation has m nodes. Suppose for each word, there are maximally k different ETs containing it, we have m <_ kn . Let b be the max breadth factor in the packed forest, it can be shown that the decoder visits at most mb nodes during execution. Hence, we have: which is linear to the input size. Combined with a polynomial time parsing algorithm, the whole decoding process is polynomial time. We implemented the above approach for a Chinese-English machine translation system. We used an automatic syntactic parser (Bikel, 2002) to produce the parallel parse trees. The parser was trained using the Penn English/Chinese Treebanks. We then used the algorithm in (Xia 2001) to convert the phrasal structure trees to dependency trees to acquire the parallel dependency trees. The statistics of the datasets we used are shown as follows: The training set consists of Xinhua newswire data from LDC and the FBIS data (mostly news), both filtered to ensure parallel sentence pair quality. We used the development test data from the 2001 NIST MT evaluation workshop as our test data for the MT system performance. In the testing data, each input Chinese sentence has 4 English translations as references. Our MT system was evaluated using the n-gram based Bleu (Papineni et al., 2002) and NIST machine translation evaluation software. We used the NIST software package “mteval” version 11a, configured as case-insensitive. In comparison, we deployed the GIZA++ MT modeling tool kit, which is an implementation of the IBM Models 1 to 4 (Brown et al., 1993; AlOnaizan et al., 1999; Och and Ney, 2003). The IBM models were trained on the same training data as our system. We used the ISI Rewrite decoder (Germann et al. 2001) to decode the IBM models. The results are shown in Figure 9. The score types “I” and “C” stand for individual and cumulative n-gram scores. The final NIST and Bleu scores are marked with bold fonts. The evaluation results show that the NIST score achieved a 97.3% increase, while the Bleu score increased by 21.1%. In terms of decoding speed, the Rewrite decoder took 8102 seconds to decode the test sentences on a Xeon 1.2GHz 2GB memory machine. On the same machine, the SDIG decoder took 3 seconds to decode, excluding the parsing time. The recent advances in parsing have achieved parsers with 3 O(n ) time complexity without the grammar constant (McDonald et al., 2005). It can be expected that the total decoding time for SDIG can be as short as 0.1 second per sentence. Neither of the two systems has any specific translation components, which are usually present in real world systems (E.g. components that translate numbers, dates, names, etc.) It is reasonable to expect that the performance of SDIG can be further improved with such specific optimizations. We noticed that the SDIG system outputs tend to be longer than those of the IBM Model 4 system, and are closer to human translations in length. This partly explains why the IBM Model 4 system has slightly higher individual n-gram precision scores (while the SDIG system outputs are still better in terms of absolute matches). The relative orders between the parent and child ETs in the output tree is currently kept the same as the orders in the input tree. Admittedly, we benefited from the fact that both Chinese and English are SVO languages, and that many of orderings between the arguments and adjuncts can be kept the same. However, we did notice that this simple “ostrich” treatment caused outputs such as “foreign financial institutions the president of”. While statistical modeling of children reordering is one possible remedy for this problem, we believe simple linguistic treatment is another, as the output of the SDIG system is an English dependency tree rather than a string of words. In this paper we presented a syntax-based statistical MT system based on a Synchronous Dependency Insertion Grammar and a non-isomorphic stochastic tree-to-tree transducer. A graphical model for the transducer is defined and a polynomial time decoding algorithm is introduced. The results of our current implementation were evaluated using the NIST and Bleu automatic MT evaluation software. The evaluation shows that the SDIG system outperforms an IBM Model 4 based system in both speed and quality. Future work includes a full-fledged version of SDIG and a more sophisticated MT pipeline with possibly a tri-gram language model for decoding.
Joshua: An Open Source Toolkit for Parsing-Based Machine Translation describe an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.' When designing our toolkit, we applied general principles of software engineering to achieve three major goals: Extensibility, end-to-end coherence, and scalability. Extensibility: The Joshua code is organized into separate packages for each major aspect of functionality. In this way it is clear which files contribute to a given functionality and researchers can focus on a single package without worrying about the rest of the system. Moreover, to minimize the problems of unintended interactions and unseen dependencies, which is common hinderance to extensibility in large projects, all extensible components are defined by Java interfaces. Where there is a clear point of departure for research, a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hindering repeatability of experiments. To combat these issues, the Joshua toolkit integrates most critical components of the machine translation pipeline. Moreover, each component can be treated as a stand-alone tool and does not rely on the rest of the toolkit we provide. Scalability: Our third design goal was to ensure that the decoder is scalable to large models and data sets. The parsing and pruning algorithms are carefully implemented with dynamic programming strategies, and efficient data structures are used to minimize overhead. Other techniques contributing to scalability includes suffix-array grammar extraction, parallel and distributed decoding, and bloom filter language models. Below we give a short description about the main functions implemented in our Joshua toolkit. Rather than inducing a grammar from the full parallel training data, we made use of a method proposed by Kishore Papineni (personal communication) to select the subset of the training data consisting of sentences useful for inducing a grammar to translate a particular test set. This method works as follows: for the development and test sets that will be translated, every n-gram (up to length 10) is gathered into a map W and associated with an initial count of zero. Proceeding in order through the training data, for each sentence pair whose source-to-target length ratio is within one standard deviation of the average, if any n-gram found in the source sentence is also found in W with a count of less than k, the sentence is selected. When a sentence is selected, the count of every n-gram in W that is found in the source sentence is incremented by the number of its occurrences in the source sentence. For our submission, we used k = 20, which resulted in 1.5 million (out of 23 million) sentence pairs being selected for use as training data. There were 30,037,600 English words and 30,083,927 French words in the subsampled training corpus. Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f|e) and reverse translation probability p(e|f) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all rules from the training set. The current code requires suffix array rule extraction to be run as a pre-processing step to extract the rules needed to translate a particular test set. However, we are currently extending the decoder to directly access the suffix array. This will allow the decoder at runtime to efficiently extract exactly those rules needed to translate a particular sentence, without the need for a rule extraction pre-processing step. Grammar formalism: Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Currently, it only handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs (e.g., (Galley et al., 2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). Chart parsing: Given a source sentence to decode, the decoder generates a one-best or k-best translations using a CKY algorithm. Specifically, the decoding algorithm maintains a chart, which contains an array of cells. Each cell in turn maintains a list of proven items. The parsing process starts with the axioms, and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backpointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivations from the hypergraph. Parallel and distributed decoding: We also implement parallel decoding and a distributed language model by exploiting multi-core and multi-processor architectures and distributed computing techniques. More details on these two features are provided by Li and Khudanpur (2008b). In addition to the distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring function in Java. This Java implementation is able to read the standard ARPA backoff n-gram models, and thus the decoder can be used independently from the SRILM toolkit.3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n-grams. This native implementation is more scalable than the basic Java LM implementation. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). Johsua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu. The optimization consists of a series of line-optimizations along the dimensions corresponding to the parameters. The search across a dimension uses the efficient method of Och (2003). Each iteration of our MERT implementation consists of multiple weight updates, each reflecting a greedy selection of the dimension giving the most gain. Each iteration also optimizes several random “intermediate initial” points in addition to the one surviving from the previous iteration, as an approximation to performing multiple random restarts. More details on the MERT method and the implementation can be found in Zaidan (2009).4 We assembled a very large French-English training corpus (Callison-Burch, 2009) by conducting a web crawl that targted bilingual web sites from the Canadian government, the European Union, and various international organizations like the Amnesty International and the Olympic Committee. The crawl gathered approximately 40 million files, consisting of over 1TB of data. We converted pdf, doc, html, asp, php, etc. files into text, and preserved the directory structure of the web crawl. We wrote set of simple heuristics to transform French URLs onto English URLs, and considered matching documents to be translations of each other. This yielded 2 million French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section 2.1. For language model training, we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09. This data consisted of 21.2 million English sentences with half a billion words. We used SRILM to train a 5-gram language model using a vocabulary containing the 500,000 most frequent words in this corpus. Note that we did not use the English side of the parallel corpus as language model training data. To tune the system parameters we used News Test Set from WMT08 (Callison-Burch et al., 2008), which consists of 2,051 sentence pairs with 43 thousand English words and 46 thousand French words. This is in-domain data that was gathered from the same news sources as the WMT09 test set. The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In this system, we have used both deterministic annealing (for training) and variational decoding (for decoding). We have described a scalable toolkit for parsingbased machine translation. It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffixarray grammar extraction (Callison-Burch et al., 2005; Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable. The decoder achieves state of the art translation performance. This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone.
Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. Incrementality in parsing has been advocated for at least two different reasons. The first is mainly practical and has to do with real-time applications such as speech recognition, which require a continually updated analysis of the input received so far. The second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987). However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003). In some ways, this approach can be seen as a compromise between traditional full and partial parsing. Essentially, it is a kind of full parsing in that the goal is to build a complete syntactic analysis for the input string, not just identify major constituents. But it resembles partial parsing in being robust, efficient and deterministic. Taken together, these properties seem to make dependency parsing suitable for incremental processing, although existing implementations normally do not satisfy this constraint. For example, Yamada and Matsumoto (2003) use a multipass bottom-up algorithm, combined with support vector machines, in a way that does not result in incremental processing. In this paper, we analyze the constraints on incrementality in deterministic dependency parsing and argue that strict incrementality is not achievable. We then analyze the algorithm proposed in Nivre (2003) and show that, given the previous result, this algorithm is optimal from the point of view of incrementality. Finally, we evaluate experimentally the degree of incrementality achieved with the algorithm in practical parsing. In a dependency structure, every word token is dependent on at most one other word token, usually called its head or regent, which means that the structure can be represented as a directed graph, with nodes representing word tokens and arcs representing dependency relations. In addition, arcs may be labeled with specific dependency types. Figure 1 shows a labeled dependency graph for a simple Swedish sentence, where each word of the sentence is labeled with its part of speech and each arc labeled with a grammatical function. In the following, we will restrict our attention to unlabeled dependency graphs, i.e. graphs without labeled arcs, but the results will apply to labeled dependency graphs as well. We will also restrict ourselves to projective dependency graphs (Mel’cuk, 1988). Formally, we define these structures in the following way: We write wz < wj to express that wz precedes wj in the string W (i.e., i < j); we write wz —* wj to say that there is an arc from wz to wj; we use —** to denote the reflexive and transitive closure of the arc relation; and we use H and H* for the corresponding undirected relations, i.e. wz H wj iff wz —* wj or wj —* wz. 2. A dependency graph D = (W, A) is wellformed iff the five conditions given in Figure 2 are satisfied. The task of mapping a string W = w1· · ·wn to a dependency graph satisfying these conditions is what we call dependency parsing. For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). Having defined dependency graphs, we may now consider to what extent it is possible to construct these graphs incrementally. In the strictest sense, we take incrementality to mean that, at any point during the parsing process, there is a single connected structure representing the analysis of the input consumed so far. In terms of our dependency graphs, this would mean that the graph being built during parsing is connected at all times. We will try to make this more precise in a minute, but first we want to discuss the relation between incrementality and determinism. It seems that incrementality does not by itself imply determinism, at least not in the sense of never undoing previously made decisions. Thus, a parsing method that involves backtracking can be incremental, provided that the backtracking is implemented in such a way that we can always maintain a single structure representing the input processed up to the point of backtracking. In the context of dependency parsing, a case in point is the parsing method proposed by Kromann (Kromann, 2002), which combines heuristic search with different repair mechanisms. In this paper, we will nevertheless restrict our attention to deterministic methods for dependency parsing, because we think it is easier to pinpoint the essential constraints within a more restrictive framework. We will formalize deterministic dependency parsing in a way which is inspired by traditional shift-reduce parsing for context-free grammars, using a buffer of input tokens and a stack for storing previously processed input. However, since there are no nonterminal symbols involved in dependency parsing, we also need to maintain a representation of the dependency graph being constructed during processing. We will represent parser configurations by triples (5, I, A), where 5 is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph. (Since the nodes of the dependency graph are given by the input string, only the arc relation needs to be represented explicitly.) Given an input string W, the parser is initialized to (nil, W, 0) and terminates when it reaches a configuration (5, nil, A) (for any list 5 and set of arcs A). The input string W is accepted if the dependency graph D = (W, A) given at termination is well-formed; otherwise W is rejected. In order to understand the constraints on incrementality in dependency parsing, we will begin by considering the most straightforward parsing strategy, i.e. left-to-right bottom-up parsing, which in this case is essentially equivalent to shift-reduce parsing with a context-free grammar in Chomsky normal form. The parser is defined in the form of a transition system, represented in Figure 3 (where wi and wj are arbitrary word tokens): the two topmost tokens on the stack, wi and wj, by a right-directed arc wi —* wj and reduces them to the head wi. 3. The transition Shift pushes the next input token wi onto the stack. The transitions Left-Reduce and RightReduce are subject to conditions that ensure that the Single head condition is satisfied. For Shift, the only condition is that the input list is non-empty. As it stands, this transition system is nondeterministic, since several transitions can often be applied to the same configuration. Thus, in order to get a deterministic parser, we need to introduce a mechanism for resolving transition conflicts. Regardless of which mechanism is used, the parser is guaranteed to terminate after at most 2n transitions, given an input string of length n. Moreover, the parser is guaranteed to produce a dependency graph that is acyclic and projective (and satisfies the singlehead constraint). This means that the dependency graph given at termination is well-formed if and only if it is connected. We can now define what it means for the parsing to be incremental in this framework. Ideally, we would like to require that the graph (W —I, A) is connected at all times. However, given the definition of Left-Reduce and Right-Reduce, it is impossible to connect a new word without shifting it to the stack first, so it seems that a more reasonable condition is that the size of the stack should never exceed 2. In this way, we require every word to be attached somewhere in the dependency graph as soon as it has been shifted onto the stack. We may now ask whether it is possible to achieve incrementality with a left-to-right bottom-up dependency parser, and the answer turns out to be no in the general case. This can be demonstrated by considering all the possible projective dependency graphs containing only three nodes and checking which of these can be parsed incrementally. Figure 4 shows the relevant structures, of which there are seven altogether. We begin by noting that trees (2–5) can all be constructed incrementally by shifting the first two tokens onto the stack, then reducing – with Right-Reduce in (2–3) and Left-Reduce in (4–5) – and then shifting and reducing again – with Right-Reduce in (2) and (4) and LeftReduce in (3) and (5). By contrast, the three remaining trees all require that three tokens are Initialization hnil, W, ∅i Termination hS, nil, Ai Left-Reduce hwjwi|S, I, Ai → hwj|S, I, A ∪ {(wj, wi)}i ¬∃wk(wk, wi) ∈ A Right-Reduce hwjwi|S, I, Ai → hwi|S, I, A ∪ {(wi, wj)}i ¬∃wk(wk, wj) ∈ A Shift hS, wi|I, Ai → hwi|S, I, Ai shifted onto the stack before the first reduction. However, the reason why we cannot parse the structure incrementally is different in (1) compared to (6–7). In (6–7) the problem is that the first two tokens are not connected by a single arc in the final dependency graph. In (6) they are sisters, both being dependents on the third token; in (7) the first is the grandparent of the second. And in pure dependency parsing without nonterminal symbols, every reduction requires that one of the tokens reduced is the head of the other(s). This holds necessarily, regardless of the algorithm used, and is the reason why it is impossible to achieve strict incrementality in dependency parsing as defined here. However, it is worth noting that (2–3), which are the mirror images of (6–7) can be parsed incrementally, even though they contain adjacent tokens that are not linked by a single arc. The reason is that in (2–3) the reduction of the first two tokens makes the third token adjacent to the first. Thus, the defining characteristic of the problematic structures is that precisely the leftmost tokens are not linked directly. The case of (1) is different in that here the problem is caused by the strict bottom-up strategy, which requires each token to have found all its dependents before it is combined with its head. For left-dependents this is not a problem, as can be seen in (5), which can be processed by alternating Shift and Left-Reduce. But in (1) the sequence of reductions has to be performed from right to left as it were, which rules out strict incrementality. However, whereas the structures exemplified in (6–7) can never be processed incrementally within the present framework, the structure in (1) can be handled by modifying the parsing strategy, as we shall see in the next section. It is instructive at this point to make a comparison with incremental parsing based on extended categorial grammar, where the structures in (6–7) would normally be handled by some kind of concatenation (or product), which does not correspond to any real semantic combination of the constituents (Steedman, 2000; Morrill, 2000). By contrast, the structure in (1) would typically be handled by function composition, which corresponds to a well-defined compositional semantic operation. Hence, it might be argued that the treatment of (6–7) is only pseudo-incremental even in other frameworks. Before we leave the strict bottom-up approach, it can be noted that the algorithm described in this section is essentially the algorithm used by Yamada and Matsumoto (2003) in combination with support vector machines, except that they allow parsing to be performed in multiple passes, where the graph produced in one pass is given as input to the next pass.' The main motivation they give for parsing in multiple passes is precisely the fact that the bottomup strategy requires each token to have found all its dependents before it is combined with its head, which is also what prevents the incremental parsing of structures like (1). In order to increase the incrementality of deterministic dependency parsing, we need to combine bottom-up and top-down processing. More precisely, we need to process left-dependents bottom-up and right-dependents top-down. In this way, arcs will be added to the dependency graph as soon as the respective head and dependent are available, even if the dependent is not complete with respect to its own dependents. Following Abney and Johnson (1991), we will call this arc-eager parsing, to distinguish it from the standard bottom-up strategy discussed in the previous section. Using the same representation of parser configurations as before, the arc-eager algorithm can be defined by the transitions given in Figure 5, where wi and wj are arbitrary word tokens (Nivre, 2003): the stack to the next input token wj, and pushes wj onto the stack. 4. The transition Shift (SH) pushes the next input token wi onto the stack. The transitions Left-Arc and Right-Arc, like their counterparts Left-Reduce and RightReduce, are subject to conditions that ensure lA purely terminological, but potentially confusing, difference is that Yamada and Matsumoto (2003) use the term Right for what we call Left-Reduce and the term Left for Right-Reduce (thus focusing on the position of the head instead of the position of the dependent). that the Single head constraint is satisfied, while the Reduce transition can only be applied if the token on top of the stack already has a head. The Shift transition is the same as before and can be applied as long as the input list is non-empty. Comparing the two algorithms, we see that the Left-Arc transition of the arc-eager algorithm corresponds directly to the Left-Reduce transition of the standard bottom-up algorithm. The only difference is that, for reasons of symmetry, the former applies to the token on top of the stack and the next input token instead of the two topmost tokens on the stack. If we compare Right-Arc to Right-Reduce, however, we see that the former performs no reduction but simply shifts the newly attached right-dependent onto the stack, thus making it possible for this dependent to have rightdependents of its own. But in order to allow multiple right-dependents, we must also have a mechanism for popping right-dependents off the stack, and this is the function of the Reduce transition. Thus, we can say that the action performed by the Right-Reduce transition in the standard bottom-up algorithm is performed by a Right-Arc transition in combination with a subsequent Reduce transition in the arc-eager algorithm. And since the RightArc and the Reduce can be separated by an arbitrary number of transitions, this permits the incremental parsing of arbitrary long rightdependent chains. Defining incrementality is less straightforward for the arc-eager algorithm than for the standard bottom-up algorithm. Simply considering the size of the stack will not do anymore, since the stack may now contain sequences of tokens that form connected components of the dependency graph. On the other hand, since it is no longer necessary to shift both tokens to be combined onto the stack, and since any tokens that are popped off the stack are connected to some token on the stack, we can require that the graph (5, AS) should be connected at all times, where AS is the restriction of A to 5, i.e. AS = {(wi, wj) E A|wi, wj E 51. Given this definition of incrementality, it is easy to show that structures (2–5) in Figure 4 can be parsed incrementally with the arc-eager algorithm as well as with the standard bottomup algorithm. However, with the new algorithm we can also parse structure (1) incrementally, as wj Initialization hnil, W, ∅i Termination hS, nil, Ai Left-Arc hwi|S, wj|I, Ai → hS, wj|I, A ∪ {(wj, wi)}i ¬∃wk(wk, wi) ∈ A Right-Arc hwi|S, wj|I, Ai → hwj|wi|S, I, A ∪ {(wi, wj)}i ¬∃wk(wk, wj) ∈ A Reduce hwi|S, I, Ai → hS, I, Ai ∃wj(wj, wi) ∈ A Shift hS, wi|I, Ai → hwi|S, I, Ai is shown by the following transition sequence: hnil, abc, ∅i ↓ (Shift) ha, bc, ∅i ↓ (Right-Arc) hba, c, {(a, b)}i ↓ (Right-Arc) hcba, nil, {(a, b), (b, c)}i We conclude that the arc-eager algorithm is optimal with respect to incrementality in dependency parsing, even though it still holds true that the structures (6–7) in Figure 4 cannot be parsed incrementally. This raises the question how frequently these structures are found in practical parsing, which is equivalent to asking how often the arc-eager algorithm deviates from strictly incremental processing. Although the answer obviously depends on which language and which theoretical framework we consider, we will attempt to give at least a partial answer to this question in the next section. Before that, however, we want to relate our results to some previous work on context-free parsing. First of all, it should be observed that the terms top-down and bottom-up take on a slightly different meaning in the context of dependency parsing, as compared to their standard use in context-free parsing. Since there are no nonterminal nodes in a dependency graph, top-down construction means that a head is attached to a dependent before the dependent is attached to (some of) its dependents, whereas bottomup construction means that a dependent is attached to its head before the head is attached to its head. However, top-down construction of dependency graphs does not involve the prediction of lower nodes from higher nodes, since all nodes are given by the input string. Hence, in terms of what drives the parsing process, all algorithms discussed here correspond to bottom-up algorithms in context-free parsing. It is interesting to note that if we recast the problem of dependency parsing as context-free parsing with a CNF grammar, then the problematic structures (1), (6–7) in Figure 4 all correspond to rightbranching structures, and it is well-known that bottom-up parsers may require an unbounded amount of memory in order to process rightbranching structure (Miller and Chomsky, 1963; Abney and Johnson, 1991). Moreover, if we analyze the two algorithms discussed here in the framework of Abney and Johnson (1991), they do not differ at all as to the order in which nodes are enumerated, but only with respect to the order in which arcs are enumerated; the first algorithm is arc-standard while the second is arc-eager. One of the observations made by Abney and Johnson (1991), is that arc-eager strategies for context-free parsing may sometimes require less space than arcstandard strategies, although they may lead to an increase in local ambiguities. It seems that the advantage of the arc-eager strategy for dependency parsing with respect to structure (1) in Figure 4 can be explained along the same lines, although the lack of nonterminal nodes in dependency graphs means that there is no corresponding increase in local ambiguities. Although a detailed discussion of the relation between context-free parsing and dependency parsing is beyond the scope of this paper, we conjecture that this may be a genuine advantage of dependency representations in parsing. In order to measure the degree of incrementality achieved in practical parsing, we have evaluated a parser that uses the arc-eager parsing algorithm in combination with a memory-based classifier for predicting the next transition. In experiments reported in Nivre et al. (2004), a parsing accuracy of 85.7% (unlabeled attachment score) was achieved, using data from a small treebank of Swedish (Einarsson, 1976), divided into a training set of 5054 sentences and a test set of 631 sentences. However, in the present context, we are primarily interested in the incrementality of the parser, which we measure by considering the number of connected components in (5, As) at different stages during the parsing of the test data. The results can be found in Table 1, where we see that out of 16545 configurations used in parsing 613 sentences (with a mean length of 14.0 words), 68.9% have zero or one connected component on the stack, which is what we require of a strictly incremental parser. We also see that most violations of incrementality are fairly mild, since more than 90% of all configurations have no more than three connected components on the stack. Many violations of incrementality are caused by sentences that cannot be parsed into a wellformed dependency graph, i.e. a single projective dependency tree, but where the output of the parser is a set of internally connected components. In order to test the influence of incomplete parses on the statistics of incrementality, we have performed a second experiment, where we restrict the test data to those 444 sentences (out of 613), for which the parser produces a well-formed dependency graph. The results can be seen in Table 2. In this case, 87.1% of all configurations in fact satisfy the constraints of incrementality, and the proportion of configurations that have no more than three connected components on the stack is as high as 99.5%. It seems fair to conclude that, although strict word-by-word incrementality is not possible in deterministic dependency parsing, the arc-eager algorithm can in practice be seen as a close approximation of incremental parsing. In this paper, we have analyzed the potential for incremental processing in deterministic dependency parsing. Our first result is negative, since we have shown that strict incrementality is not achievable within the restrictive parsing framework considered here. However, we have also shown that the arc-eager parsing algorithm is optimal for incremental dependency parsing, given the constraints imposed by the overall framework. Moreover, we have shown that in practical parsing, the algorithm performs incremental processing for the majority of input structures. If we consider all sentences in the test data, the share is roughly two thirds, but if we limit our attention to well-formed output, it is almost 90%. Since deterministic dependency parsing has previously been shown to be competitive in terms of parsing accuracy (Yamada and Matsumoto, 2003; Nivre et al., 2004), we believe that this is a promising approach for situations that require parsing to be robust, efficient and (almost) incremental. The work presented in this paper was supported by a grant from the Swedish Research Council (621-2002-4207). The memorybased classifiers used in the experiments were constructed using the Tilburg Memory-Based Learner (TiMBL) (Daelemans et al., 2003). Thanks to three anonymous reviewers for constructive comments on the submitted paper.
SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task proposes a simple way to evaluate automatic extraction of temporalrelations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise eval uation of temporal relations. The task not only allows straightforward evaluation, italso avoids the complexities of full tempo ral parsing. Newspaper texts, narratives and other texts describe events that occur in time and specify the temporallocation and order of these events. Text comprehen sion, amongst other capabilities, clearly requires the capability to identify the events described in a text and locate these in time. This capability is crucial to a wide range of NLP applications, from document summarization and question answering to machine translation.Recent work on the annotation of events and temporal relations has resulted in both a de-facto stan dard for expressing these relations and a hand-builtgold standard of annotated texts. TimeML (Puste jovsky et al, 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al, 2003b; Boguraev et al., forthcoming) was originally conceived of as aproof of concept that illustrates the TimeML lan guage, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al, 2006; Boguraev et al, forthcoming).An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expres sions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemedmore effective. Thus we here present an initial eval uation exercise based on three limited tasks that webelieve are realistic both from the perspective of as sembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks. They are also tasks, whichshould they be performable automatically, have ap plication potential. The tasks as originally proposed were modified slightly during the course of resource development for the evaluation exercise due to constraints on dataand annotator availability. In the following we de scribe the tasks as they were ultimately realized in the evaluation. There were three tasks ? A, B and C. For allthree tasks the data provided for testing and train ing includes annotations identifying: (1) sentence boundaries; (2) all temporal referring expression as 75 specified by TIMEX3; (3) all events as specifiedin TimeML; (4) selected instances of temporal re lations, as relevant to the given task. For tasks A and B a restricted set of event terms were identified ? those whose stems occurred twenty times or more in TimeBank. This set is referred to as the Event Target List or ETL.TASK A This task addresses only the temporal re lations holding between time and event expressions that occur within the same sentence. Furthermore only event expressions that occur within the ETL areconsidered. In the training and test data, TLINK an notations for these temporal relations are provided, the difference being that in the test data the relation type is withheld. The task is to supply this label. TASK B This task addresses only the temporal relations holding between the Document Creation Time (DCT) and event expressions. Again onlyevent expressions that occur within the ETL are con sidered. As in Task A, TLINK annotations for these temporal relations are provided in both training and test data, and again the relation type is withheld in the test data and the task is to supply this label. TASK C Task C relies upon the idea of their beinga main event within a sentence, typically the syn tactically dominant verb. The aim is to assign thetemporal relation between the main events of adja cent sentences. In both training and test data the main events are identified (via an attribute in the event annotation) and TLINKs between these main events are supplied. As for Tasks A and B, the task here is to supply the correct relation label for these TLINKs. The TempEval annotation language is a simplifiedversion of TimeML 1. For TempEval, we use the fol lowing five tags: TempEval, s, TIMEX3, EVENT, and TLINK. TempEval is the document root and s marks sentence boundaries. All sentence tags in the TempEval data are automatically created using the Alembic Natural Language processing tools. The other three tags are discussed here in more detail:1See http://www.timeml.org for language specifica tions and annotation guidelines ? TIMEX3. Tags the time expressions in the text. It is identical to the TIMEX3 tag in TimeML. See the TimeML specifications and guidelines for further details on this tag and its attributes. Each document has one special TIMEX3 tag,the Document Creation Time, which is inter preted as an interval that spans a whole day. EVENT. Tags the event expressions in the text. The interpretation of what an event is is taken from TimeML where an event is a cover term for predicates describing situations that happen or occur as well as some, but not all, stative predicates. Events can be denoted by verbs,nouns or adjectives. The TempEval event an notation scheme is somewhat simpler than thatused in TimeML, whose complexity was designed to handle event expressions that intro duced multiple event instances (consider, e.g. He taught on Wednesday and Friday). Thiscomplication was not necessary for the Tem pEval data. The most salient attributes encodetense, aspect, modality and polarity informa tion. For TempEval task C, one extra attribute is added: mainevent, with possible values YES and NO. ? TLINK. This is a simplified version of the TimeML TLINK tag. The relation types for the TimeML version form a fine-grained set based on James Allen?s interval logic (Allen, 1983). For TempEval, we use only six relation typesincluding the three core relations BEFORE, AFTER, and OVERLAP, the two less specific relations BEFORE-OR-OVERLAP and OVERLAP OR-AFTER for ambiguous cases, and finally therelation VAGUE for those cases where no partic ular relation can be established. As stated above the TLINKs of concern for each task are explicitly included in the training and in thetest data. However, in the latter the relType at tribute of each TLINK is set to UNKNOWN. For each task the system must replace the UNKNOWN values with one of the six allowed values listed above. The EVENT and TIMEX3 annotations were takenverbatim from TimeBank version 1.2.2 The annota 2TimeBank 1.2 is available for free through the Linguistic Data Consortium, see http://www.timeml.org for more 76tion procedure for TLINK tags involved dual annotation by seven annotators using a web-based anno tation interface. After this phase, three experiencedannotators looked at all occurrences where two an notators differed as to what relation type to select and decided on the best option. For task C, there was an extra annotation phase where the main events were marked up. Main events are those events that are syntactically dominant in the sentences.It should be noted that annotation of temporal relations is not an easy task for humans due to ram pant temporal vagueness in natural language. As aresult, inter-annotator agreement scores are well be low the often kicked-around threshold of 90%, both for the TimeML relation set as well as the TempEvalrelation set. For TimeML temporal links, an inter annotator agreement of 0.77 was reported, whereagreement was measured by the average of preci sion and recall. The numbers for TempEval are even lower, with an agreement of 0.72 for anchorings of events to times (tasks A and B) and an agreement of0.65 for event orderings (task C). Obviously, num bers like this temper the expectations for automatic temporal linking. The lower number for TempEval came a bit asa surprise because, after all, there were fewer relations to choose form. However, the TempEval an notation task is different in the sense that it did not give the annotator the option to ignore certain pairs of events and made it therefore impossible to skip hard-to-classify temporal relations. In full temporal annotation, evaluation of temporal annotation runs into the same issues as evaluation of anaphora chains: simple pairwise comparisons maynot be the best way to evaluate. In temporal annota tion, for example, one may wonder how the response in (1) should be evaluated given the key in (2). (1) {A before B, A before C, B equals C} (2) {A after B, A after C, B equals C}Scoring (1) at 0.33 precision misses the interde pendence between the temporal relations. What we need to compare is not individual judgements but two partial orders. details. For TempEval however, the tasks are defined in a such a way that a simple pairwise comparison is possible since we do not aim to create a full temporal graph and judgements are made in isolation. Recall that there are three basic temporal relations (BEFORE, OVERLAP, and AFTER) as well as three disjunctions over this set (BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE). The addition of these disjunctions raises the question of how to score a response of, for example, BEFORE given akey of BEFORE-OR-OVERLAP. We use two scor ing schemes: strict and relaxed. The strict scoring scheme only counts exact matches as success. For example, if the key is OVERLAP and the responseBEFORE-OR-OVERLAP than this is counted as fail ure. We can use standard definitions of precision and recall Precision = Rc/R Recall = Rc/Kwhere Rc is number of correct answers in the response, R the total number of answers in the re sponse, and K the total number of answers in the key. For the relaxed scoring scheme, precision and recall are defined as Precision = Rcw/R Recall = Rcw/K where Rcw reflects the weighted number of correctanswers. A response is not simply counted as 1 (correct) or 0 (incorrect), but is assigned one of the val ues in table 1. B O A B-O O-A V B 1 0 0 0.5 0 0.33 O 0 1 0 0.5 0.5 0.33 A 0 0 1 0 0.5 0.33 B-O 0.5 0.5 0 1 0.5 0.67 O-A 0 0.5 0.5 0.5 1 0.67 V 0.33 0.33 0.33 0.67 0.67 1 Table 1: Evaluation weights This scheme gives partial credit for disjunctions,but not so much that non-commitment edges out pre cise assignments. For example, assigning VAGUE as the relation type for every temporal relation results in a precision of 0.33. 77 Six teams participated in the TempEval tasks. Three of the teams used statistics exclusively, one used arule-based system and the other two employed a hy brid approach. This section gives a short description of the participating systems. CU-TMP trained three support vector machine (SVM) models, one for each task. All models used the gold-standard TimeBank features for events and times as well as syntactic features derived from the text. Additionally, the relation types obtained by running the task B system on the training data for Task A and Task C, were added as a feature to the two latter systems. A subset of features was selectedusing cross-validations on the training data, discarding features whose removal improved the cross validation F-score. When applied to the test data, the Task B system was run first in order to supplythe necessary features to the Task A and Task C sys tems.LCC-TE automatically identifies temporal refer ring expressions, events and temporal relations in text using a hybrid approach, leveraging variousNLP tools and linguistic resources at LCC. For tem poral expression labeling and normalization, they used a syntactic pattern matching tool that deploys a large set of hand-crafted finite state rules. For event detection, they used a small set of heuristics as well as a lexicon to determine whether or not a token is an event, based on the lemma, part of speech and WordNet senses. For temporal relation discovery, LCC-TE used a large set of syntactic and semantic features as input to a machine learning components.NAIST-japan defined the temporal relation iden tification task as a sequence labeling model, in which the target pairs ? a TIMEX3 and an EVENT? are linearly ordered in the document. For analyz ing the relative positions, they used features fromdependency trees which are obtained from a dependency parser. The relative position between the tar get EVENT and a word in the target TIMEX3 is used as a feature for a machine learning based relation identifier. The relative positions between a word inthe target entities and another word are also intro duced. The USFD system uses an off-the-shelf Machine Learning suite(WEKA), treating the assignment of temporal relations as a simple classification task. The features used were the ones provided in theTempEval data annotation together with a few features straightforwardly computed from the docu ment without any deeper NLP analysis.WVALI?s approach for discovering intra sentence temporal relations relies on sentence-levelsyntactic tree generation, bottom-up propaga tion of the temporal relations between syntactic constituents, a temporal reasoning mechanism that relates the two targeted temporal entities to their closest ancestor and then to each other, and on conflict resolution heuristics. In establishing the temporal relation between an event and theDocument Creation Time (DCT), the temporal ex pressions directly or indirectly linked to that event are first analyzed and, if no relation is detected, the temporal relation with the DCT is propagatedtop-down in the syntactic tree. Inter-sentence tem poral relations are discovered by applying several heuristics and by using statistical data extracted from the training corpus. XRCE-T used a rule-based system that relies on a deep syntactic analyzer that was extended to treattemporal expressions. Temporal processing is inte grated into a more generic tool, a general purpose linguistic analyzer, and is thus a complement for a better general purpose text understanding system.Temporal analysis is intertwined with syntacticosemantic text processing like deep syntactic analysis and determination of thematic roles. TempEval specific treatment is performed in a post-processing stage. The results for the six teams are presented in tables 2, 3, and 4. team strict relaxed P R F P R F CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63 LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60 NAIST 0.61 0.61 0.61 0.63 0.63 0.63 USFD* 0.59 0.59 0.59 0.60 0.60 0.60 WVALI 0.62 0.62 0.62 0.64 0.64 0.64 XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41 average 0.59 0.54 0.56 0.62 0.57 0.59 stddev 0.03 0.13 0.10 0.01 0.12 0.08 Table 2: Results for Task A 78 team strict relaxed P R F P R F CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76 LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74 NAIST 0.75 0.75 0.75 0.76 0.76 0.76 USFD* 0.73 0.73 0.73 0.74 0.74 0.74 WVALI 0.80 0.80 0.80 0.81 0.81 0.81 XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71 average 0.76 0.72 0.74 0.78 0.74 0.75 stddev 0.03 0.08 0.05 0.03 0.06 0.03 Table 3: Results for Task B team strict relaxed P R F P R F CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58 LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58 NAIST 0.49 0.49 0.49 0.53 0.53 0.53 USFD* 0.54 0.54 0.54 0.57 0.57 0.57 WVALI 0.54 0.54 0.54 0.64 0.64 0.64 XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58 average 0.51 0.51 0.51 0.58 0.58 0.58 stddev 0.05 0.05 0.05 0.04 0.04 0.04 Table 4: Results for Task C All tables give precision, recall and f-measure for both the strict and the relaxed scoring scheme, aswell as averages and standard deviation on the pre cision, recall and f-measure numbers. The entry for USFD is starred because the system developers are co-organizers of the TempEval task.3 For task A, the f-measure scores range from 0.34 to 0.62 for the strict scheme and from 0.41 to 0.63 for the relaxed scheme. For task B, the scores range from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed). Finally, task C scores range from 0.42 to 0.55 (strict) and from 0.56 to 0.66 (relaxed).The differences between the systems is not spec tacular. WVALI?s hybrid approach outperforms the other systems in task B and, using relaxed scoring, in task C as well. But for task A, the winners barely edge out the rest of the field. Similarly, for task C using strict scoring, there is no system that clearly separates itself from the field.It should be noted that for task A, and in lesser ex tent for task B, the XRCE-T system has recall scores that are far below all other systems. This seemsmostly due to a choice by the developers to not as sign a temporal relation if the syntactic analyzer did not find a clear syntactic relation between the two 3There was a strict separation between people assisting in the annotation of the evaluation corpus and people involved in system development. elements that needed to be linked for the TempEval task. EvaluationThe evaluation approach of TempEval avoids the in terdependencies that are inherent to a network of temporal relations, where relations in one part of the network may constrain relations in any other part ofthe network. To accomplish that, TempEval delib erately focused on subtasks of the larger problem of automatic temporal annotation. One thing we may want to change to the present TempEval is the definition of task A. Currently, it instructs to temporally link all events in a sentence to all time expressions in the same sentence. In the future we may consider splitting this into two tasks, where one subtask focuses on those anchorings thatare very local, like ?...White House spokesman Marlin Fitzwater [said] [late yesterday] that...?. We expect both inter-annotator agreement and system per formance to be higher on this subtask. There are two research avenues that loom beyondthe current TempEval: (1) definition of other subtasks with the ultimate goal of establishing a hierar chy of subtasks ranked on performance of automatictaggers, and (2) an approach to evaluate entire time lines. Some other temporal linking tasks that can be considered are ordering of consecutive events in a sentence, ordering of events that occur in syntacticsubordination relations, ordering events in coordi nations, and temporal linking of reporting events to the document creation time. Once enough temporallinks from all these subtasks are added to the entire temporal graph, it becomes possible to let confidence scores from the separate subtasks drive a con straint propagation algorithm as proposed in (Allen, 1983), in effect using high-precision relations to constrain lower-precision relations elsewhere in the graph. With this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons. Instead the entire timeline must be evaluated. Initial ideas regarding this focus on transforming the temporal graph of a document into a set of partial orders built 79 around precedence and inclusion relations and then evaluating each of these partial orders using some kind of edit distance measure.4 We hope to have taken the first baby steps with the three TempEval tasks. We would like to thank all the people who helped prepare the data for TempEval, listed here in no particular order: Amber Stubbs, Jessica Littman, Hongyuan Qiu, Emin Mimaroglu, Emma Barker, Catherine Havasi, Yonit Boussany, Roser Saur??, and Anna Rumshisky. Thanks also to all participants to this new task: Steven Bethard and James Martin (University ofColorado at Boulder), Congmin Min, Munirathnam Srikanth and Abraham Fowler (Language Computer Corporation), Yuchang Cheng, Masayuki Asa hara and Yuji Matsumoto (Nara Institute of Science and Technology), Mark Hepple, Andrea Setzer and Rob Gaizauskas (University of Sheffield), CarolineHageg`e and Xavier Tannier (XEROX Research Cen tre Europe), and Georgiana Pus?cas?u (University of Wolverhampton and University of Alicante). Part of the work in this paper was funded bythe DTO/AQUAINT program under grant num ber N61339-06-C-0140 and part funded by the EU VIKEF project (IST- 2002-507173).
A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. I(lavans, M. Liberman, M. Jl4arcus, S. Roukos, B. Santorini, T. Strzalkowski IBM Research Division, Thomas J. Watson Research Center Yorktown Heights, NY 10598 The problem of quantitatively comparing tile perfor- mance of different broad-coverage rammars of En- glish has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even tile simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (IIewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boe- ing), Don tfindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in com- mon only the following constituents, when each gram- marian provides the single parse which he/she would ideally want his/her grammar to specify for three sam- ple Brown Corpus sentences: The famed Yankee Clipper, now retired, has been as- sisting (as (a batting coach)). One of those cai)ital-gains ventures, ill fact, has sad- dled him (with Gore Court). lie said this constituted a (very serious) misuse (of the (Criminal court) processes). Specific differences among grammars which con- tribute to this apparent disparateness of analysis in- clude the treatmeat of punctuation as independent to- kens or, on the other hand, as parasites on the words to which they attach in writing; the recursive attach- ment of auxiliary elements to the right of Verb Phrase nodes, versus their incorporation there en bloc; the grouping of pre-infinitiva,1 "to" either with the main verb alone or with the entire Verb Phrase that it in- tro(luces; and the employment or non-employment of "null nodes" as a device in the grammar; as well as 306 other differences. Despite the seeming intractability of this problem, it appears to us that a solution to it is now at hand. We propose an evaluation pro- cedure with these characteristics: it judges a parse based only on the constituent boundaries it stipulates (and not the names it assigns to these constituents); it compares the parse to a "hand-parse" of the same sentence from the University of Pennsylvania Tree- bank; and it yields two principal measures for each parse submitted. The procedure has three steps. For each parse to be evaluated: (1) erase from the fully-parsed sentence all instances of: auxiliaries, "not", pre-infinitival "to", null categories, possessive ndings (% and ), and all word-external punctuation (e.g. " , ; - ) ;  (2) recur- sively erase all parenthesis pairs enclosing either a sin- gle constituent or word, or nothing at all; (3) compute goodness cores (Crossing Parentheses, and Recall) for the input parse, by comparing it to a similarly- reduced version of the Penn Treebank parse of the same sentence. For example, for the Brown Corpus sentence: Miss Xydis was best when she did not need to be too probing, consider the candidate parse: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WI[ADV when))) ( ie -s  (PRO she)) (VP ((VPAST did) (NEG ,tot) (V need)) (VP((X to) (V be)) (ADJP(ADV too) (ADJ probing))))))(? After step-one rasures, this becomes: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WIIADV wheu))) (NP-s (PRO she)) (VP((VPAST) (NEG) (V need)) (VP((X)  (V be)) (ADJP(ADV too) (ADJ probing)))))) (? (FIN)) And after step-two erasures: (S(NP-s Miss Xydis) (VP was best) (S when she (VP need (V be (ADJP too probing))))) The Uuiversity of Pennsylvania Treebank output for this sentence, after steps one and two have been ap- plied to it, is: (S(NP Miss Xydis) (VP was best (SBAR when (S she (VP need (VP be (ADJP too probing))))))) Step three consists of comparing the candidate parse to the treebank parse and deriving two scores: (1) The Crossing Parentheses score is the number of times the treebank has a parenthesization such as, say, (A (B C)) and the parse being evaluated has a paren- thesization for the same input of ((A B) C)), i.e. there are parentheses which "cross". (2) The Recall score is the number of parenthesis pairs in the intersection of tlle candidate and treebank parses (T intersection C) divided by the number of parenthesis pairs in the treebank parse T, viz. (T intersection C) / T. This score provides an additional measure of the degree of fit between the standard and tile candidate parses; in theory a RecMl of 1 certifies a candidate parse as in- cluding all constituent boundaries that are essential to the analysis of the input sentence. We applie d this metric to 14 sentences selected from the Brown Cor- pus and analyzed by each of the grammarians named above in the manner that each wished his/her gram- mar to do. Instead of using the UPenn Treebank as a standard, we used the automaticMly computed "ma- jority parse" of each sentence obtained from the set of candidate parses themselves. The average Crossing Parentheses rate over all our grammars was .4%, with a corresponding Recall score of 94%. We have agreed on three additionM categories of systematic alteration to our input parses which we believe will significantly improve the correlation between our "ideal parses", i.e. our individuM goals, and our standard. Even at the current level of fit, we feel comfortable Mlow- ing one of our number, the UPenn parse, to serve as the standard parse, since, crucially, it. is produced by hand. Our intention is to apply the current metric to more Brown Corpus data "ideally parsed" by us, and then to employ it to measure the performance of our grammars, run automatically, on a 1)enchma.rk set of sentences. APPENDIX: EVALUATION PROCEDURE FOR COMPUTER ENGLISH GRAMMARS O. Input format A parse for evaluation should consist initially of: (a) the input word string, tokenized as follows: (I) Any tokens containing punctuation marks are enclosed by vertical bars, e.g. ~DAlbert~ I~,oooI (2) Contracted forms in which the abbreviated verb is used in the sentence under analysis as a main verb, as opposed to an auxiliary, are to be split: you ve  -> you lvel (In "Youve a good reason for that." but not in "Youve been here often.") Johns -> John lsl (In "Johns (i.e. is) a good friend" or "Johns (i.e. has) a good friend" but not "Johns (i.e. is) leaving" and not "Johns (i.e. has) been here" (3) Hyphenated words, numbers and miscellaneous digital expressions are left as is (i.e. not split), i.e. ~co-signersl (and not "co I-I signers")| 12,0001 (and not "2 I , I  0 o 0")~ lall-womanl~ Ififty-threel: Ifree-for-alll| 56th~ 13/.1~ 1212-~88-9o271~ (b) the parse of the input word string with respect to the grammar under evaluation (I) Each grammatical constituent of the input is grouped using a pair of parentheses, e.g. 307 "(((I)) ((see) ((Ed))))" (2) Constituent labels may, optionally, immediately fol low left parentheses and~or immediately precede right parentheses, e.g. (S (N (N Sue)) (V (V sees) (N (N Tom))))  = : ( ( (Sue)  ) ( ( sees) ( (Tom)  ) ) )e tc . I. Erasures of Input Elements The f irst of the two steps necessary to prepare init ial parsed input for evaluat ion consists  of erasing the fo l lowing types of word (token) str ings from the parse: (a) Auxi l iar ies Examples are : "would go there" -? "go there", "has been laughing" - ? "laughing", "does sing it correct ly" - ? "sing it correctly", but not: "is a cup", "is blue", "has a dollar", "does the laundry" (b) "Not" E.g. "is not in here" -> "is in here", "Not precisely asleep, John sort of dozed" - ? "precisely asleep, John sort of dozed" (c ) Pre- inf init ival "to" E.g. "she opted to retire" - ? "she opted retire", "how to construe it" - ? "how construe it" (d) Null categories Example 1 : ("getting more pro letters than con"): (NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) (Npl ) ) ) NOTA BENE - ? (NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) ( ))); NOTA BENE Example 2 : ("The lawyer with whom I studied law"): (NP (DET The ) (N lawyer) (S-REL (PP (P with) (NP whom ) ) (NP I) (VP (V studied) (NP (N law)) (PP 0)))) NOTA BENE - ? (NP (DET The) (N lawyer ) (S- REL (PP (P with) (NP whom) ) (NP I) (VP (V studied) (NP (N law)) (PP ) ) )) NOTA BENE (e) Possess ive endings ( s,  ) E.g. " ILorisl mother" (i.e. the mother  of Lori) - ? "Lori mother" (f) Word-external  punctuat ion (quotes, commas, periods, dashes, etc. ) The "blue book" was there - ? The blue book was there Your f i rst  , second and third ideas -? Your f i rst second and third ideas This is it. -> This is it A l l - -or  almost al l - -of  them -> All or almost all of  them But leave as is: 13,~56 18.2gl 13/17/g01 111:301 Ip.m.I I1)1 Ieh.D.I IU.N. I Ineer-do-welll 308 2. Erasures of Constituent Delimiters, i.e. Parentheses The second of the two steps necessary to prepare initial parsed input for evaluation consists of erasing parenthesis pairs, proceeding recursively, from the most to the least deeply embedded portion of the parenthesization, whenever they enclose either a single constituent or word, or nothing at all. Example: "Miss Xydis was best when she did not need to be too probing." Original parse (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST did ) (MEG not ) (V need )) (vP ((x to ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (? Parse with all erasures performed except those of const i tuentdel imiters (parentheses): (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST ) (MEG ) (V need )) (vP ((x ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (? Parse with all constituent delimiters erased which are superfluous by the above definition: (S (NP-s Miss Xydis ) (VP was best ) (S when she (vP need (vP be (ADJP too probing))))) NOTE: Any single-word adverbs which are left behind, as it were, by the erasure of auxil iary elements, are attached to the highest node of the immediately fol lowing verb constituent. Example: (will probably have) (seen Milton) -> ( probably ) (seen Milton) -> (probably seen Milton) 3. Redefinit ion of Selected Constituents The third step in the process of preparing initial parsed input for evaluation is necessary only if the parse submitted treats any of three particular constructions in a manner different from the canonical analysis currently accepted by the group. This step consists of redrawing constituent boundaries in conformity with the adopted standard. The three constructions involved are extraposition, modification of noun phrases, and sequences of prepositions which occur constituent-init ial ly and~or 309 particles which occur constituent-finally. (a) Extraposition The treatment accepted at present attaches the extraposed clause to the topmost node of the host (sentential) clause. Example: If initial analysis is: (It (is (necessary (for us to leave)))) Then change to standard as follows: (It (is necessary) (for us to leave)) NOTE: The fol lowing is not an example of extraposition, and therefore not to be modified, although it seems to differ only minimally from a genuine extraposition sentence such as: "It seemed like a good idea to begin early": (It (seemed (like ((a good meeting) (to begin early))))) (b) Modification of Noun Phrases The treatment accepted at present attaches the modified "core" noun phrase and all of its modifiers from a single (noun phrase) node: Example: If initial analysis is: ((((the tree (that (we saw))) (with (orange leaves))) (that (was (very old)))) Then change to standard as follows: ((the tree) (that (we saw)) (with (orange leaves)) (that (was (very old)))) (c) Sequences of Constituent-Initial Prepositions and~or Constituent-Final Particles For sequences of prepositions occurring at the start of a prepositional phrase, the currently accepted practice is to attach each individually to the preposit ional-phrase node. For sequences of particles which come at the end of a verb phrase or other constituent with a verbal head, the adopted standard is, likewise, to attach each individually to the top node of the constituent: Example: If initial analysis is: (We (were (out (of (oatmeal cookies))))) Then change to standard as follows: (We (were (out of (oatmeal cookies)))) ~. Computation of Evaluation Statistics (a) Number of Constituents Incompatible With Standard Parse For the sentence under analysis, compare the constituents as del imited by the standard parse with those del imited by the parse for evaluation. The first statistic computed for each sentence is the number of constituents in the parse being evaluated which "cross", i.e. are neither subsstrings nor superstrings of, the constituents of the standard parse. Example: Standard parse: ((The prospect) (of (cutting back spending))) Parse for evaluation: (The (prospect (of ((cutting back) spending)))) The (non-unary) constituents of the parse for evaluation are: 310 1. The prospect of cutting backspend ing 2. prospect of cutting back spending 3. of cutting back spending 4. cutting back spending 5. cutting back While both constituents 2 and 5 differ from the standard, only 2 qualif ies as a "crossing" violation, as 5 is merely a substring of a constituent of the standard parse. So the "Constituents Incompatible With Standard" score for this sentence is I. (b) "Recall" and "Precision" of Parse Being Evaluated As a preliminary to computing Recall: Number of Standard-Parse Constituents in Candidate Total Number of Standard-Parse Constituents and Precision: Number of Candidate-Parse Constituents in Standard Total Number of Candidate-Parse Constituents the total number of constituents in the standard parse, and in the candidate parse, are simply counted. Notice that "Number of Standard-Parse Constituents in Candidate" and "Number of Candidate-Parse Constituents in Standard" are merely different names for the same object--the intersection of the set of standard-parse constituents with the set of candidate-parse constituents. So the final count prel iminary to the computation of Recall and Precision is the number of elements in that intersection. To return to the first example of the last subsection: Standard parse: ((The prospect) (of (cutting back spending))) Parse for evaluation: (The (prospect (of ((cutting back) spending)))) there are  4 standard-parse constituents, if the convention is adopted of excluding unary constituents~ and 5 candidate-parse constituents, under the same convention. Three of these are common to both sets, i.e. the intersection here is 3. Computing Recall and Precision is accomplished for this parse as follows: Recall = 3 / Precision = 3 / 5 . (C) Combining Statistics Gathered In order to evaluate a set of parses, first simply compute a distribution over "Incompatible Constituents" scores for the parses in the set, e.g. Incompatible Constituents: 0 I 2 Frequency: 3 I I (Total = 5) Next, average the Recall and Precision scores for the various parses in the set, e.g. Average Recall = (3 /4  + 7 /8 + 2/4  + 518 + 314)  / 5 = .700 Average Precision = (3 /5 + 7 /10  + 2 /5  + 5 /10 + 315)  / 5 = .560 311
Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language— words, phrases, and sentences—is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation. The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language— words, phrases, and sentences—is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation. Although everyone may be familiar with the notion of paraphrase in its most fundamental sense, there is still room for elaboration on how paraphrases may be automatically generated or elicited for use in language processing applications. In this survey, we make an attempt at such an elaboration. An important outcome of this survey is the discovery that there are a large variety of paraphrase generation methods, each with widely differing sets of characteristics, in terms of performance as well as ease of deployment. We also find that although many paraphrase methods are developed with a particular application in mind, all methods share the potential for more general applicability. Finally, we observe that the choice of the most appropriate method for an application depends on proper matching of the characteristics of the produced paraphrases with an appropriate method. It could be argued that it is premature to survey an area of research that has shown promise but has not yet been tested for a long enough period (and in enough systems). However, we believe this argument actually strengthens the motivation for a survey that can encourage the community to use paraphrases by providing an applicationindependent, cohesive, and condensed discussion of data-driven paraphrase generation techniques. We should also acknowledge related work that has been done on furthering the community’s understanding of paraphrases. Hirst (2003) presents a comprehensive survey of paraphrasing focused on a deep analysis of the nature of a paraphrase. The current survey focuses instead on delineating the salient characteristics of the various paraphrase generation methods with an emphasis on describing how they could be used in several different NLP applications. Both these treatments provide different but valuable perspectives on paraphrasing. The remainder of this section formalizes the concept of a paraphrase, scopes out the coverage of this survey’s discussion, and provides broader context and motivation by discussing applications in which paraphrase generation has proven useful, along with examples. Section 2 briefly describes the tasks of paraphrase recognition and textual entailment and their relationship to paraphrase generation and extraction. Section 3 forms the major contribution of this survey by examining various corpora-based techniques for paraphrase generation, organized by corpus type. Section 4 examines recent work done to construct various types of paraphrase corpora and to elicit human judgments for such corpora. Section 5 considers the task of evaluating the performance of paraphrase generation and extraction techniques. Finally, Section 6 provides a brief glimpse of the future trends in paraphrase generation and Section 7 concludes the survey with a summary. The concept of paraphrasing is most generally defined on the basis of the principle of semantic equivalence: A paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form. Paraphrases may occur at several levels. Individual lexical items having the same meaning are usually referred to as lexical paraphrases or, more commonly, synonyms, for example, (hot, warm) and (eat, consume). However, lexical paraphrasing cannot be restricted strictly to the concept of synonymy. There are several other forms such as hyperonymy, where one of the words in the paraphrastic relationship is either more general or more specific than the other, for example, (reply, say) and (landlady, hostess). The term phrasal paraphrase refers to phrasal fragments sharing the same semantic content. Although these fragments most commonly take the form of syntactic phrases ((work on, soften up) and (take over, assume control of)) they may also be patterns with linked variables, for example, (Y was built by X, X is the creator of Y). Two sentences that represent the same semantic content are termed sentential paraphrases, for example, (I finished my work, I completed my assignment). Although it is possible to generate very simple sentential paraphrases by simply substituting words and phrases in the original sentence with their respective semantic equivalents, it is significantly more difficult to generate more interesting ones such as (He needed to make a quick decision in that situation, The scenario required him to make a split-second judgment). Culicover (1968) describes some common forms of sentential paraphrases. The idea of paraphrasing has been explored in conjunction with, and employed in, a large number of natural language processing applications. Given the difficulty inherent in surveying such a diverse task, an unfortunate but necessary remedy is to impose certain limits on the scope of our discussion. In this survey, we will be restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More specifically, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for specific applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques that can generate the types of paraphrases under examination in this survey: phrasal and sentential. 1.3.1 Query and Pattern Expansion. One of the most common applications of paraphrasing is the automatic generation of query variants for submission to information retrieval systems or of patterns for submission to information extraction systems. Culicover (1968) describes one of the earliest theoretical frameworks for query keyword expansion using paraphrases. One of the earliest works to implement this approach (Sp¨arckJones and Tait 1984) generates several simple variants for compound nouns in queries submitted to a technical information retrieval system. For example: Original : circuit details Variant 1 : details about the circuit Variant 2 : the details of circuits 1 Inferring words to be similar based on similar contexts can be thought of as the most common instance of employing distributional similarity. The concept of distributional similarity also turns out to be quite important for phrasal paraphrase generation and is discussed in more detail in Section 3.1. these techniques is usually effected by utilizing the query log (a log containing the record of all queries submitted to the system) to determine semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original: simultaneous measurements Variant: concurrent measures Original: development area Variant: area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Riezler et al. (2007) expand a query by generating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: Finally, paraphrases have also been used to improve the task of relation extraction (Romano et al. 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphrastic patterns for relation extraction by applying semi-supervised paraphrase induction to a very large monolingual corpus. For example, for the relation of “acquisition,” they collect: task for a given set of data and using the output so created as a reference against which to measure the performance of the system. The two applications where comparison against human-authored reference output has become the norm are machine translation and document summarization. In machine translation evaluation, the translation hypotheses output by a machine translation system are evaluated against reference translations created by human translators by measuring the n-gram overlap between the two (Papineni et al. 2002). However, it is impossible for a single reference translation to capture all possible verbalizations that can convey the same semantic content. This may unfairly penalize translation hypotheses that have the same meaning but use n-grams that are not present in the reference. For example, the given system output S will not have a high score against the reference R even though it conveys precisely the same semantic content: S: We must consider the entire community. R: We must bear in mind the community as a whole. One solution is to use multiple reference translations, which is expensive. An alternative solution, tried in a number of recent approaches, is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference translation so as to award credit to parts of the translation hypothesis that are semantically, even if not lexically, correct (Owczarzak et al. 2006; Zhou, Lin, and Hovy 2006). In evaluation of document summarization, automatically generated summaries (peers) are also evaluated against reference summaries created by human authors (models). Zhou et al. (2006) propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of n-gram overlap between peer summaries and multiple model summaries. 1.3.3 Machine Translation. Besides being used in evaluation of machine translation systems, paraphrasing has also been applied to directly improve the translation process. Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to improve a statistical phrase-based machine translation system. Such a system works by dividing the given sentence into phrases and translating each phrase individually by looking up its translation in a table. The coverage of the translation system is improved by allowing any source phrase that does not have a translation in the table to use the translation of one of its paraphrases. For example, if a given Spanish sentence contains the phrase presidente de Brazil but the system does not have a translation for it, another Spanish phrase such as presidente brasile˜no may be automatically detected as a paraphrase of presidente de Brazil; then if the translation table contains a translation for the paraphrase, the system can use the same translation for the given phrase. Therefore, paraphrasing allows the translation system to properly handle phrases that it does not otherwise know how to translate. Another important issue for statistical machine translation systems is that of reference sparsity. The fundamental problem that translation systems have to face is that there is no such thing as the correct translation for any sentence. In fact, any given source sentence can often be translated into the target language in many valid ways. Because there can be many “correct answers,” almost all models employed by SMT systems require, in addition to a large bitext, a held-out development set comprising multiple high-quality, human-authored reference translations in the target language in order to tune their parameters relative to a translation quality metric. However, given the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can benefit immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraphrase recognition. For a multi-document summarization system, detecting redundancy is a very important concern because two sentences from different documents may convey the same semantic content and it is important not to repeat the same information in the summary. On this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set of sentences by detecting paraphrastic parts and fusing them into a single coherent sentence. Recognizing similar semantic content is also critical for text simplification systems (Marsi and Krahmer 2005b). Information extraction enables the detection of regularities of information structure—events which are reported many times, about different individuals and in different forms—and making them explicit so that they can be processed and used in other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together extraction patterns to improve the cohesion of the extracted information. Another recently proposed natural language processing task is that of recognizing textual entailment: A piece of text T is said to entail a hypothesis H if humans reading T will infer that H is most likely true. The observant reader will notice that, in this sense, the task of paraphrase recognition can simply be formulated as bidirectional entailment recognition. The task of recognizing entailment is an application-independent task and has important ramifications for almost all other language processing tasks that can derive benefit from some form of applied semantic inference. For this reason, the task has received noticeable attention in the research community and annual communitywide evaluations of entailment systems have been held in the form of the Recognizing Textual Entailment (RTE) Challenges (Dagan, Glickman, and Magnini 2006; Bar-Haim et al. 2007; Sekine et al. 2007; Giampiccolo et al. 2008). Looking towards the future, Dagan (2008) suggests that the textual entailment task provides a comprehensive framework for semantic inference and argues for building a concrete inference engine that not only recognizes entailment but also searches for all entailing texts given an entailment hypothesis H and, conversely, generates all entailed statements given a text T. Given such an engine, Dagan claims that paraphrase generation is simply a matter of generating all entailed statements given any sentence. Although this is a very attractive proposition that defines both paraphrase generation and recognition in terms of textual entailment, there are some important caveats. For example, textual entailment cannot guarantee that the entailed hypothesis H captures all of the same meaning as the given text T. Consider the following example: Although both H1 and H2 are entailed by T, they are not strictly paraphrases of T because some of the semantic content has not been carried over. This must be an important consideration when building the proposed entailment engine. Of course, even these approximately semantically equivalent constructions may prove useful in an appropriate downstream application. The relationship between paraphrasing and entailment is more tightly entwined than it might appear. Entailment recognition systems sometimes rely on the use of paraphrastic templates or patterns as inputs (Iftene 2009) and might even use paraphrase recognition to improve their performance (Bosma and Callison-Burch 2007). In fact, examination of some RTE data sets in an attempt to quantitatively determine the presence of paraphrases has shown that a large percentage of the set consists of paraphrases rather than typical entailments (Bayer et al. 2005; Garoufi 2007). It has also been observed that, in the entailment challenges, it is relatively easy for submitted systems to recognize constructions that partially overlap in meaning (approximately paraphrastic) from those that are actually bound by an entailment relation. On the flip side, work has also been done to extend entailment recognition techniques for the purpose of paraphrase recognition (Rus, McCarthy, and Lintean 2008). Detection of semantic similarity and, to some extent, that of bidirectional entailment is usually an implicit part of paraphrase generation. However, given the interesting and diverse work that has been done in both these areas, we feel that any significant discussion beyond the treatment above merits a separate, detailed survey. In this section, we explore in detail the data-driven paraphrase generation approaches that have emerged and have become extremely popular in the last decade or so. These corpus-based methods have the potential of covering a much wider range of paraphrasing phenomena and the advantage of widespread availability of corpora. We organize this section by the type of corpora used to generate the paraphrases: a single monolingual corpus, monolingual comparable corpora, monolingual parallel corpora, and bilingual parallel corpora. This form of organization, in our opinion, is the most instructive because most of the algorithmic decisions made for paraphrase generation will depend heavily on the type of corpus used. For instance, it is reasonable to assume that a different set of considerations will be paramount when using a large single monolingual corpus than when using bilingual parallel corpora. However, before delving into the actual paraphrasing methods, we believe that it would be very useful to explain the motivation behind distributional similarity, an extremely popular technique that can be used for paraphrase generation with several different types of corpora. We devote the following section to such an explanation. The idea that a language possesses distributional structure was first discussed at length by Harris (1954). The term represents the notion that one can describe a language in terms of relationships between the occurrences of its elements (words, morphemes, phonemes) relative to the occurrence of other elements. The name for the phenomenon is derived from an element’s distribution—sets of elements in particular positions that the element occurs with to produce an utterance or a sentence. More specifically, Harris presents several empirical observations to support the hypothesis that such a structure exists naturally for language. Here, we closely quote these observations: Given these observations, it is relatively easy to characterize the concept of distributional similarity: words or phrases that share the same distribution—the same set of words in the same context in a corpus—tend to have similar meanings. Figure 1 shows the basic idea behind phrasal paraphrase generation techniques that leverage distributional similarity. The input corpus is usually a single or set of monolingual corpora (parallel or non-parallel). After preprocessing—which may include tagging the parts of speech, generating parse trees, and other transformations—the next step is to extract pairs of words or phrases (or patterns) that occur in the same context in the corpora and hence may be considered (approximately) semantically equivalent. This extraction may be accomplished by several means (e.g., by using a classifier employing contextual features or by finding similar paths in dependency trees). Although it is possible to stop at this point and consider this list as the final output, the list usually contains a lot of noise and may require additional filtering based on other criteria, such as collocations counts from another corpus (or the Web). Finally, some techniques may go even further and attempt to generalize the filtered list of paraphrase pairs into templates or rules which may then be applied to other sentences to generate their paraphrases. Note that generalization as a post-processing step may not be necessary if the induction process can extract distributionally similar patterns directly. One potential disadvantage of relying on distributional similarity is that items that are distributionally similar may not necessarily end up being paraphrastic: Both A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis. elements of the pairs (boys, girls), (cats, dogs), (high, low) can occur in similar contexts but are not semantically equivalent. In this section, we concentrate on paraphrase generation methods that operate on a single monolingual corpus. Most, if not all, such methods usually perform paraphrase induction by employing the idea of distributional similarity as outlined in the previous section. Besides the obvious caveat discussed previously regarding distributional similarity, we find that the other most important factor affecting the performance of these methods is the choice of distributional ingredients—that is, the features used to formulate the distribution of the extracted units. We consider three commonly used techniques that generate phrasal paraphrases (or paraphrastic patterns) from a single monolingual corpus but use very different distributional features in terms of complexity. The first uses only surface-level features and the other two use features derived from additional semantic knowledge. Although the latter two methods are able to generate more sophisticated paraphrases by virtue of more specific and more informative ingredients, we find that doing so usually has an adverse effect on their coverage. Pas¸ca and Dienes (2005) use as their input corpus a very large collection of Web documents taken from the repository of documents crawled by Google. Although using Web documents as input data does require a non-trivial pre-processing phase since such documents tend to be noisier, there are certainly advantages to the use of Web documents as the input corpus: It does not require parallel (or even comparable) documents and can allow leveraging of even larger document collections. In addition, the extracted paraphrases are not tied to any specific domain and are suitable for general application. Algorithm 1 shows the details of the induction process. Steps 3–6 extract all n-grams of a specific kind from each sentence: Each n-gram has Lc words at the beginning, between M1 to M2 words in the middle, and another Lc words at the end. Steps 7–13 can intuitively be interpreted as constructing a textual anchor A—by concatenating a fixed number of words from the left and the right—for each candidate paraphrase C and storing the (anchor, candidate) tuple in H. These anchors are taken to constitute the distribution of the words and phrases under inspection. Finally, each occurrence of a pair of potential paraphrases, that is, a pair sharing one or more anchors, is counted. The final set of phrasal paraphrastic pairs is returned. This algorithm embodies the spirit of the hypothesis of distributional similarity: It considers all words and phrases that are distributionally similar—those that occur with the same sets of anchors (or distributions)—to be paraphrases of each other. Additionally, the larger the set of shared anchors for two candidate phrases, the stronger the likelihood that they are paraphrases of each other. After extracting the list of paraphrases, less likely phrasal paraphrases are filtered out by using an appropriate count threshold. Pas¸ca and Dienes (2005) attempt to make their anchors even more informative by attempting variants where they extract the n-grams only from sentences that include specific additional information to be added to the anchor. For example, in one variant, they only use sentences where the candidate phrase is surrounded by named entities Algorithm 1 (Pa¸sca and Dienes 2005). Induce a set of phrasal paraphrase pairs H with associated counts from a corpus of pre-processed Web documents. Summary. Extract all n-grams from the corpus longer than a pre-stipulated length. Compute a lexical anchor for each extracted n-gram. Pairs of n-grams that share lexical anchors are then construed to be paraphrases. on both sides and they attach the nearest pair of entities to the anchor. As expected, the paraphrases do improve in quality as the anchors become more specific. However, they also report that as anchors are made more specific by attaching additional information, the likelihood of finding a candidate pair with the same anchor is reduced. The ingredients for measuring distributional similarity in a single corpus can certainly be more complex than simple phrases used by Pas¸ca and Dienes. Lin and Pantel (2001) discuss how to measure distributional similarity over dependency tree paths in order to induce generalized paraphrase templates such as:2 Whereas single links between nodes in a dependency tree represent direct semantic relationships, a sequence of links, or a path, can be understood to represent an indirect relationship. Here, a path is named by concatenating the dependency relationships and lexical items along the way but excluding the lexical items at the end. In this way, a path can actually be thought of as a pattern with variables at either end. Consider the first dependency tree in Figure 2. One dependency path that we could extract would be between the node John and the node problem. We start at John and see that the first item in the tree is the dependency relation subject that connects a noun to a verb and so we append that information to the path.3 The next item in the tree is the word found and we append its lemma (find) to the path. Next is the semantic relation object connecting a verb to a noun and we append that. The process continues until we reach the other slot (the word problem) at which point we stop.4 The extracted path is shown below the tree. Similarly, we can extract a path for the second dependency tree. Let’s briefly mention the terminology associated with such paths: Intuitively, one can imagine a path to be a complex representation of the pattern X finds answer to Y, where X and Y are variables. This representation for a path is a perfect fit for an extended version of the distributional similarity hypothesis: If similar sets of words fill the same variables for two different patterns, then the patterns may be considered to have similar meaning, which is indeed the case for the paths in Figure 2. Lin and Pantel (2001) use newspaper text as their input corpus and create dependency parses for all the sentences in the corpus in the pre-processing step. Algorithm 2 provides the details of the rest of the process: Steps 1 and 2 extract the paths and compute their distributional properties, and Steps 3–14 extract pairs of paths which are Two different dependency tree paths (a and b) that are considered paraphrastic because the same words (John and problem) are used to fill the corresponding slots (shown co-indexed) in both the paths. The implied meaning of each dependency path is also shown. similar, insofar as such properties are concerned.5 At the end, we have sets of paths (or inference rules) that are considered to have similar meanings by the algorithm. The performance of their dependency-path based algorithm depends heavily on the root of the extracted path. For example, whereas verbs frequently tend to have several modifiers, nouns tend to have no more than one. However, if a word has any fewer than two modifiers, no path can go through it as the root. Therefore, the algorithm tends to perform better for paths with verbal roots. Another issue is that this algorithm, despite the use of more informative distributional features, can generate several incorrect or implausible paraphrase patterns (inference rules). Recent work has shown how to filter out incorrect inferences when using them in a downstream application (Pantel et al. 2007). Finally, there is no reason for the distributional features to be in the same language as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a bilingual approach to extract English relation-based paraphrastic patterns of the form (w1, R, w2), where w1 and w2 are English words connected by a dependency link with the semantic relation R. Figure 3 shows a simple example based on their approach. First, instances of one type of pattern are extracted from a parsed monolingual corpus. In the figure, for example, a single instance of the pattern (verb, IN, pobj) has been extracted. Several new, potentially paraphrastic, English candidate patterns are then induced by replacing each of the English words with its synonyms in WordNet, one at a time. The figure shows the list of induced patterns for the given example. Next, each of the English words in each candidate pattern is translated to Chinese, via a bilingual dictionary.6 Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus. Given that the bilingual dictionary may contain multiple Chinese translations for a given English word, several Chinese patterns may be created for each English candidate pattern. Each Chinese pattern is assigned a probability value via a simple bag-of-words translation model (built from a small bilingual corpus) and a language model (trained on a Chinese collocation database); all translated patterns, along with their probability values, are then considered to be features of the particular English candidate pattern. Any English pattern can subsequently be compared to another by computing cosine similarity over their shared features. English collocation pairs whose similarity value exceeds some threshold are construed to be paraphrastic. The theme of a trade-off between the precision of the generated paraphrase set—by virtue of the increased informativeness of the distributional features—and its coverage is seen in this work as well. When using translations from the bilingual dictionary, a knowledge-rich resource, the authors report significantly higher precision than comparable methods that rely only on monolingual information to compute the distributional similarity. Predictably, they also find that recall values obtained with their dictionarybased method are lower than those obtained by other methods. Paraphrase generation techniques using a single monolingual corpus have to rely on some form of distributional similarity because there are no explicit clues available that indicate semantic equivalence. In the next section, we look at paraphrasing methods operating over data that does contain such explicit clues. It is also possible to generate paraphrastic phrase pairs from a parallel corpus where each component of the corpus is in the same language. Obviously, the biggest advantage of parallel corpora is that the sentence pairs are paraphrases almost by definition; they represent different renderings of the same meaning created by different translators making different lexical choices. In effect, they contain pairs (or sets) of sentences that are either semantically equivalent (sentential paraphrases) or have significant semantic overlap. Extraction of phrasal paraphrases can then be effected by extracting phrasal correspondences from a set of sentences that represent the same (or similar) semantic content. We present four techniques in this section that generate paraphrases by finding such correspondences. The first two techniques attempt to do so by relying, again, on the paradigm of distributional similarity: one by positing a bootstrapping distributional similarity algorithm and the other by simply adapting the previously described dependency path similarity algorithm to work with a parallel corpus. The next two techniques rely on more direct, non-distributional methods to compute the required correspondences. Barzilay and McKeown (2001) align phrasal correspondences by attempting to move beyond a single-pass distributional similarity method. They propose a bootstrapping algorithm that allows for the gradual refinement of the features used to determine similarity and yields improved paraphrase pairs. As their input corpus, they use multiple human-written English translations of literary texts such as Madame Bovary and Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic expressions because different translators would use their own words while still preserving the meaning of the original text. The parallel components are obtained by performing sentence alignment (Gale and Church 1991) on the corpora to obtain sets of parallel sentences that are then lemmatized, part-of-speech tagged and chunked in order to identify all the verb and noun phrases. The bootstrapping algorithm is then employed to incrementally learn better and better contextual features that are then leveraged to generate semantically similar phrasal correspondences. Figure 4 shows the basic steps of the algorithm. To seed the algorithm, some fake paraphrase examples are extracted by using identical words from either side of the aligned sentence pair. For example, given the following sentence pair: S1: Emma burst into tears and he tried to comfort her. S2: Emma cried and he tried to console her. A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora. (tried, tried), (her, her) may be extracted as positive examples and (tried, Emma), (tried, console) may be extracted as negative examples. Once the seeding examples are extracted, the next step is to extract contextual features for both the positive and the negative examples. These features take the form of aligned part-of-speech sequences of a given length from the left and the right of the example. For instance, we can extract the contextual feature [(L1 : PRP1,R1 : TO1), (L2 : PRP1,R2 : TO1)] of length 1 for the positive example (tried, tried). This particular contextual feature contains two tuples, one for each sentence. The first tuple (L1 : PRP1,R1 : TO1) indicates that, in the first sentence, the POS tag sequence to the left of the word tried is a personal pronoun (he) and the POS tag sequence to the right of tired is the preposition to. The second tuple is identical for this case. Note that the tags of identical tokens are indicated as such by subscripts on the POS tags. All such features are extracted for both the positive and the negative examples for all lengths less than or equal to some specified length. In addition, a strength value is calculated for each positive (negative) contextual feature f using maximum likelihood estimation as follows: strength(f )Number of positive (negative) examples surrounded by f _ Total occurrences off The extracted list of contextual features is thresholded on the basis of this strength value. The remaining contextual rules are then applied to the corpora to obtain additional positive and negative paraphrase examples that, in turn, lead to more refined contextual rules, and so on. The process is repeated for a fixed number of iterations or until no new paraphrase examples are produced. The list of extracted paraphrases at the end of the final iteration represents the final output of the algorithm. In total, about 9, 000 phrasal (including lexical) paraphrases are extracted from 11 translations of five works of classic literature. Furthermore, the extracted paraphrase pairs are also generalized into about 25 patterns by extracting part-of-speech tag sequences corresponding to the tokens of the paraphrase pairs. Barzilay and McKeown also perform an interesting comparison with another technique that was originally developed for compiling translation lexicons from bilingual parallel corpora (Melamed 2001). This technique first compiles an initial lexicon using simple co-occurrence statistics and then uses a competitive linking algorithm (Melamed 1997) to improve the quality of the lexicon. The authors apply this technique to their monolingual parallel data and observe that the extracted paraphrase pairs are of much lower quality than the pairs extracted by their own method. We present similar observations in Section 3.5 and highlight that although more recent translation techniques— specifically ones that use phrases as units of translation—are better suited to the task of generating paraphrases than the competitive linking approach, they continue to suffer from the same problem of low precision. On the other hand, such techniques can take good advantage of large bilingual corpora and capture a much larger variety of paraphrastic phenomena. Ibrahim, Katz, and Lin (2003) propose an approach that applies a modified version of the dependency path distributional similarity algorithm proposed by Lin and Pantel (2001) to the same monolingual parallel corpus (multiple translations of literary works) used by Barzilay and McKeown (2001). The authors claim that their technique is more tractable than Lin and Pantel’s approach since the sentence-aligned nature of the input parallel corpus obviates the need to compute similarity over tree paths drawn from sentences that have zero semantic overlap. Furthermore, they also claim that their technique exploits the parallel nature of a corpus more effectively than Barzilay and McKeown’s approach simply because their technique uses tree paths and not just lexical information. Specifically, they propose the following modifications to Lin and Pantel’s algorithm: Despite the authors’ claims, they offer no quantitative evaluation comparing their paraphrases with those from Lin and Pantel (2001) or from Barzilay and McKeown (2001). It is also possible to find correspondences between the parallel sentences using a more direct approach instead of relying on distributional similarity. Pang, Knight, and Marcu (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases can simply be read off. More specifically, they use the Multiple-Translation Chinese corpus that was originally developed for machine translation evaluation and contains 11 human-written English translations for each sentence in a news document. Using several sentences explicitly equivalent in semantic content has the advantage of being a richer source for paraphrase induction. As a pre-processing step, any group (of 11 sentences) that contains sentences longer than 45 words is discarded. Next, each sentence in each of the groups is parsed. All the parse trees are then iteratively merged into a shared forest. The merging algorithm proceeds top-down and continues to recursively merge constituent nodes that are expanded identically. It stops upon reaching the leaves or upon encountering the same constituent node expanded using different grammar rules. Figure 5(a) shows how the merging algorithm would work on two simple parse trees. In the figure, it is apparent that the leaves of the forest encode paraphrasing information. However, the merging only allows identical constituents to be considered as paraphrases. In addition, keyword-based heuristics need to be employed to prevent inaccurate merging of constituent nodes due to, say, alternations of active and passive voices among the The merging algorithm. (a) How the merging algorithm works for two simple parse trees to produce a shared forest. Note that for clarity, not all constituents are expanded fully. Leaf nodes with two entries represent paraphrases. (b) The word lattice generated by linearizing the forest in (a). sentences in the group. Once the forest is created, it is linearized to create the word lattice by traversing the nodes in the forest top-down and producing an alternative path in the lattice for each merged node. Figure 5(b) shows the word lattice generated for the simple two-tree forest. The lattices also require some post-processing to remove redundant edges and nodes that may have arisen due to parsing errors or limitations in the merging algorithm. The final output of the paraphrasing algorithm is a set of word lattices, one for each sentence group. These lattices can be used as sources of lexical as well as phrasal paraphrases. All alternative paths between any pair of nodes can be considered to be paraphrases of each other. For example, besides the obvious lexical paraphrases, the paraphrase pair (ate at cafe, chowed down at bistro) can also be extracted from the lattice in Figure 5(b). In addition, each path between the START and the END nodes in the lattice represents a sentential paraphrase of the original 11 sentences used to create the lattice. The direct alignment approach is able to leverage the sheer width (number of parallel alternatives per sentence position; 11 in this case) of the input corpus to do away entirely with any need for measuring distributional similarity. In general, it has several advantages. It can capture a very large number of paraphrases: Each lattice has on the order of hundreds or thousands of paths depending on the average length of the sentence group that it was generated from. In addition, the paraphrases produced are of better quality than other approaches employing parallel corpora for paraphrase induction discussed so far. However, the approach does have a couple of drawbacks: the lattices described is built using 11 manually written translations of the same sentence, each by a different translator. There are very few corpora that provide such a large number of human translations. In recent years, most MT corpora have had no more than four references, which would certainly lead to much sparser word lattices and smaller numbers of paraphrases that can be extracted. In fact, given the cost and amount of effort required for humans to translate a relatively large corpus, it is common to encounter corpora with only a single human translation. With such a corpus, of course, this technique would be unable to produce any paraphrases. One solution might be to augment the relatively few human translations with translations obtained from automatic machine translation systems. In fact, the corpus used (Huang, Graff, and Doddington 2002) also contains, besides the 11 human translations, 6 translations of the same sentence by machine translation systems available on the Internet at the time. However, no experiments are performed with the automatic translations. Finally, an even more direct method to align equivalences in parallel sentence pairs can be effected by building on word alignment techniques from the field of statistical machine translation (Brown et al. 1990). Current state-of-the-art SMT methods rely on unsupervised induction of word alignment between two bilingual parallel sentences to extract translation equivalences that can then be used to translate a given sentence in one language into another language. The same methods can be applied to monolingual parallel sentences without any loss of generality. Quirk, Brockett, and Dolan (2004) use one such method to extract phrasal paraphrase pairs. Furthermore, they use these extracted phrasal pairs to construct sentential paraphrases for new sentences. Mathematically, Quirk, Brockett, and Dolan’s approach to sentential paraphrase generation may be expressed in terms of the typical channel model equation for statistical machine translation: The equation denotes the search for the optimal paraphrase Ep for a given sentence E. We may use Bayes’ Theorem to rewrite this as: where P(Ep) is an n-gram language model providing a probabilistic estimate of the fluency of a hypothesis Ep and P(E|Ep) is the translation model, or more appropriately for paraphrasing, the replacement model, providing a probabilistic estimate of what is essentially the semantic adequacy of the hypothesis paraphrase. Therefore, the optimal sentential paraphrase may loosely be described as one that fluently captures most, if not all, of the meaning contained in the input sentence. It is important to provide a brief description of the parallel corpus used here because unsupervised induction of word alignments typically requires a relatively large number of parallel sentence pairs. The monolingual parallel corpus (or more accurately, quasiparallel, since not all sentence pairs are fully semantically equivalent) is constructed by scraping on-line news sites for clusters of articles on the same topic. Such clusters contain the full text of each article and the dates and times of publication. After removing the mark-up, the authors discard any pair of sentences in a cluster where the difference in the lengths or the edit distance is larger than some stipulated value. This method yields a corpus containing approximately 140, 000 quasi-parallel sentence pairs {(E1, E2)}, where E1 = e11e21 ... em1 , E2 = e12e22 ... en2. The following examples show that the proposed method can work well: For more details on the creation of this corpus, we refer the reader to Dolan, Quirk, and Brockett (2004) and, more specifically, to Section 4. Algorithm 3 shows how to Algorithm 3 (Quirk, Dolan, and Brockett 2004). Generate a set M of phrasal paraphrases with associated likelihood values from a monolingual parallel corpus C. Summary. Estimate a simple English to English phrase translation model from C using word alignments. Use this model to create sentential paraphrases as explained later. 4: for each word-aligned sentence pair (E1, E2)a in C do 5: Extract pairs of contiguous subsequences ( ¯e1, ¯e2) such that: generate a set of phrasal paraphrase pairs and compute a probability value for each such pair. In Step 2, a simple parameter estimation technique (Brown et al. 1993) is used to compute, for later use, the probability of replacing any given word with another. Step 3 computes a word alignment (indicated by a) between each pair of sentences. This alignment indicates for each word ei in one string that word ej in the other string from which it was most likely produced (denoted here by ei ∼ ej). Steps 4–7 extract, from each pair of sentences, pairs of short contiguous phrases that are aligned with each other according to this alignment. Note that each such extracted pair is essentially a phrasal paraphrase. Finally, a probability value is computed for each such pair by assuming that each word of the first phrase can be replaced with each word of the second phrase. This computation uses the lexical replacement probabilities computed in Step 2. Now that a set of scored phrasal pairs has been extracted, these pairs can be used to generate paraphrases for any unseen sentence. Generation proceeds by creating a lattice for the given sentence. Given a sentence E, the lattice is populated as follows: Figure 6 shows an example lattice. Once the lattice has been constructed, it is straightforward to extract the 1-best paraphrase by using a dynamic programming algorithm such as Viterbi decoding and extracting the optimal path from the lattice as scored by the product of an n-gram language model and the replacement model. In addition, as with SMT decoding, it is also possible to extract a list of n-best paraphrases from the lattice by using the appropriate algorithms (Soong and Huang 1990; Mohri and Riley 2002). Quirk, Brockett, and Dolan (2004) borrow from the statistical machine translation literature so as to align phrasal equivalences as well as to utilize the aligned phrasal equivalences to rewrite new sentences. The biggest advantage of this method is its SMT inheritance: It is possible to produce multiple sentential paraphrases for any new A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris. Alternate paths between various nodes represent phrasal replacements. The probability values associated with each edge are not shown for the sake of clarity. sentence, and there is no limit on the number of sentences that can be paraphrased.7 However, there are certain limitations: All of these limitations combined lead to paraphrases that, although grammatically sound, contain very little variety. Most sentential paraphrases that are generated involve little more than simple substitutions of words and short phrases. In Section 3.5, we will discuss other approaches that also find inspiration from statistical machine translation and attempt to circumvent the above limitations by using a bilingual parallel corpus instead of a monolingual parallel corpus. Whereas it is clearly to our advantage to have monolingual parallel corpora, such corpora are usually not very readily available. The corpora usually found in the real world are comparable instead of being truly parallel: Parallelism between sentences is replaced by just partial semantic and topical overlap at the level of documents. Therefore, for monolingual comparable corpora, the task of finding phrasal correspondences becomes harder because the two corpora may only be related by way of describing events under the same topic. In such a scenario, possible paraphrasing methods either (a) forgo any attempts at directly finding such correspondences and fall back to the distributional similarity workhorse or, (b) attempt to directly induce a form of coarsegrained alignment between the two corpora and leverage this alignment. In this section, we describe three methods that generate paraphrases from such comparable corpora. The first method falls under category (a): Here the elements whose distributional similarity is being measured are paraphrastic patterns and the distributions themselves are the named entities with which the elements occur in various sentences. In contrast, the next two methods fall under category (b) and attempt to directly discover correspondences between two comparable corpora by leveraging a novel alignment algorithm combined with some similarity heuristics. The difference between the two latter methods lies only in the efficacy of the alignment algorithm. Shinyama et al. (2002) use two sets of 300 news articles from two different Japanese newspapers from the same day as their source of paraphrases. The comparable nature of the articles is ensured because both sets are from the same day. During pre-processing, all named entities in each article are tagged and dependency parses are created for each sentence in each article. The distributional similarity driven algorithm then proceeds as follows: At the end, the output is a list of generalized paraphrase patterns with named entity types as variables. For example, the algorithm may generate the following two patterns as paraphrases: (PERSON) is promoted to (POST) the promotion of (PERSON) to (POST) is decided As a later refinement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specific concepts represented by keywords. The idea of enlisting named entities as proxies for detecting semantic equivalence is interesting and has certainly been explored before (see the discussion regarding Pas¸ca and Dienes [2005] in Section 3.2). However, it has some obvious disadvantages. The authors manually evaluate the technique by generating paraphrases for two specific domains (arrest events and personnel hirings) and find that while the precision is reasonably good, the coverage is very low primarily due to restrictions on the patterns that may be extracted in Step 2. In addition, if the average number of entities in sentences is low, the likelihood of creating incorrect paraphrases is confirmed to be higher. Let us now consider the altogether separate idea of deriving coarse-grained correspondences by leveraging the comparable nature of the corpora. Barzilay and Lee (2003) attempt to do so by generating compact sentence clusters in template form (stored as word lattices with slots) separately from each corpora and then pairing up templates from one corpus with those from the other. Once the templates are paired up, a new incoming sentence that matches one member of a template pair gets rendered as the other member, thereby generating a paraphrase. They use as input a pair of corpora: the first (C1) consisting of clusters of news articles published by Agence France Presse (AFP) and the second (C2) consisting of those published by Reuters. The two corpora may be considered comparable since the articles in each are related to the same topic and were published during the same time frame. Algorithm 4 shows some details of how their technique works. Steps 3–18 show how to cluster topically related sentences, construct a word lattice from such a cluster, and convert that into a slotted lattice—basically a word lattice with certain nodes recast as variables or empty slots. The clustering is done so as to bring together sentences pertaining to the same topics and having similar structure. The word lattice is the product of an algorithm that computes a multiple-sequence alignment (MSA) for a cluster of sentences (Step 6). A very brief outline of such an algorithm, originally developed to compute an alignment for a set of three or more protein or DNA sequences, is as follows:9 The word lattice so generated now needs to be converted into a slotted lattice to allow its use as a paraphrase template. Slotting is performed based on the following intuition: Areas of high variability between backbone nodes, that is, several distinct parallel paths, may correspond to template arguments and can be collapsed into one slot that can be filled by these arguments. However, multiple parallel paths may also appear in the lattice because of simple synonymy and those paths must be retained for paraphrase generation to be useful. To differentiate between the two cases, a synonymy threshold s of 30% is used, as shown in Steps 8–14. The basic idea behind the threshold is that as the number of sentences increases, the number of different arguments will increase faster than the number of synonyms. Figure 7 shows how a very simple word lattice may be generalized into a slotted lattice. Once all the slotted lattices have been constructed for each corpus, Steps 19–24 try to match the slotted lattices extracted from one corpus to those extracted from the other by referring back to the sentence clusters from which the original lattices were Algorithm 4 (Barzilay and Lee 2003). Generate set M of matching lattice pairs given a pair of comparable corpora C1 and C2. Summary. Gather topically related sentences from C1 into clusters. Do the same for C2. Convert each sentence cluster into a slotted lattice using a multiple-sequence alignment (MSA) algorithm. Compare all lattice pairs and output those likely to be paraphrastic. generated, comparing the sentences that were written on the same day and computing a comparison score based on overlap between the sets of arguments that fill the slots. If this computed score is greater than some fixed threshold value b, then the two lattices (or patterns) are considered to be paraphrases of each other. Besides generating pairs of paraphrastic patterns, the authors go one step further and actually use the patterns to generate paraphrases for new sentences. Given such a sentence S, the first step is to find an existing slotted lattice from either corpus that aligns best with S, in terms of the previously mentioned alignment scoring function. If some lattice is found as a match, then all that remains is to take all corresponding lattices from the other corpus that are paired with this lattice and use them to create An example showing the generalization of the word lattice (a) into a slotted lattice (b). The word lattice is produced by aligning seven sentences. Nodes having in-degrees > 1 occur in more than one sentence. Nodes with thick incoming edges occur in all sentences. multiple rewritings (paraphrases) for S. Rewriting in this context is a simple matter of substitution: For each slot in the matching lattice, we know not only the argument from the sentence that fills it but also the slot in the corresponding rewriting lattice. As far as the quality of acquired paraphrases is concerned, this approach easily outperforms almost all other sentential paraphrasing approaches surveyed in this article. However, a paraphrase is produced only if the incoming sentence matches some existing template, which leads to a strong bias favoring quality over coverage. In addition, the construction and generalization of lattices may become computationally expensive when dealing with much larger corpora. We can also compare and contrast Barzilay and Lee’s work and the work from Section 3.3 that seems most closely related: that of Pang, Knight, and Marcu (2003). Both take sentences grouped together in a cluster and align them into a lattice using a particular algorithm. Pang, Knight, and Marcu have a pre-defined size for all clusters since the input corpus is an 11-way parallel corpus. However, Barzilay and Lee have to construct the clusters from scratch because their input corpus has no pre-defined notion of parallelism at the sentence level. Both approaches use word lattices to represent and induce paraphrases since a lattice can efficiently and compactly encode n-gram similarities (sets of shared overlapping word sequences) between a large number of sentences. However, the two approaches are also different in that Pang, Knight, and Marcu use the parse trees of all sentences in a cluster to compute the alignment (and build the lattice), whereas Barzilay and Lee use only surface level information. Furthermore, Barzilay and Lee can use their slotted lattice pairs to generate paraphrases for novel and unseen sentences, whereas Pang, Knight, and Marcu cannot paraphrase new sentences at all. Shen et al. (2006) attempt to improve Barzilay and Lee’s technique by trying to include syntactic constraints in the cluster alignment algorithm. In that way, it is doing something similar to what Pang, Knight, and Marcu do but with a comparable corpus instead of a parallel one. More precisely, whereas Barzilay and Lee use a relatively simple alignment scoring function based on purely lexical features, Shen et al. try to bring syntactic features into the mix. The motivation is to constrain the relatively free nature of the alignment generated by the MSA algorithm—which may lead to the generation of grammatically incorrect sentences—by using informative syntactic features. In their approach, even if two words are a lexical match—as defined by Barzilay and Lee (2003)—they are further inspected in terms of certain pre-defined syntactic features. Therefore, when computing the alignment similarity score, two lexically matched words across a sentence pair are not considered to fully match unless their score on syntactic features also exceeds a preset threshold. The syntactic features constituting the additional constraints are defined in terms of the output of a chunk parser. Such a parser takes as input the syntactic trees of the sentences in a topic cluster and provides the following information for each word: With this information and a heuristic to compute the similarity between two words in terms of their POS and IOB tags, the alignment similarity score can be calculated as the sum of the heuristic similarity value for the given two words and the heuristic similarity values for each corresponding node in the two IOB chains. If this score is higher than some threshold and the two words have similar positions in their respective sentences, then the words are considered to be a match and can be aligned. Given this alignment algorithm, the word lattice representing the global alignment is constructed in an iterative manner similar to the MSA approach. Shen et al. (2006) present evidence from a manual evaluation that sentences sampled from lattices constructed via the syntactically informed alignment method receive higher grammaticality scores as compared to sentences from the lattices constructed via the purely lexical method. However, they present no analysis of the actual paraphrasing capacity of their, presumably better aligned, lattices. Indeed, they explicitly mention that their primary goal is to measure the correlation between the syntax-augmented scoring function and the correctness of the sentences being generated from such lattices, even if the sentences do not bear a paraphrastic relationship to the input. Even if one were to assume that the syntax-based alignment method would result in better paraphrases, it still would not address the primary weakness of Barzilay and Lee’s method: Paraphrases are only generated for new sentences that match an already existing lattice, leading to lower coverage. In the last decade, there has been a resurgence in research on statistical machine translation. There has also been an accompanying dramatic increase in the number of available bilingual parallel corpora due to the strong interest in SMT from both the public and private sectors. Recent research in paraphrase generation has attempted to leverage these very large bilingual corpora. In this section, we look at such approaches that rely on the preservation of meaning across languages and try to recover said meaning by using cues from the second language. Using bilingual parallel corpora for paraphrasing has the inherent advantage that sentences in the other language are exactly semantically equivalent to sentences in the intended paraphrasing language. Therefore, the most common way to generate paraphrases with such a corpus exploits both its parallel and bilingual natures: Align phrases across the two languages and consider all co-aligned phrases in the intended language to be paraphrases. The bilingual phrasal alignments can simply be generated by using the automatic techniques developed for the same task in the SMT literature. Therefore, arguably the most important factor affecting the performance of these techniques is usually the quality of the automatic bilingual phrasal (or word) alignment techniques. One of the most popular methods leveraging bilingual parallel corpora is that proposed by Bannard and Callison-Burch (2005). This technique operates exactly as described above by attempting to infer semantic equivalence between phrases in the same language indirectly with the second language as a bridge. Their approach builds on one of the initial steps used to train a phrase-based statistical machine translation system (Koehn, Och, and Marcu 2003). Such systems rely on phrase tables—a tabulation of correspondences between phrases in the source language and phrases in the target language. These tables are usually extracted by inducing word alignments between sentence pairs in a training corpus and then incrementally building longer phrasal correspondences from individual words and shorter phrases. Once such a tabulation of bilingual phrasal correspondences is available, correspondences between phrases in one language may be inferred simply by using the phrases in the other language as pivots. Algorithm 5 shows how monolingual phrasal correspondences are extracted from a bilingual corpus C by using word alignments. Steps 3–7 extract bilingual phrasal correspondences from each sentence pair in the corpus by using heuristically induced bidirectional word alignments. Figure 8 illustrates this extraction process for two example sentence pairs. For each pair, a matrix shows the alignment between the Chinese and the English words. Element (i, j) of the matrix is filled if there is an alignment link between the ith Chinese word and the jth English word ej. All phrase pairs consistent with the word alignment are then extracted. A consistent phrase pair can intuitively be thought of as a sub-matrix where all alignment points for its rows and columns are inside it and never outside. Next, Steps 8–11 take all English phrases that correspond to the same foreign phrase and infer them all to be paraphrases of each other.10 For example, the English paraphrase pair (effectively contained, under control) is obtained from Figure 8 by pivoting on the Chinese phrase , shown underlined for both matrices. Algorithm 5 (Bannard and Callison-Burch 2005). Generate set M of monolingual paraphrase pairs given a bilingual parallel corpus C. Summary. Extract bilingual phrase pairs from C using word alignments and standard SMT heuristics. Pivot all pairs of English phrases on any shared foreign phrases and consider them paraphrases. The alignment notation from Algorithm 3 is employed. where both p(¯ej |f¯) and p(¯f |¯ek) can be computed using maximum likelihood estimation as part of the bilingual phrasal extraction process: number of times f¯ is extracted with ¯ej number of times f¯ is extracted with any e¯ p( ¯ej |f¯ ) = Once the probability values are obtained, the most likely paraphrase can be chosen for any phrase. Bannard and Callison-Burch (2005) are able to extract millions of phrasal paraphrases from a bilingual parallel corpus. Such an approach is able to capture a large variety of paraphrastic phenomena in the inferred paraphrase pairs but is seriously limited by the bilingual word alignment technique. Even state-of-the-art alignment methods from SMT are known to be notoriously unreliable when used for aligning phrase pairs. The authors find via manual evaluation that the quality of the phrasal paraphrases obtained via manually constructed word alignments is significantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when using Arabic—a language significantly different from English—as the pivot language.11 In this study, we found that the paraphrase pairs in the sample set could be grouped into the following three broad categories: form of one of the words in the phrases and cannot really be considered paraphrases. Examples: (ten ton, ten tons), (caused clouds, causing clouds). Besides there being obvious linguistic differences between Arabic and English, the primary reason for the generation of phrase pairs that lie in categories (a) and (b) is incorrectly induced alignments between the English and Arabic words, and hence, phrases. Therefore, a good portion of subsequent work on paraphrasing using bilingual corpora, as discussed below focuses on using additional machinery or evidence to cope with the noisy alignment process. Before we continue, we believe it would be useful to draw a connection between Bannard and Callison-Burch’s (2005) work and that of Wu and Zhou (2003) as discussed in Section 3.2. Note that both of these techniques rely on a secondary language to provide the cues for generating paraphrases in the primary language. However, Wu and Zhou rely on a pre-compiled bilingual dictionary to discover these cues whereas Bannard and Callison-Burch have an entirely datadriven discovery process. In an attempt to address some of the noisy alignment issues, Callison-Burch (2008) recently proposed an improvement that places an additional syntactic constraint on the phrasal paraphrases extracted via the pivot-based method from bilingual corpora and showed that using such a constraint leads to a significant improvement in the quality of the extracted paraphrases.12 The syntactic constraint requires that the extracted paraphrase be of the same syntactic type as the original phrase. With this constraint, estimating the paraphrase probability now requires the incorporation of syntactic type into the equation: where s(e) denotes the syntactic type of the English phrase e. As before, maximum likelihood estimation is employed to compute the two component probabilities: number of times f¯ is extracted with ¯ej and type s(ek) number of times f¯ is extracted with any e¯ and type s(ek) p(¯ej |¯f, s(ek)) If the syntactic types are restricted to be simple constituents (NP, VP, etc. ), then using this constraint will actually exclude some of the paraphrase pairs that could have been extracted in the unconstrained approach. This leads to the familiar precisionrecall tradeoff: It only extracts paraphrases that are of higher quality, but the approach has a significantly lower coverage of paraphrastic phenomena that are not necessarily syntactically motivated. To increase the coverage, complex syntactic types such as those used in Combinatory Categorial Grammars (Steedman 1996) are employed, which can help denote a syntactic constituent with children missing on the left and/or right hand sides. An example would be the complex type VP/(NP/NNS) which denotes a verb phrase missing a noun phrase to its right which, in turn, is missing a plural noun to its right. The primary benefit of using complex types is that less useful paraphrastic phrase pairs from different syntactic categories such as (accurately, precise), that would have been allowed in the unconstrained pivot-based approach, are now disallowed. The biggest advantage of this approach is the use of syntactic knowledge as one form of additional evidence in order to filter out phrase pairs from categories (a) and (b) as defined in the context of our manual inspection of pivot-based paraphrases above. Indeed, the authors conduct a manual evaluation to show that the syntactically constrained paraphrase pairs are better than those produced without such constraints. However, there are two additional benefits of this technique: We must also note that requiring syntactic constraints for pivot-based paraphrase extraction restricts the approach to those languages where a reasonably good parser is available. An obvious extension of the Callison-Burch style approach is to use the collection of pivoted English-to-English phrase pairs to generate sentential paraphrases for new sentences. Madnani et al. (2008a) combine the pivot-based approach to paraphrase acquisition with a well-defined English-to-English translation model that is then used in an (unmodified) SMT system, yielding sentential paraphrases by means of “translating” input English sentences. However, instead of fully lexicalized phrasal correspondences as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former by replacing aligned sub-phrases with non-terminal symbols. For example, given the initial phrase pair , growth rate has been effectively contained), the hierarchical phrase pair (X1 X2, X1 has been X2) can be formed.13 Each hierarchical phrase pair can also have certain features associated with it that are estimated via maximum likelihood estimation during the extraction process. Such phrase pairs can formally be considered the rules of a bilingual synchronous context-free grammar (SCFG). Translation with SCFGs is equivalent to parsing the string in the source language using these rules to generate the highest-scoring tree and then reading off the tree in target order. For the purposes of this survey, it is sufficient to state that efficient methods to extract such rules, to estimate their features, and to translate with them are now well established. For more details on building SCFG-based models and translating with them, we refer the reader to (Chiang 2006, 2007). Once a set of bilingual hierarchical rules has been extracted along with associated features, the pivoting trick can be applied to infer monolingual hierarchical paraphrase pairs (or paraphrastic patterns). However, the patterns are not the final output and are actually used as rules from a monolingual SCFG grammar in order to define an English-to-English translation model. Features for each monolingual rule are estimated in terms of the features of the bilingual pairs that the rule was inferred from. A sentential paraphrase can then be generated for any given sentence by using this model along with an n-gram language model and a regular SMT decoder to paraphrase (or monolingually translate) any sentence just as one would translate bilingually. The primary advantage of this approach is the ability to produce good quality sentential paraphrases by leveraging the SMT machinery to address the noise issue. However, although the decoder and the language model do serve to counter the noisy word alignment process, they do so only to a degree and not entirely. Again, we must draw a connection between this work and that of Quirk, Brockett, and Dolan (2004) (discussed in Section 3.3) because both treat paraphrasing as monolingual translation. However, as outlined in the discussion of that work, Quirk, Brockett, and Dolan use a relatively simplistic translation model and decoder which leads to paraphrases with little or no lexical variety. In contrast, Madnani et al. use a more complex translation model and an unmodified state-of-the-art SMT decoder to produce paraphrases that are much more diverse. Of course, the reliance of the latter approach on automatic word alignments does inevitably lead to much noisier sentential paraphrases than those produced by Quirk, Brockett, and Dolan. Kok and Brockett (2010) present a novel take on generating phrasal paraphrases with bilingual corpora. As with most approaches based on parallel corpora, they also start with phrase tables extracted from such corpora along with the corresponding phrasal translation probabilities. However, instead of performing the usual pivoting step with the bilingual phrases in the table, they take a graphical approach and represent each phrase in the table as a node, leading to a bipartite graph. Two nodes in the graph are connected to each other if they are aligned to each other. In order to extract paraphrases, they sample random paths in the graph from any English node to another. Note that the traditional pivot step is equivalent to a path of length two: one English phrase to the foreign pivot phrase and then to the potentially paraphrastic English phrase. By allowing paths of lengths longer than two, this graphical approach can find more paraphrases for any given English phrase. Furthermore, instead of restricting themselves to a single bilingual phrase table, they take as input a number of phrase tables, each corresponding to a different pair of six languages. Similar to the single-table case, each phrase in each table is represented as a node in a graph that is no longer bipartite in nature. By allowing edges to exist between nodes of all the languages if they are aligned, the pivot can now even be a set of nodes rather than a single node in another language. For example, one could easily find the following path in such a graph: ate lunch → aßen zu ittag (German) → aten een hapje (Dutch) → had a bite In general, each edge is associated with a weight corresponding to the bilingual phrase translation probability. Random walks are then sampled from the graph in such a way that only paths of high probability end up contributing to the extracted paraphrases. Obviously, the alignment errors discussed in the context of simple pivoting will also have an adverse effect on this approach. In order to prevent this, the authors add special feature nodes to the graph in addition to regular nodes. These feature nodes represent domain-specific knowledge of what would make good paraphrases. For example, nodes representing syntactic equivalence classes of the start and end words of the English phrases are added. This indicates that phrases that start and end with the same kind of words (interrogatives or articles) are likely to be paraphrases. Astute readers will make the following observations about the syntactic feature nodes used by the authors: The authors extract paraphrases for a small set of input English paraphrases and show that they are able to generate a larger percentage of correct paraphrases compared to the syntactically constrained approach proposed by Callison-Burch (2008). They conduct no formal evaluation of the coverage of their approach but show that, in a limited setting, it is higher than that for the syntactically constrained pivot-based approach. However, they perform no comparisons of their coverage with the original pivot-based approach (Bannard and Callison-Burch 2005). Before we present some specific techniques from the literature that have been employed to evaluate paraphrase generation methods, it is important to examine some recent work that has been done on constructing paraphrase corpora. As part of this work, human subjects are generally asked to judge whether two given sentences are paraphrases of each other. We believe that a detailed examination of this manual evaluation task provides an illuminating insight into the nature of a paraphrase in a practical, rather than a theoretical, context. In addition, it has obvious implications for any method, whether manual or automatic, that is used to evaluate the performance of a paraphrase generator. Dolan and Brockett (2005) were the first to attempt to build a paraphrase corpus on a large scale. The Microsoft Research Paraphrase (MSRP) Corpus is a collection of 5,801 sentence pairs, each manually labeled with a binary judgment as to whether it constitutes a paraphrase or not. As a first step, the corpus was created using a heuristic extraction method in conjunction with an SVM-based classifier that was trained to select likely sentential paraphrases from a large monolingual corpus containing news article clusters. However, the more interesting aspects of the task were the subsequent evaluation of these extracted sentence pairs by human annotators and the set of issues encountered when defining the evaluation guidelines for these annotators. It was observed that if the human annotators were instructed to mark only the sentence pairs that were strictly semantically equivalent or that exhibited bidirectional entailment as paraphrases, then the results were limited to uninteresting sentence pairs such as the following: S1: The euro rose above US$1.18, the highest price since its January 1999 launch. S2: The euro rose above $1.18, the highest level since its launch in January 1999. S1: However, without a carefully controlled study, there was little clear proof that the operation actually improves people’s lives. S2: But without a carefully controlled study, there was little clear proof that the operation improves people’s lives. Instead, they discovered that most of the complex paraphrases—ones with alternations more interesting than simple lexical synonymy and local syntactic changes— exhibited varying degrees of semantic divergence. For example: Therefore, in order to be able to create a richer paraphrase corpus, one with many complex alternations, the instructions to the annotators had to be relaxed; the degree of mismatch accepted before a sentence pair was judged to be fully semantically divergent (or “non-equivalent”) was left to the human subjects. It is also reported that, given the idiosyncratic nature of each sentence pair, only a few formal guidelines were generalizable enough to take precedence over the subjective judgments of the human annotators. Despite the somewhat loosely defined guidelines, the inter-annotator agreement for the task was 84%. However, a kappa score of 62 indicated that the task was overall a difficult one (Cohen 1960). At the end, 67% of the sentence pairs were judged to be paraphrases of each other and the rest were judged to be non-equivalent.14 Although the MSRP Corpus is a valuable resource and its creation provided valuable insight into what constitutes a paraphrase in the practical sense, it does have some shortcomings. For example, one of the heuristics used in the extraction process was that the two sentences in a pair must share at least three words. Using this constraint rules out any paraphrase pairs that are fully lexically divergent but still semantically equivalent. The small size of the corpus, when combined with this and other such constraints, precludes the use of the corpus as training data for a paraphrase generation or extraction system. However, it is fairly useful as a freely available test set to evaluate paraphrase recognition methods. On a related note, Fujita and Inui (2005) take a more knowledge-intensive approach to building a Japanese corpus containing sentence pairs with binary paraphrase judgments and attempt to focus on variety and on minimizing the human annotation cost. The corpus contains 2,031 sentence pairs each with a human judgment indicating whether the paraphrase is correct or not. To build the corpus, they first stipulate a typology of paraphrastic phenomena (rewriting light-verb constructions, for example) and then manually create a set of morpho-syntactic paraphrasing rules and patterns describing each type of paraphrasing phenomenon. A paraphrase generation system using these rules (Fujita et al. 2004) is then applied to a corpus containing Japanese news articles, and example paraphrases are generated for the sentences in the corpus. These paraphrase pairs are then handed to two human annotators who create binary judgments for each pair indicating whether or not the paraphrase is correct. Using a class-oriented approach is claimed to have a two-fold advantage: The biggest disadvantage of this approach is that only two types of paraphrastic phenomena are used: light-verb constructions and transitivity alternations (using intransitive verbs in place of transitive verbs). The corpus indeed captures almost all examples of both types of paraphrastic phenomena and any that are absent can be easily covered by adding one or two more patterns to the class. The claim of reduced annotation cost is not necessarily borne out by the observations. Despite partitioning the annotation task by types, it was still difficult to provide accurate annotation guidelines. This led to a significant difference in annotation time—with some annotations taking almost twice as long as others. Given the small size of the corpus, it is unlikely that it may be used as training data for corpus-based paraphrase generation methods and, like the MSRP corpus, would be best suited to the evaluation of paraphrase recognition techniques. Most recently, Cohn, Callison-Burch, and Lapata (2008) describe a different take on the creation of a monolingual parallel corpus containing 900 sentence pairs with paraphrase annotations that can be used for both development and evaluation of paraphrase systems. These paraphrase annotations take the form of alignments between the words and sequences of words in each sentence pair; these alignments are analogous to the word- and phrasal-alignments induced in SMT systems that were illustrated in Section 3.5. As is the case with SMT alignments, the paraphrase annotations can be of different forms: one-word-to-one-word, one-word-to-many-words, as well as fully phrasal alignments.15 The authors start from a sentence-aligned paraphrase corpus compiled from three corpora that we have already described elsewhere in this survey: (1) the sentence pairs judged equivalent from the MSRP Corpus: (2) the Multiple Translation Chinese (MTC) corpus of multiple human-written translations of Chinese news stories used by Pang, Knight, and Marcu (2003); and (3) two English translations of the French novel Twenty Thousand Leagues Under the Sea, a subset of the monolingual parallel corpus used by Barzilay and McKeown (2001). The words in each sentence pair from this corpus are then aligned automatically to produce the initial paraphrase annotations that are then refined by two human annotators. The annotation guidelines required that the annotators judge which parts of a given sentence pair were in correspondence and to indicate this by creating an alignment between those parts (or correcting already existing alignments, if present). Two parts were said to correspond if they could be substituted for each other within the specific context provided by the respective sentence pair. In addition, the annotators were instructed to classify the created alignments as either sure (the two parts are clearly substitutable) or possible (the two parts are slightly divergent either in terms of syntax or semantics). For example, given the following paraphrastic sentence pair: the phrase pair (the convention, the meeting) will be aligned as a sure correspondence whereas the phrase pair (was of profound significance, could have very long-term effects) will be aligned as a possible correspondence. Other examples of possible correspondences could include the same stem expressed as different parts-of-speech (such as (significance, significantly)) or two non-synonymous verbs (such as (this is also, this also marks)). For more details on the alignment guidelines that were provided to the annotators, we refer the reader to (Callison-Burch, Cohn, and Lapata 2006). Extensive experiments are conducted to measure inter-annotator agreements and obtain good agreement values but they are still low enough to confirm that it is difficult for humans to recognize paraphrases even when the task is formulated differently. Overall, such a paraphrase corpus with detailed paraphrase annotations is much more informative than a corpus containing binary judgments at the sentence level such as the MSRP corpus. As an example, because the corpus contains paraphrase annotations at the word as well as phrasal levels, it can be used to build systems that can learn from these annotations and generate not only fully lexicalized phrasal paraphrases but also syntactically motivated paraphrastic patterns. To demonstrate the viability of the corpus for this purpose, a grammar induction algorithm (Cohn and Lapata 2007) is applied—originally developed for sentence compression—to the parsed version of their paraphrase corpus and the authors show that they can learn paraphrastic patterns such as those shown in Figure 9. In general, building paraphrase corpora, whether it is done at the sentence level or at the sub-sentential level, is extremely useful for the fostering of further research and development in the area of paraphrase generation. Whereas other language processing tasks such as machine translation and document summarization usually have multiple annual community-wide evaluations using An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008). standard test sets and manual as well as automated metrics, the task of automated paraphrasing does not. An obvious reason for this disparity could be that paraphrasing is not an application in and of itself. However, the existence of similar evaluations for other tasks that are not applications, such as dependency parsing (Buchholz and Marsi 2006; Nivre et al. 2007) and word sense disambiguation (Senseval), suggests otherwise. We believe that the primary reason is that, over the years, paraphrasing has been employed in an extremely fragmented fashion. Paraphrase extraction and generation are used in different forms and with different names in the context of different applications (for example: synonymous collocation extraction, query expansion). This usage pattern does not allow researchers in one community to share the lessons learned with those from other communities. In fact, it may even lead to research being duplicated across communities. However, more recent work—some of it discussed in this survey—on extracting phrasal paraphrases (or patterns) does include direct evaluation of the paraphrasing itself: The original phrase and its paraphrase are presented to multiple human judges, along with the contexts in which the phrase occurs in the original sentence, who are asked to determine whether the relationship between the two phrases is indeed paraphrastic (Barzilay and McKeown 2001; Barzilay and Lee 2003; Ibrahim, Katz, and Lin 2003; Pang, Knight, and Marcu 2003). A more direct approach is to substitute the paraphrase in place of the original phrase in its sentence and present both sentences to judges who are then asked to judge not only their semantic equivalence but also the grammaticality of the new sentence (Bannard and Callison-Burch 2005; CallisonBurch 2008). Motivation for such substitution-based evaluation is discussed in CallisonBurch (2007): the basic idea being that items deemed to be paraphrases may behave as such only in some contexts and not others. Szpektor, Shnarch, and Dagan (2007) posit a similar form of evaluation for textual entailment wherein the human judges are not only presented with the entailment rule but also with a sample of sentences that match its left-hand side (called instances), and then asked to assess whether the rule holds under each specific instance. Sentential paraphrases may be evaluated in a similar fashion without the need for any surrounding context (Quirk, Brockett, and Dolan 2004). An intrinsic evaluation of this form must employ the usual methods for avoiding any bias and for maximizing inter-judge agreement. In addition, we believe that, given the difficulty of this task even for human annotators, adherence to strict semantic equivalence may not always be a suitable guideline and intrinsic evaluations must be very carefully designed. A number of these approaches also perform extrinsic evaluations, in addition to the intrinsic one, by utilizing the extracted or generated paraphrases to improve other applications such as machine translation (Callison-Burch, Koehn, and Osborne 2006) and others as described in Section 1. Another option when evaluating the quality of a paraphrase generation method is that of using automatic measures. The traditional automatic evaluation measures of precision and recall are not particularly suited to this task because, in order to use them, a list of reference paraphrases has to be constructed against which these measures may be computed. Given that it is extremely unlikely that any such list will be exhaustive, any precision and recall measurements will not be accurate. Therefore, other alternatives are needed. Since the evaluation of paraphrases is essentially the task of measuring semantic similarity or of paraphrase recognition, all of those metrics, including the ones discussed in Section 2, can be employed here. Most recently, Callison-Burch, Cohn, and Lapata (2008) discuss ParaMetric, another automatic measure that may be used to evaluate paraphrase extraction methods. This work follows directly from the work done by the authors to create the paraphraseannotated corpus described in the previous section. Recall that this corpus contains paraphrastic sentence pairs with annotations in the form of alignments between their respective words and phrases. It is posited that to evaluate any paraphrase generation method, one could simply have it produce its own set of alignments for the sentence pairs in the corpus and precision and recall could then be computed over alignments instead of phrase pairs. These alignment-oriented precision (Palign) and recall (Ralign) measures are computed as follows: where (s1,s2) denotes a sentence pair, NM(s1,s2) denotes the phrases extracted via the manual alignments for the pair (s1, s2), and NP(s1,s2) denotes the phrases extracted via the automatic alignments induced using the paraphrase method P that is to be evaluated. The phrase extraction heuristic used to compute NP and NM from the respective alignments is the same as that employed by Bannard and Callison-Burch (2005) and illustrated in Figure 8. Although using alignments as the basis for computing precision and recall is a clever trick, it does require that the paraphrase generation method be capable of producing alignments between sentence pairs. For example, the methods proposed by Pang, Knight, and Marcu (2003) and Quirk, Brockett, and Dolan (2004) for generating sentential paraphrases from monolingual parallel corpora and described in Section 3.3 do produce alignments as part of their respective algorithms. Indeed, Callison-Burch et al. provide a comparison of their pivot-based approach—operating on bilingual parallel corpora—with the two monolingual approaches just mentioned in terms of ParaMetric, since all three methods are capable of producing alignments. However, for other approaches that do not necessarily operate at the level of sentences and cannot produce any alignments, falling back on estimates of traditional formulations of precision and recall is suggested. There has also been some preliminary progress toward using standardized test sets for intrinsic evaluations. A test set containing 20 AFP articles (484 sentences) about violence in the Middle East that was used for evaluating the lattice-based paraphrase technique in (Barzilay and Lee 2003) has been made freely available.16 In addition to the original sentences for which the paraphrases were generated, the set also contains the paraphrases themselves and the judgments assigned by human judges to these paraphrases. The paraphrase-annotated corpus discussed in the previous section would also fall under this category of resources. As with many other fields in NLP, paraphrase generation also lacks serious extrinsic evaluation (Belz 2009). As described herein, many paraphrase generation techniques are developed in the context of a host NLP application and this application usually serves as one form of extrinsic evaluation for the quality of the paraphrases generated by that technique. However, as yet there is no widely agreed-upon method of extrinsically evaluating paraphrase generation. Addressing this deficiency should be a crucial consideration for any future community-wide evaluation effort. An important dimension for any area of research is the availability of fora where members of the community may share their ideas with their colleagues and receive valuable feedback. In recent years, a number of such fora have been made available to the automatic paraphrasing community (Inui and Hermjakob 2003; Tanaka et al. 2004; Dras and Yamamoto 2005; Sekine et al. 2007), which represents an extremely important step toward countering the fragmented usage pattern described previously. It is important for any survey to provide a look to the future of the surveyed task and general trends for the corresponding research methods. We identify several such trends in the area of paraphrase generation that are gathering momentum. The Influence of the Web. The Web is rapidly becoming one of the most important sources of data for natural language processing applications, which should not be surprising given its phenomenal rate of growth. The (relatively) freely available Web data, massive in scale, has already had a definite influence over data-intensive techniques such as those employed for paraphrase generation (Pas¸ca and Dienes 2005). However, the availability of such massive amounts of Web data comes with serious concerns for efficiency and has led to the development of efficient methods that can cope with such large amounts of data. Bhagat and Ravichandran (2008) extract phrasal paraphrases by measuring distributional similarity over a 150GB monolingual corpus (25 billion words) via locality sensitive hashing, a randomized algorithm that involves the creation of fingerprints for vectors in space (Broder 1997). Because vectors that are more similar are more likely to have similar fingerprints, vectors (or distributions) can simply be compared by comparing their fingerprints, leading to a more efficient distributional similarity algorithm (Charikar 2002; Ravichandran, Pantel, and Hovy 2005). We also believe that the influence of the Web will extend to other avenues of paraphrase generation such as the aforementioned extrinsic evaluation or lack thereof. For example, Fujita and Sato (2008b) propose evaluating phrasal paraphrase pairs, automatically generated from a monolingual corpus, by querying the Web for snippets related to the pairs and using them as features to compute the pair’s paraphrasability. Combining Multiple Sources of Information. Another important trend in paraphrase generation is that of leveraging multiple sources of information to determine whether two units are paraphrastic. For example, Zhao et al. (2008) improve the sentential paraphrases that can be generated via the pivot method by leveraging five other sources in addition to the bilingual parallel corpus itself: (1) a corpus of Web queries similar to the phrase, (2) definitions from the Encarta dictionary, (3) a monolingual parallel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and then combined in a log-linear paraphrasing-as-translation model proposed by Madnani et al. (2007). A manual inspection reveals that using multiple sources of information yields paraphrases with much higher accuracy. We believe that such exploitation of multiple types of resources and their combinations is an important development. Zhao et al. (2009) further increase the utility of this combination approach by incorporating application specific constraints on the pivoted paraphrases. For example, if the output paraphrases need to be simplified versions of the input sentences, then only those phrasal paraphrase pairs are used where the output is shorter than the input. Use of SMT Machinery. In theory, statistical machine translation is very closely related to paraphrase generation since the former also relies on finding semantic equivalence, albeit in a second language. Hence, there have been numerous paraphrasing approaches that have relied on different components of an SMT pipeline (word alignment, phrase extraction, decoding/search) as we saw in the preceding pages of this survey. Despite the obvious convenience of using SMT components for the purpose of monolingual translation, we must consider that doing so usually requires additional work to deal with the added noise due to the nature of such components. We believe that SMT research will continue to influence research in paraphrasing; both by providing readyto-use building blocks and by necessitating development of methods to effectively use such components for the unintended task of paraphrase generation. Domain-Specific Paraphrasing. Recently, work has been done to generate phrasal paraphrases in specialized domains. For example, in the field of health literacy, it is well known that documents for health consumers are not very well-targeted to their purported audience. Recent research has shown how to generate a lexicon of semantically equivalent phrasal (and lexical) pairs of technical and lay medical terms from monolingual parallel corpora (Elhadad and Sutaria 2007) as well as monolingual comparable corpora (Del´eger and Zweigenbaum 2009). Examples include pairs such as (myocardial infarction, heart attack) and (leucospermia, increased white cells in the sperm). In another domain, Max (2008) proposes an adaptation of the pivot-based method to generate rephrasings of short text spans that could help a writer revise a text. Because the goal is to assist a writer in making revisions, the rephrasings do not always need to bear a perfect paraphrastic relationship to the original, a scenario suited for the pivot-based method. Several variants of such adaptations are developed that generate candidate rephrasings driven by fluency, semantic equivalence, and authoring value, respectively. We also believe that a large-scale annual community-wide evaluation should become a trend since it is required to foster further research in, and use of, paraphrase extraction and generation. Although there have been recent workshops and tasks on paraphrasing and entailment as discussed in Section 5, this evaluation would be much more focused, providing sets of shared guidelines and resources, in the spirit of the recent NIST MT Evaluation Workshops (NIST 2009). Over the last two decades, there has been much research on paraphrase extraction and generation within a number of research communities in natural language processing, in order to improve the specific application with which that community is concerned. However, a large portion of this research can be easily adapted for more widespread use outside its particular host and can provide significant benefits to the whole field. Only recently have there been serious efforts to conduct research on the topic of paraphrasing by treating it as an important natural language processing task independent of a host application. In this article, we have presented a comprehensive survey of the task of paraphrase extraction and generation motivated by the fact that paraphrases can help in a multitude of applications such as machine translation, text summarization, and information extraction. The aim was to provide an application-independent overview of paraphrase generation, while also conveying an appreciation for the importance and potential use of paraphrasing in the field of NLP research. We show that there are a large variety of paraphrase generation methods and each such method has a very different set of characteristics, in terms of both its performance and its ease of deployment. We also observe that whereas most of the methods in this survey can be used in multiple applications, the choice of the most appropriate method depends on how well the characteristics of the produced paraphrases match the requirements of the downstream application in which the paraphrases are being utilized.
Tree-To-String Alignment Template For Statistical Machine Translation We present a novel translation model on alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is to be translated into a target string eI1 = el, ... , ei, ... , eI. Here, I is the length of the target string, and J is the length of the source string. substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of operations that transform a target parse tree into a source string. Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus. In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels. In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models. Similarly to (Galley et al., 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules. The major difference is that we model the syntax of the source language instead of the target side. As a result, the task of our decoder is to find the best target string while Galley’s is to seek the most likely target tree. A tree-to-string alignment template z is a triple T, 5, A), which describes the alignment A between a source parse tree T = T(FJ' 1 ) 2 and a target string 5 = E' is also composed of both terminals (target words) and non-terminals (placeholders). An alignment A is defined as a subset of the Cartesian product of source and target symbol positions: 2We use T(·) to denote a parse tree. To reduce notational overhead, we use T(z) to represent the parse tree in z. Similarly, S(z) denotes the string in z. In the following, we formally describe how to introduce tree-to-string alignment templates into probabilistic dependencies to model Pr(ei|fJ1 ) 3. In a first step, we introduce the hidden variable T(fJ1 ) that denotes a parse tree of the source senNext, another hidden variable D is introduced to detach the source parse tree T (fJ1 ) into a sequence of K subtrees T 1K with a preorder transversal. We assume that each subtree Tk produces a target string 5k. As a result, the sequence of subtrees T1K produces a sequence of target strings 5K1 , which can be combined serially to generate the target sentence ei. We assume that Pr(e,|D,T(fJ1 ),fJ1) = Pr(5K1|T1K) because ei is actually generated by the derivation of 5K1 . Note that we omit an explicit dependence on the detachment D to avoid notational overhead. 3The notational convention will be as follows. We use the symbol Pr(·) to denote general probability distribution with no specific assumptions. In contrast, for model-based probability distributions, we use generic symbol p(·). To further decompose Pr(˜S |T˜), the tree-tostring alignment template, denoted by the variable z, is introduced as a hidden variable. ⇒ X3 X4 of China ⇒ economic X4 of China ⇒ economic development of China Following Och and Ney (2002), we base our model on log-linear framework. Hence, all knowledge sources are described as feature functions that include the given source string fJ1 , the target string eI1, and hidden variables. The hidden variable T(fJ1 ) is omitted because we usually make use of only single best output of a parser. As we assume that all detachment have the same probability, the hidden variable D is also omitted. As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified. For our experiments we use the following seven feature functions 4 that are analogous to default feature set of Pharaoh (Koehn, 2004). To simplify the notation, we omit the dependence on the hidden variables of the model. Therefore, the TAT-based translation model can be decomposed into four sub-models: Figure 2 shows how TATs work to perform translation. First, the input source sentence is parsed. Next, the parse tree is detached into five subtrees with a preorder transversal. For each subtree, a TAT is selected and applied to produce a string. Finally, these strings are combined serially to generate the translation (we use X to denote the non-terminal): 4When computing lexical weighting features (Koehn et al., 2003), we take only terminals into account. If there are no terminals, we set the feature value to 1. We use lex(·) to denote lexical weighting. We denote the number of TATs used for decoding by K and the length of target string by L To extract tree-to-string alignment templates from a word-aligned, source side parsed sentence pair hT (fJ1 ), eI1, Ai, we need first identify TSAs (TreeString-Alignment) using similar criterion as suggested in (Och and Ney, 2004). A TSA is a triple Usually, we can extract a very large amount of TATs from training data using the above rules, making both training and decoding very slow. Therefore, we impose three restrictions to reduce the magnitude of extracted TATs: This constraint requires that both the first and last symbols in the target string must be aligned to some source symbols. Table 1 shows the TATs extracted from the TSA in Figure 3 with h = 2 and c = 2. As we restrict that T(fj2 j1 ) must be a subtree of T(fJ1 ), TATs may be treated as syntactic hierarchical phrase pairs (Chiang, 2005) with tree structure on the source side. At the same time, we face the risk of losing some useful non-syntactic phrase pairs. For example, the phrase pair +{' AOL ALA H President Bush made can never be obtained in form of TAT from the TSA in Figure 3 because there is no subtree for that source string. We approach the decoding problem as a bottom-up beam search. To translate a source sentence, we employ a parser to produce a parse tree. Moving bottomup through the source parse tree, we compute a list of candidate translations for the input subtree rooted at each node with a postorder transversal. Candidate translations of subtrees are placed in stacks. Figure 4 shows the organization of candidate translation stacks. A candidate translation contains the following information: A TAT z is usable to a parse tree T if and only if T(z) is rooted at the root of T and covers part of nodes of T. Given a parse tree T, we find all usable TATs. Given a usable TAT z, if T(z) is equal to T, then 5(z) is a candidate translation of T. If T(z) covers only a portion of T, we have to compute a list of candidate translations for T by replacing the non-terminals of 5(z) with candidate translations of the corresponding uncovered subtrees. For example, when computing the candidate translations for the tree rooted at node 8, the TAT used in Figure 5 covers only a portion of the parse tree in Figure 4. There are two uncovered subtrees that are rooted at node 2 and node 7 respectively. Hence, we replace the third symbol with the candidate translations in stack 2 and the first symbol with the candidate translations in stack 7. At the same time, the feature values and probabilities are also accumulated for the new candidate translations. To speed up the decoder, we limit the search space by reducing the number of TATs used for each input node. There are two ways to limit the TAT table size: by a fixed limit (tatTable-limit) of how many TATs are retrieved for each input node, and by a probability threshold (tatTable-threshold) that specify that the TAT probability has to be above some value. On the other hand, instead of keeping the full list of candidates for a given node, we keep a top-scoring subset of the candidates. This can also be done by a fixed limit (stack-limit) or a threshold (stack-threshold). To perform recombination, we combine candidate translations that share the same leading and trailing bigrams in each stack. Our experiments were on Chinese-to-English translation. The training corpus consists of 31,149 sentence pairs with 843,256 Chinese words and 949, 583 English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy. Given the word-aligned bilingual corpus, we obtained 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting. To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005). We used default pruning settings for Pharaoh except that we set the distortion limit to 4. On the same word-aligned training data, it took us about one month to parse all the 31,149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al., 2005). The parser was trained on articles 1 − 270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate. Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350,575 TATs (88,066 used on test corpus). To run our decoder Lynx on development and test corpus, we set tatTable-limit = 20, tatTable-threshold = 0, stack-limit = 100, and stack-threshold = 0.00001. Table 2 shows the results on test set using Pharaoh and Lynx with different feature settings. The 95% confidence intervals were computed using Zhang’s significance tester (Zhang et al., 2004). We modified it to conform to NIST’s current definition of the BLEU brevity penalty. For Pharaoh, eight features were used: distortion model d, a trigram language model lm, phrase translation probabilities O(f|e) and O(e|f), lexical weightings lex(f|e) and lex(e|f), phrase penalty pp, and word penalty wp. For Lynx, seven features described in section 2 were used. We find that Lynx outperforms Pharaoh with all feature settings. With full features, Lynx achieves an absolute improvement of 0.006 over Pharaoh (3.1% relative). This difference is statistically significant (p < 0.01). Note that Lynx made use of only 88,066 TATs on test corpus while 221, 453 bilingual phrases were used for Pharaoh. The feature weights obtained by minimum error rate training for both Pharaoh and Lynx are shown in Table 3. We find that φ(f|e) (i.e. h2) is not a helpful feature for Lynx. The reason is that we use only a single non-terminal symbol instead of assigning phrasal categories to the target string. In addition, we allow the target string consists of only non-terminals, making translation decisions not always based on lexical evidence. It is interesting to use bilingual phrases to strengthen the TAT-based model. As we mentioned before, some useful non-syntactic phrase pairs can never be obtained in form of TAT because we restrict that there must be a corresponding parse tree for the source phrase. Moreover, it takes more time to obtain TATs than bilingual phrases on the same training data because parsing is usually very time-consuming. Given an input subtree T (Fj j ), if Fj j is a string of terminals, we find all bilingual phrases that the source phrase is equal to Fj j . Then we build a TAT for each bilingual phrase (fJ, 1 , A): the tree of the TAT is T (Fj j ), the string is eI, 1 , and the alignment is A. If a TAT built from a bilingual phrase is the same with a TAT in the TAT table, we prefer to the greater translation probabilities. Table 4 shows the effect of using bilingual phrases for Lynx. Note that these bilingual phrases are the same with those used for Pharaoh. We also conducted an experiment on large data to further examine our design philosophy. The training corpus contains 2.6 million sentence pairs. We used all the data to extract bilingual phrases and a portion of 800K pairs to obtain TATs. Two trigram language models were used for Lynx. One was trained on the 2.6 million English sentences and another was trained on the first 1/3 of the Xinhua portion of Gigaword corpus. We also included rule-based translations of named entities, dates, and numbers. By making use of these data, Lynx achieves a BLEU score of 0.2830 on the 2005 NIST Chinese-to-English MT evaluation test set, which is a very promising result for linguistically syntax-based models. In this paper, we introduce tree-to-string alignment templates, which can be automatically learned from syntactically-annotated training data. The TAT-based translation model improves translation quality significantly compared with a stateof-the-art phrase-based decoder. Treated as special TATs without tree on the source side, bilingual phrases can be utilized for the TAT-based model to get further improvement. It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT. Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements. We feel that both parsing and word alignment qualities have important effects on the TATbased model. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al., 2005). This work is supported by National High Technology Research and Development Program contract “Generally Technical Research and Basic Database Establishment of Chinese Platform”(Subject No. 2004AA114010). We are grateful to Deyi Xiong for providing the parser and Haitao Mi for making the parser more efficient and robust. Thanks to Dr. Yajuan Lv for many helpful comments on an earlier draft of this paper.
"?ff?yb ?r?.>??nB?=o?>?D???EG?$??En?hDF=?B??G? MC?QM?XZY?vLW?? ?RUW?OQW@] _LT;?G] [? ?Z?????h?S?fi?`????? ?;?fi???L?*?@?`? ?h?`?fi???L???`?h?F?`?fi?fi? ?F?`?.???!???fi? ???.?S?????S????? ?`???fi?h?y???n????? ?????`?????.?S? ?`?fi?h?fi?U?.?S?fi???`? ?S?`?{???f???$???$?`? ?.?`?fi?fi?`?????;?fi? ?L?h?r???h?S?fi? ???fi???`?.????? ?;?G???h???fi?h? ?{?h????????;?fi? ?????F?`?fi?fi? ?;?`?????;?{???`? ?`???.?L?????J? ?L?;?fi?h?S????h?h???F? ?;?fi?!???h?L???+??? ?{?F?F?8???S?`? ?Z?G?.?`?fi?h?F? ?$?`???F?`?fi?fi? ?`?!?;?{?{?????S?;?fi???L? ?`??6?;?fi???h? ???x?fi?h?6?{?`?fi?????G?fi??? ?.?`?fi?fi???L? ?;?`?Z?fi?;?`???`? ?1???`?????F?n?1?{?fi? ?!???h???*???F? ?fi???ff?S?fi???{???G???h? ?????`?{?fi?`???h? ?????fi?h???;?`?L? ff ???`?`?????`???`???$?F?n?-?S?? ?fi?;?{?????fi?h? ???`?????F?L????? ?;?fi?h?fi???`? ???????F?G?~???`?fi? > ???`?{???`????? ?;?fi?????fi?;?fi?????fi???S?`????? ?h?S?fi?fe ???$?fi?h?r?{? ?;?fi?h?fi?q?`? ?S?`?S?fi?r?.?L?fi???fi???`? ???`?S?F?fi?L?fi???`???f?S?`? ???fi?*?????fi? ?.?S???`?y?F?n? ?ff?????S???`?fi? ?`?+?;?.???`?`? ???j?+?h?L?h?8?????h? ?h?L?h?8?????h? ???fi???h?ff??? ?q?S?fi?fi?`?????L?fi? ?????h???n?f?@?`? ?`?fi?`???fi?h? ???x?fi?h?S?fi?S?@?`?fi??? ?{???fi?q?@?fi? ???`?+?F?`?fi?fi? ?`?fi?h?S?fi?!?????`?1???!?? ?????F?`?fi?{?{?f?????h?S?;? ?;?fi???`?????`?;?8?????`? ?????f?fi?h?U?F?`??? ?;?e?$?)?;????? ?S???h???F?`?fi?fi? ???L???fi?h?fi? ?.?L?fi???fi???`? ?S?h?fi?`?ff????? ???;?fi???L???Z?G?.?`?fi?h? ?&?ff???fi?h?fi? ?h?`?fi?6?`?1?fi?h?q?j?S?`? ???!?@?`?!?L?????fi???h???{?h??? ?6???????$???$?L???`? ???C?`?S?+?8?ff? ?6?S?;?{?fi?h?fi? ?`???`?`????fi? ???;?fi?`?????S?x?`? ???U?????.?`?fi?h? ???????L?h??F?F?8??? ???ff?S?fi???{?`??? ?`???.?`?fi?h?S??S? ?fi?;?fi?`?S???Z?G?.?`?fi?h?F? ???C?`?S?U?fi?h? ???;?fi???L?r?.?S?8?ff?S? ???L???`???-?@? ???S???fi???h?L? ?;?.???`?`?????n?h? ?;?6?F?`?fi?fi? ?h???.?S?????S? ?%) ?1?;?{???ff? ?8_ZK O????X?[MCY?v?KNW?XL[fWNX?OQvLW?? ???h?h???L???`? ?????`?!?fi?h?q?F?`?fi?fi? ?fi?h?????fi?n? ff|$)#"%9?=  J7 ?????F? ?L?h???n?h??????? ???fi?h?r?~?`???F? ;: ???S???fi???h?h? ?L???!????????G???h? ?*?L???`??????? ?6?fi?{?;???h?S?.? ???fi?{?;???;?{?{???????x?fi?h???fi?;?fi?`?S? ?fi?fi?S?U?F?G? ?fi?!?`???fi?h?q?@? ?;?fi?h?fi???L?????fi?$?????j? 318 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 C R CA ROC curves for feature-group MLP models all base model base-dependent base-independent source target source and target 26: atal-ngram ?????L?h?fi? OQV?WN[?$WNX?M ? [ TLV??IZM?ILWNK?W?OJ?? u.YfiCVQV?[YfiOQXL[??Q?] MC?8[jW??~IZM??Q[&? ?*???L?h???{?h???`?j?F?`?fi?fi? ?.?L?fi???fi???L???????fi?h???fi?S?@?S?fi? ???n?8?`?????L?h? ?fi?h???fi?S?@?S??? ?`?1?n?S?S?h?fi?fi? ff #(:e?F?`?fi?fi? ff ?h?!?h?S?fi? ?f?;?fi???h?`?f?fi?h? ???h?S???;?fi???S? ?????`?r?S?`?S?fi???Z?G?.?`?fi?h? ?fi???Q???`?r?fi?h?????;?8?S?`????? ?;?fi?h?fi?????.? ?;?fi?h?fi???fi?{?;?*?h?S???S? ?9;?,9?25= #7,+ )?25?? 25 =?<)"%2O= ?????????`? ???L???h?S???S? ?F?S?fi?fi?`?????.?;? 0 > ^
???*?fi?;?{?? ?`???fi?h???fi? ???L?{?????fi?h?L??? ffr?????F?h?`?F?fi?????fi?h???fi? ?????;????8???@? ?;?fi?h?fi????? ?h?S?fi?`?????$???S? ?`?fi???fi?;?fi?`?S?????`?h?L?{?;?`???~?`??? ?.?S???`?q?n??? ?r?S???`?fi?fi???{?S? ?&?!???fi???fi?h?ff?fi? ?n?S?S?h?fi?fi? ?????`?Z?x?.?L?fi???fi???L?????x?fi?h? ?fi?;?fi?`?S?ff??? ?y?L???`?+?q?fi???;? ?Z?!???`?S???fi?h?r?{?`??? @#(: ?;?~???L???h??? ?????;???L???????ff? ?????fi?h?r?`?????L?h? ???L?h??F???.?L?fi???fi???L? ?fi?&?.?S?fi?@?`? ?fi?h???.?S?fi?@?`? ?`?{?F?6?`???`???x?`???fi?h? ???S???`?fi?fi???{?S? ???;?fi?`?S???{?`??? ?`?????`?S?fi???fi? ?L?h???.?S?fi?@?`? ???fi?h???!?`???`? 25 =?<)"%2O= ?1?;?{???/#??F? ?`???fi?S?.?`?fi??? ?S???`?fi?fi???@?n???h? ???6????????F?fi?h? ?fi?h???{?`?fi? ???????????fi?*?;?fi? ?;???S????`?.???ff?{?`??? ?fM?Y?vLWNXL[ KN[M?VQXLWNX ? ?Q[M?V?Y?v?WNX ?QT? ?%)Z] 5 vL[?M?K?W ?"
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization This paper presents the first round of the on Textual Entailment for organized within SemEval-2012. The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved. The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated. The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization. However, mainly due to the absence of cross-lingual textual entailment (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. The CLTE task aims at prompting research to fill this gap. Along such direction, research can now benefit from recent advances in other fields, especially machine translation (MT), and the availability of: i) large amounts of parallel and comparable corpora in many languages, ii) open source software to compute word-alignments from parallel corpora, and iii) open source software to set up MT systems. We believe that all these resources can positively contribute to develop inference mechanisms for multilingual data. Content synchronization represents a challenging application scenario to test the capabilities of advanced NLP systems. Given two documents about the same topic written in different languages (e.g. Wiki pages), the task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents. Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other. The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgments for two text fragments are respectively mapped into judgments about semantic equivalence and novelty. Alternatively, the task can be seen as a machine translation evaluation problem, where judgments about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other. The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task. In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as useful evidence to draw entailment decisions), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE. However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and large room for mutual improvement. Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments (see Figure 1 for Spanish/English examples of each judgment): In this task, both T1 and T2 are assumed to be true statements. Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task. Four CLTE corpora have been created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN). The datasets are released in the XML format shown in Figure 1. The dataset was created following the crowdsourcing methodology proposed in (Negri et al., 2011), which consists of the following steps: only the pairs where the difference between the number of words in T1 and T2 (length diff) was below a fixed threshold (10 words) were retained.1 The final result is a monolingual English dataset annotated with multi-directional entailment judgments, which are well distributed over length diff values ranging from 0 to 9; To ensure the good quality of the datasets, all the collected pairs were manually checked and corrected when necessary. Only pairs with agreement between two expert annotators were retained. The final result is a multilingual parallel entailment corpus, where T1s are in 5 different languages (i.e. English, Spanish, German, Italian, and French), and T2s are in English. It’s worth mentioning that the monolingual English corpus, a by-product of our data collection methodology, will be publicly released as a further contribution to the research community.2 Each dataset consists of 1,000 pairs (500 for training and 500 for test), balanced across the four entailment judgments (bidirectional, forward, backward, and no entailment). For each language combination, the distribution of the four entailment judgments according to length diff is shown in Figure 2. Vertical bars represent, for each length diff value, the proportion of pairs belonging to the four entailment classes. As can be seen, the length diff constraint applied to the length difference in the monolingual English pairs (step 3 of the creation process) is substantially reflected in the cross-lingual datasets for all language combinations. In fact, as shown in Table 1, the majority of the pairs is always included in the same length diff range (approximately [-5,+5]) and, within this range, the distribution of the four classes is substantially uniform. Our assumption is that such data distribution makes entailment judgments based on mere surface features such as sentence length ineffective, thus encouraging the development of alternative, deeper processing strategies. Evaluation results have been automatically computed by comparing the entailment judgments returned by each system with those manually assigned by human annotators. The metric used for systems’ ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set. Additionally, we calculated precision, recall, and F1 measures for each of the four entailment judgment categories taken separately. These scores aim at giving participants the possibility to gain clearer insights into their system’s behavior on the entailment phenomena relevant to the task. For each language combination, two baselines considering the length difference between T1 and T2 have been calculated (besides the trivial 0.25 accuracy score obtained by assigning each test pair in the balanced dataset to one of the four classes): judgments returned by the two classifiers are composed into a single multi-directional judgment (“YES-YES”=“bidirectional”, “YESNO”=“forward”, “NO-YES”=“backward”, “NO-NO”=“no entailment”); Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using a linear kernel with default parameters. Baseline results are reported in Table 2. Although the four CLTE datasets are derived from the same monolingual EN-EN corpus, baseline results present slight differences due to the effect of translation into different languages. Participants were allowed to submit up to five runs for each language combination. A total of 17 teams registered to participate in the task and downloaded the training set. Out of them, 12 downloaded the test set and 10 (including one of the task organizers) submitted valid runs. Eight teams produced submissions for all the language combinations, while two teams participated only in the SP-EN task. In total, 92 runs have been submitted and evaluated (29 for SP-EN, and 21 for each of the other language pairs). Despite the novelty and the difficulty of the problem, these numbers demonstrate the interest raised by the task, and the overall success of the initiative. Accuracy results are reported in Table 3. As can be seen from the table, overall accuracy scores are quite different across language pairs, with the highest result on SP-EN (0.632), which is considerably higher than the highest score on DE-EN (0.558). This might be due to the fact that most of the participating systems rely on a “pivoting” approach that addresses CLTE by automatically translating T1 in the same language of T2 (see Section 6). Regarding the DE-EN dataset, pivoting methods might be penalized by the lower quality of MT output when German T1s are translated into English. The comparison with baselines results leads to interesting observations. First of all, while all systems significantly outperform the lowest 1-class baseline (0.25), both other baselines are surprisingly hard to beat. This shows that, despite the effort in keeping the distribution of the entailment classes uniform across different length diff values, eliminating the correlation between sentences’ length and correct entailment decisions is difficult. As a consequence, although disregarding semantic aspects of the problem, features considering such information are quite effective. In general, systems performed better on the SPEN dataset, with most results above the binary baseline (8 out of 10), and half of the systems above the multi-class baseline. For the other language pairs the results are lower, with only 3 out of 8 participants above the two baselines in all datasets. Average results reflect this situation: the average scores are always above the binary baseline, whereas only the SP-EN average result is higher than the multiclass baseline(0.44 vs. 0.43). To better understand the behaviour of each system (also in relation to the different language combinations), Table 4 provides separate precision, recall, and F1 scores for each entailment judgment, calculated over the best runs of each participating team. Overall, the results suggest that the “bidirectional” and “no entailment” categories are more problematic than “forward” and “backward” judgments. For most datasets, in fact, systems’ performance on “bidirectional” and “no entailment” is significantly lower, typically on recall. Except for the DE-EN dataset (more problematic on “forward”), also average F1 results on these judgments are lower. This might be due to the fact that, for all datasets, the vast majority of “bidirectional” and “no entailment” judgments falls in a length diff range where the distribution of the four classes is more uniform (see Figure 2). Similar reasons can justify the fact that “backward” entailment results are consistently higher on all datasets. Compared with “forward” entailment, these judgments are in fact less scattered across the entire length diff range (i.e. less intermingled with the other classes). A rough classification of the approaches adopted by participants can be made along two orthogonal dimensions, namely: Concerning the former dimension, most of the systems (6 out of 10) adopted a pivoting approach, relying on Google Translate (4 systems), Microsoft Bing Translator (1), or a combination of Google, Bing, and other MT systems (1) to produce English T2s. Regarding the latter dimension, the compositional approach was preferred to multi-class classification (6 out of 10). The best performing system relies on a “hybrid” approach (combining monolingual and cross-lingual alignments) and a compositional strategy. Besides the frequent recourse to MT tools, other resources used by participants include: on-line dictionaries for the translation of single words, word alignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus. More in detail: BUAP [pivoting, compositional] (Vilari˜no et al., 2012) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (Google Translate3 and the OpenOffice Thesaurus4). Similarity measures (e.g. Jaccard index) and rules are respectively used to annotate the two resulting sentence pairs with entailment judgments and combine them in a single decision. CELI [cross lingual, compositional & multiclass] (Kouylekov, 2012) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. Word overlap and similarity measures are then used in different approaches to the task. In one run (Run 1), they are used to train a classifier that assigns separate entailment judgments for each direction. Such judgments are finally composed into a single one for each pair. In the other runs, the same features are used for multi-class classification. DirRelCond3 [cross lingual, compositional] (Perini, 2012) uses bilingual dictionaries (Freedict5 and WordReference6) to translate content words into English. Then, entailment decisions are taken combining directional relatedness scores between words in both directions (Perini, 2011). FBK [cross lingual, compositional & multiclass] (Mehdad et al., 2012a) uses cross-lingual matching features extracted from lexical phrase tables, semantic phrase tables, and dependency relations (Mehdad et al., 2011; Mehdad et al., 2012b; Mehdad et al., 2012c). The features are used for multi-class and binary classification using SVMs. HDU [hybrid, compositional] (W¨aschle and Fendrich, 2012) uses a combination of binary classifiers for each entailment direction. The classifiers use both monolingual alignment features based on METEOR (Banerjee and Lavie, 2005) alignments (translations obtained from Google Translate), and cross-lingual alignment features based on GIZA++ (Och and Ney, 2000) (word alignments learned on Europarl). ICT [pivoting, compositional] (Meng et al., 2012) adopts a pivoting method (using Google Translate and an in-house hierarchical MT system), and the open source EDITS system (Kouylekov and Negri, 2010) to calculate similarity scores between monolingual English pairs. Separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid CLTE judgments. JU-CSE-NLP [pivoting, compositional] (Neogi et al., 2012) uses Microsoft Bing translator7 to produce monolingual English pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment decisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additional training material. SoftCard [pivoting, multi-class] (Jimenez et al., 2012) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisions using information about overlapping sub-segments as features. Despite the novelty of the problem and the difficulty to capture multi-directional entailment relations across languages, the first round of the Crosslingual Textual Entailment for Content Synchronization task organized within SemEval-2012 was a successful experience. This year a new interesting challenge has been proposed, a benchmark for four language combinations has been released, baseline results have been proposed for comparison, and a monolingual English dataset has been produced as a by-product which can be useful for monolingual TE research. The interest shown by participants was encouraging: 10 teams submitted a total of 92 runs for all the language pairs proposed. Overall, the results achieved on all datasets are encouraging, with best systems significantly outperforming the proposed baselines. It is worth observing that the nature of the task, which lies between semantics and machine translation, led to the participation of teams coming from both these communities, showing interesting opportunities for integration and mutual improvement. The proposed approaches reflect this situation, with teams traditionally working on MT now dealing with entailment, and teams traditionally participating in the RTE challenges now dealing with cross-lingual alignment techniques. Our ambition, for the future editions of the CLTE task, is to further consolidate the bridge between the semantics and MT communities. This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853). The authors would also like to acknowledge Giovanni Moretti from CELCT for evaluation scripts and technical assistance, and the volunteer translators that contributed to the creation of the dataset:
Pronunciation Modeling For Improved Spelling Correction This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction. The proposed method builds an explicit error model for word pronunciations. By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction. Spelling errors are generally grouped into two classes (Kuckich, 1992) — typographic and cognitive. Cognitive errors occur when the writer does not know how to spell a word. In these cases the misspelling often has the same pronunciation as the correct word ( for example writing latex as latecks). Typographic errors are mostly errors related to the keyboard; e.g., substitution or transposition of two letters because their keys are close on the keyboard. Damerau (1964) found that 80% of misspelled words that are non-word errors are the result of a single insertion, deletion, substitution or transposition of letters. Many of the early algorithms for spelling correction are based on the assumption that the correct word differs from the misspelling by exactly one of these operations (M. D. Kernigan and Gale, 1990; Church and Gale, 1991; Mayes and F. Damerau, 1991). By estimating probabilities or weights for the different edit operations and conditioning on the left and right context for insertions and deletions and allowing multiple edit operations, high spelling correction accuracy has been achieved. At ACL 2000, Brill and Moore (2000) introduced a new error model, allowing generic string-to-string edits. This model reduced the error rate of the best previous model by nearly 50%. It proved advantageous to model substitutions of up to 5-letter sequences (e.g ent being mistyped as ant, ph as f, al as le, etc.) This model deals with phonetic errors significantly better than previous models since it allows a much larger context size. However this model makes residual errors, many of which have to do with word pronunciation. For example, the following are triples of misspelling, correct word and (incorrect) guess that the Brill and Moore model made: edelvise edelweiss advise bouncie bouncy bounce latecks latex lacks In this work we take the approach of modeling phonetic errors explicitly by building a separate error model for phonetic errors. More specifically, we build two different error models using the Brill and Moore learning algorithm. One of them is a letter-based model which is exactly the Brill and Moore model trained on a similar dataset. The other is a phone-sequence-to-phone-sequence error model trained on the same data as the first model, but using the pronunciations of the correct words and the estimated pronunciations of the misspellings to learn phone-sequence-to-phone-sequence edits and estimate their probabilities. At classification time, Nbest list predictions of the two models are combined using a log linear model. A requirement for our model is the availability of a letter-to-phone model that can generate pronunciations for misspellings. We build a letter-to-phone model automatically from a dictionary. The rest of the paper is structured as follows: Section 2 describes the Brill and Moore model and briefly describes how we use it to build our error models. Section 3 presents our letter-to-phone model, which is the result of a series of improvements on a previously proposed N-gram letter-tophone model (Fisher, 1999). Section 4 describes the training and test phases of our algorithm in more detail and reports on experiments comparing the new model to the Brill and Moore model. Section 6 contains conclusions and ideas for future work. Many statistical spelling correction methods can be viewed as instances of the noisy channel model. The misspelling of a word is viewed as the result of corruption of the intended word as it passes through a noisy communications channel. The task of spelling correction is a task of finding, for a misspelling w, a correct word r 2 D, where D is a given dictionary and r is the most probable word to have been garbled into w. Equivalently, the problem is to find a word r for which is maximized. Since the denominator is constant, this is the same as maximizing P (r)P (wjr). In the terminology of noisy channel modeling, the distribution P(r) is referred to as the source model, and the distribution P (wjr) is the error or channel model. Typically, spelling correction models are not used for identifying misspelled words, only for proposing corrections for words that are not found in a dictionary. Notice, however, that the noisy channel model offers the possibility of correcting misspellings without a dictionary, as long as sufficient data is available to estimate the source model factors. For example, if r = Osama bin Laden and w = Ossama bin Laden, the model will predict that the correct spelling r is more likely than the incorrect spelling w, provided that where P (wjr)IP (wjw) would be approximately the odds of doubling the s in Osama. We do not pursue this, here, however. Brill and Moore (2000) present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. The model has a set of parameters P(a ! ,) for letter sequences of lengths up to 5. An extension they presented has refined parameters P(a ! ,jPSN) which also depend on the position of the substitution in the source word. According to this model, the misspelling is generated by the correct word as follows: First, a person picks a partition of the correct word and then types each partition independently, possibly making some errors. The probability for the generation of the misspelling will then be the product of the substitution probabilities for each of the parts in the partition. For example, if a person chooses to type the word bouncy and picks the partition boun cy, the probability that she mistypes this word as boun cie will be P(boun ! boun)P(cie ! cy). The probability P(wjr) is estimated as the maximum over all partitions of r of the probability that w is generated from r given that partition. We use this method to build an error model for letter strings and a separate error model for phone sequences. Two models are learned; one model LTR (standing for “letter”) has a set of substitution probabilities P(a ! ,) where a and , are character strings, and another model PH (for “phone”) has a set of substitution probabilities P(a ! ,) where a and , are phone sequences. We learn these two models on the same data set of misspellings and correct words. For LTR, we use the training data as is and run the Brill and Moore training algorithm over it to learn the parameters of LTR. For PH, we convert the misspelling/correctword pairs into pairs of pronunciations of the misspelling and the correct word, and run the Brill and Moore training algorithm over that. For PH, we need word pronunciations for the correct words and the misspellings. As the misspellings are certainly not in the dictionary we need a letterto-phone converter that generates possible pronunciations for them. The next section describes our letter-to-phone model. There has been a lot of research on machine learning methods for letter-to-phone conversion. High accuracy is achieved, for example, by using neural networks (Sejnowski and Rosenberg, 1987), decision trees (Jiang et al., 1997), and N-grams (Fisher, 1999). We use a modified version of the method proposed by Fisher, incorporating several extensions resulting in substantial gains in performance. In this section we first describe how we do alignment at the phone level, then describe Fisher’s model, and finally present our extensions and the resulting letterto-phone conversion accuracy. The machine learning algorithms for converting text to phones usually start off with training data in the form of a set of examples, consisting of letters in context and their corresponding phones (classifications). Pronunciation dictionaries are the major source of training data for these algorithms, but they do not contain information for correspondences between letters and phones directly; they have correspondences between sequences of letters and sequences of phones. A first step before running a machine learning algorithm on a dictionary is, therefore, alignment between individual letters and phones. The alignment algorithm is dependent on the phone set used. We experimented with two dictionaries, the NETtalk dataset and the Microsoft Speech dictionary. Statistics about them and how we split them into training and test sets are shown in Table 1. The NETtalk dataset contains information for phone level alignment and we used it to test our algorithm for automatic alignment. The Microsoft Speech dictionary is not aligned at the phone level but it is much bigger and is the dictionary we used for learning our final letter-to-phone model. The NETtalk dictionary has been designed so that each letter correspond to at most one phone, so a word is always longer, or of the same length as, its pronunciation. The alignment algorithm has to decide which of the letters correspond to phones and which ones correspond to nothing (i.e., are silent). For example, the entry in NETtalk (when we remove the empties, which contain information for phone level alignment) for the word able is ABLE e b L. The correct alignment is A/e B/b L/L E/–, where – denotes the empty phone. In the Microsoft Speech dictionary, on the other hand, each letter can naturally correspond to 0, 1, or 2 phones. For example, the entry in that dictionary for able is ABLE ey b ax l. The correct alignment is A/ey B/b L/ax&l E/–. If we also allowed two letters as a group to correspond to two phones as a group, the correct alignment might be A/ey B/b LE/ax&l, but that would make it harder for the machine learning algorithm. Our alignment algorithm is an implementation of hard EM (Viterbi training) that starts off with heuristically estimated initial parameters for P (phonesIletter) and, at each iteration, finds the most likely alignment for each word given the parameters and then re-estimates the parameters collecting counts from the obtained alignments. Here phones ranges over sequences of 0 (empty), 1, and 2 phones for the Microsoft Speech dictionary and 0 or 1 phones for NETtalk. The parameters P (phonesIletter) were initialized by amethod similar to the one proposed in (Daelemans and van den Bosch, 1996). Word frequencies were not taken into consideration here as the dictionary contains no frequency information. The method we started with was the N-gram model of Fisher (1999). From training data, it learns rules that predict the pronunciation of a letter based on m letters of left and n letters of right context. The rules are of the following form: Here Lm stands for a sequence of m letters to the left of T and Rn is a sequence of n letters to the right. The number of letters in the context to the left and right varies. We used from 0 to 4 letters on each side. For example, two rules learned for the letter B were: [AB.B.OT —� — 1.0] and [B —� b .96 — .04], meaning that in the first context the letter B is silent with probability 1.0, and in the second it is pronounced as b with probability .96 and is silent with probability .04. Training this model consists of collecting counts for the contexts that appear in the data with the selected window size to the left and right. We collected counts for all configurations Lm.T.Rn for m E {0, 1, 2, 3, 41, n E {0, 1, 2, 3, 41 that occurred in the data. The model is applied by choosing for each letter T the most probable translation as predicted by the most specific rule for the context of occurrence of the letter. For example, if we want to find how to pronounce the second b in abbot we would chose the empty phone because the first rule mentioned above is more specific than the second. We implemented five extensions to the initial model which together decreased the error rate of the letterto-phone model by around 20%. These are: for a word using a fourgram vowel sequence language model The performance figures reported by Fisher (1999) are significantly higher than our figures using the basic model, which is probably due to the cleaner data used in their experiments and the differences in phoneset size. The extensions we implemented are inspired largely by the work on letter-to-phone conversion using decision trees (Jiang et al., 1997). The last extension, rescoring based on vowel fourgams, has not been proposed previously. We tested the algorithms on the NETtalk and Microsoft Speech dictionaries, by splitting them into training and test sets in proportion 80%/20% training-set to test-set size. We trained the letter-to-phone models using the training splits and tested on the test splits. We are reporting accuracy figures only on the NETtalk dataset since this dataset has been used extensively in building letter-to-phone models, and because phone accuracy is hard to determine for the nonphonetically-aligned Microsoft Speech dictionary. For our spelling correction algorithm we use a letterto-phone model learned from the Microsoft Speech dictionary, however. The results for phone accuracy and word accuracy of the initial model and extensions are shown in Table 2. The phone accuracy is the percentage correct of all phones proposed (excluding the empties) and the word accuracy is the percentage of words for which pronunciations were guessed without any error. For our data we noticed that the most specific rule that matches is often not a sufficiently good predictor. By linearly interpolating the probabilities given by the five most specific matching rules we decreased the word error rate by 14.3%. The weights for the individual rules in the top five were set to be equal. It seems reasonable to combine the predictions from several rules especially because the choice of which rule is more specific of two is arbitrary when neither is a substring of the other. For example, of the two rules with contexts A.B. and .B.B, where the first has 0 right context and the second has 0 left letter context, one heuristic is to choose the latter as more specific since right context seems more valuable than left (Fisher, 1999). However this choice may not always be the best and it proves useful to combine predictions from several rules. In Table 2 the row labeled “Interpolation of contexts” refers to this extension of the basic model. Adding a symbol for interior of word produced a gain in accuracy. Prior to adding this feature, we had features for beginning and end of word. Explicitly modeling interior proved helpful and further decreased our error rate by 4.3%. The results after this improvement are shown in the third row of Table 2. After linearly combining the predictions from the top matching rules we have a probability distribution over phones for each letter. It has been shown that modeling the probability of sequences of phones can greatly reduce the error (Jiang et al., 1997). We learned a trigram phone sequence model and used it to re-score the N-best predictions from the basic model. We computed the score for a sequence of phones given a sequence of letters, as follows: Here the probabilities P (pijl1; l2 ::: ln) are the distributions over phones that we obtain for each letter from combination of the matching rules. The weight a for the phone sequence model was estimated from a held-out set by a linear search. This model further improved our performance and the results it achieves are in the fourth row of Table 2. The final improvement is adding a term from a vowel fourgram language model to equation 1 with a weight ,Q. The term is the log probability of the sequence of vowels in the word according to a fourgram model over vowel sequences learned from the data. The final accuracy we achieve is shown in the fifth row of the same table. As a comparison, the best accuracy achieved by Jiang et al. (1997) on NETalk using a similar proportion of training and test set sizes was 65:8%. Their system uses more sources of information, such as phones in the left context as features in the decision tree. They also achieve a large performance gain by combining multiple decision trees trained on separate portions of the training data. The accuracy of our letter-tophone model is comparable to state of the art systems. Further improvements in this component may lead to higher spelling correction accuracy. Our combined error model gives the probability PCMB(wjr) where w is the misspelling and r is a word in the dictionary. The spelling correction algorithm selects for a misspelling w the word r in the dictionary for which the product P (r)PCMB(wjr) is maximized. In our experiments we used a uniform source language model over the words in the dictionary. Therefore our spelling correction algorithm selects the word r that maximizes PCMB(wjr). Brill and Moore (2000) showed that adding a source language model increases the accuracy significantly. They also showed that the addition of a language model does not obviate the need for a good error model and that improvements in the error model lead to significant improvements in the full noisy channel model. We build two separate error models, LTR and PH (standing for “letter” model and “phone” model). The letter-based model estimates a probability distribution PLTR(wjr) over words, and the phone-based model estimates a distribution PPH(pron wjpron r) over pronunciations. Using the PH model and the letter-to-phone model, we derive a distribution PPHL(wjr) in a way to be made precise shortly. We combine the two models to estimate scores as follows: The r that maximizes this score will also maximize the probability PCMB(wjr). The probabilities PPHL(wjr) are computed as follows: This equation is approximated by the expression for PPHL shown in Figure 1 after several simplifying assumptions. The probabilities P(pron rjr) are taken to be equal for all possible pronunciations of r in the dictionary. Next we assume independence of the misspelling from the right word given the pronunciation of the right word i.e. P (wjr, pron r) = P(wjpron r). By inversion of the conditional probability this is equal to P(pron rjw) multiplied by P(w)IP(pron r). Since we do not model these marginal probabilities, we drop the latter factor. Next the probability P(pron rjw) is expressed as which is approximated by the maximum term in the sum. After the following decomposition: P(pron w, pron rjw) = P(pron wjw)xP(pron rjw,pron w) ti P(pron wjw)xP(pron rjpron w) where the second part represents a final independence assumption, we get the expression in Figure 1. The probabilities P(pron wjw) are given by the letter-to-phone model. In the following subsections, we first describe how we train and apply the individual error models, and then we show performance results for the combined model compared to the letterbased error model. The error model LTR was trained exactly as described originally by Brill and Moore (2000). Given a training set of pairs fw2, r2g the algorithm estimates a set of rewrite probabilities p(a ! ,Q) which are the basis for computing probabilities PLTR(wjr). The parameters of the PH model PPH(pron wjpron r) are obtained by training a phone-sequence-to-phone-sequence error model starting from the same training set of pairs fw2, r2g of misspelling and correct word as for the LTR model. We convert this set to a set of pronunciations of misspellings and pronunciations of correct words in the following way: For each training sample fw2, r2g we generate m training samples of corresponding pronunciations where m is the number of pronunciations of the correct word r2 in our dictionary. Each of those m samples is the most probable pronunciation of w2 according to our letter-to-phone model paired with one of the possible pronunciations of r2. Using this training set, we run the algorithm of Brill and Moore to estimate a set of substitution probabilities a ! , for sequences of phones to sequences of phones. The probability PPH(pron wjpron r) is then computed as a product of the substitution probabilities in the most probable alignment, as Brill and Moore did. We tested our system and compared it to the Brill and Moore model on a dataset of around 10, 000 pairs of misspellings and corresponding correct words, split into training and test sets. The exact data sizes are 7, 385 word pairs in the training set and 1, 812 word pairs in the test set. This set is slightly different from the dataset used in Brill and Moore’s experiments because we removed from the original dataset the pairs for which we did not have the correct word in the pronunciation dictionary. Both models LTR and PH were trained on the same training set. The interpolation weight that the combined model CMB uses is also set on the training set to maximize the classification accuracy. At test time we do not search through all possible words r in the dictionary to find the one maximizing SeoreCMB(wjr). Rather, we compute the combination score only for candidate words r that are in the top N according to the PLTR(wjr) or are in the top N according to PPH(pron wjpron r) for any of the pronunciations of r from the dictionary and any of the pronunciations for w that were proposed by the letter-to-phone model. The letter-to-phone model returned for each w the 3 most probable pronunciations only. Our performance was better when we considered the top 3 pronunciations of w rather than a single most likely hypothesis. That is probably due to the fact that the 3-best accuracy of the letter-to-phone model is significantly higher than its 1-best accuracy. Table 3 shows the spelling correction accuracy when using the model LTR, PH, or both in combination. The table shows N-best accuracy results. The N-best accuracy figures represent the percent test cases for which the correct word was in the top N words proposed by the model. We chose the context size of 3 for the LTR model as this context size maximized test set accuracy. Larger context sizes neither helped nor hurt accuracy. As we can see from the table, the phone-based model alone produces respectable accuracy results considering that it is only dealing with word pronunciations. The error reduction of the combined model compared to the letters-only model is substantial: for 1-Best, the error reduction is over 23%; for 2Best, 3-Best, and 4-Best it is even higher, reaching over 46% for 4-Best. As an example of the influence of pronunciation modeling, in Table 4 we list some misspellingcorrect word pairs where the LTR model made an incorrect guess and the combined model CMB guessed accurately. We have presented a method for using word pronunciation information to improve spelling correction accuracy. The proposed method substantially reduces the error rate of the previous best spelling correction model. A subject of future research is looking for a better way to combine the two error models or building a single model that can recognize whether there is a phonetic or typographic error. Another interesting task is exploring the potential of our model in different settings such as the Web, e-mail, or as a specialized model for non-native English speakers of particular origin.
Unsupervised Models For Named Entity Classification This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples. Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision. This paper discusses the use of unlabeled examples for the problem of named entity classification. The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location. For example, a good classifier would identify Mrs. Frank as a person, Steptoe & Johnson as a company, and Honduras as a location. The approach uses both spelling and contextual rules. A spelling rule might be a simple look-up for the string (e.g., a rule that Honduras is a location) or a rule that looks at words within a string (e.g., a rule that any string containing Mr. is a person). A contextual rule considers words surrounding the string in the sentence in which it appears (e.g., a rule that any proper name modified by an appositive whose head is president is a person). The task can be considered to be one component of the MUC (MUC-6, 1995) named entity task (the other task is that of segmentation, i.e., pulling possible people, places and locations from text before sending them to the classifier). Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97). At first glance, the problem seems quite complex: a large number of rules is needed to cover the domain, suggesting that a large number of labeled examples is required to train an accurate classifier. But we will show that the use of unlabeled data can drastically reduce the need for supervision. Given around 90,000 unlabeled examples, the methods described in this paper classify names with over 91% accuracy. The only supervision is in the form of 7 seed rules (namely, that New York, California and U.S. are locations; that any name containing Mr is a person; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations). The key to the methods we describe is redundancy in the unlabeled data. In many cases, inspection of either the spelling or context alone is sufficient to classify an example. For example, in .., says Mr. Cooper, a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person. Even if an example like this is not labeled, it can be interpreted as a &quot;hint&quot; that Mr and president imply the same category. The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label, and these hints turn out to be surprisingly useful when building a classifier. We present two algorithms. The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98). (Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance. Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function. (Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples. Our first algorithm is similar to Yarowsky's, but with some important modifications motivated by (Blum and Mitchell 98). The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion. The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98). The AdaBoost algorithm was developed for supervised learning. AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples. Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree. The algorithm builds two classifiers iteratively: each iteration involves minimization of a continuously differential function which bounds the number of examples on which the two classifiers disagree. There has been additional recent work on inducing lexicons or other knowledge sources from large corpora. (Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples. (Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g., &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns. (Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations). (Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories). The approach builds from an initial seed set for a category, and is quite similar to the decision list approach described in (Yarowsky 95). More recently, (Riloff and Jones 99) describe a method they term &quot;mutual bootstrapping&quot; for simultaneously constructing a lexicon and contextual extraction patterns. The method shares some characteristics of the decision list algorithm presented in this paper. (Riloff and Jones 99) was brought to our attention as we were preparing the final version of this paper. 971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN). For example, take ..., says Maury Cooper, a vice president at S.&P. In this case, Maury Cooper is extracted. It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president). 2. The NP is a complement to a preposition, which is the head of a PP. This PP modifies another NP, whose head is a singular noun. For example, ... fraud related to work on a federally funded sewage plant in Georgia In this case, Georgia is extracted: the NP containing it is a complement to the preposition in; the PP headed by in modifies the NP a federally funded sewage plant, whose head is the singular noun plant. In addition to the named-entity string (Maury Cooper or Georgia), a contextual predictor was also extracted. In the appositive case, the contextual predictor was the head of the modifying appositive (president in the Maury Cooper example); in the second case, the contextual predictor was the preposition together with the noun it modifies (plant_in in the Georgia example). From here on we will refer to the named-entity string itself as the spelling of the entity, and the contextual predicate as the context. Having found (spelling, context) pairs in the parsed data, a number of features are extracted. The features are used to represent each example for the learning algorithm. In principle a feature could be an arbitrary predicate of the (spelling, context) pair; for reasons that will become clear, features are limited to querying either the spelling or context alone. The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period. (e.g., N.Y. would contribute this feature, IBM would not). nonalpha=x Appears if the spelling contains any characters other than upper or lower case letters. In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g., for Thomas E. Petry nonalpha= . , for A. T.&T. nonalpha.. . . ). context=x The context for the entity. The The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95). Before describing the unsupervised case we first describe the supervised version of the algorithm: Input to the learning algorithm: n labeled examples of the form (xi, y„). y, is the label of the ith example (given that there are k possible labels, y, is a member of y = {1 ... 0). xi is a set of mi features {x,1, Xi2 . . . Xim, } associated with the ith example. Each xii is a member of X, where X is a set of possible features. Output of the learning algorithm: a function h:Xxy [0, 1] where h(x, y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present. Alternatively, h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x, y). The label for a test example with features x is then defined as In this paper we define h(x, y) as the following function of counts seen in training data: Count(x,y) is the number of times feature x is seen with label y in training data, Count(x) = EyEy Count(x, y). a is a smoothing parameter, and k is the number of possible labels. In this paper k = 3 (the three labels are person, organization, location), and we set a = 0.1. Equation 2 is an estimate of the conditional probability of the label given the feature, P(yjx). 2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)). The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods. It's not clear how to apply these methods in the unsupervised case, as they required cross-validation techniques: for this reason we use the simpler smoothing method shown here. input to the unsupervised algorithm is an initial, &quot;seed&quot; set of rules. In the named entity domain these rules were Each of these rules was given a strength of 0.9999. The following algorithm was then used to induce new rules: Let Count' (x) be the number of times feature x is seen with some known label in the training data. For each label (Per s on, organization and Location), take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin. (If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper. Thus at each iteration the method induces at most n x k rules, where k is the number of possible labels (k = 3 in the experiments in this paper). step 3. Otherwise, label the training data with the combined spelling/contextual decision list, then induce a final decision list from the labeled examples where all rules (regardless of strength) are added to the decision list. We can now compare this algorithm to that of (Yarowsky 95). The core of Yarowsky's algorithm is as follows: where h is defined by the formula in equation 2, with counts restricted to training data examples that have been labeled in step 2. Set the decision list to include all rules whose (smoothed) strength is above some threshold Pmin. There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features. Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm. To measure the contribution of each modification, a third, intermediate algorithm, Yarowsky-cautious was also tested. Yarowsky-cautious does not separate the spelling and contextual features, but does have a limit on the number of rules added at each stage. (Specifically, the limit n starts at 5 and increases by 5 at each iteration.) The first modification — cautiousness — is a relatively minor change. It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations. Taking only the highest frequency rules is much &quot;safer&quot;, as they tend to be very accurate. This intuition is born out by the experimental results. The second modification is more important, and is discussed in the next section. An important reason for separating the two types of features is that this opens up the possibility of theoretical analysis of the use of unlabeled examples. (Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example. In the named entity task, X1 might be the instance space for the spelling features, X2 might be the instance space for the contextual features. By this assumption, each element x E X can also be represented as (xi, x2) E X1 x X2. Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification. Now assume we have n pairs (xi,, x2,i) drawn from X1 X X2, where the first m pairs have labels whereas for i = m+ 1...n the pairs are unlabeled. In a fully supervised setting, the task is to learn a function f such that for all i = 1...m, f (xi,i, 12,i) = yz. In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples. The key point is that the second constraint can be remarkably powerful in reducing the complexity of the learning problem. (Blum and Mitchell 98) give an example that illustrates just how powerful the second constraint can be. Consider the case where IX]. I = 1X21 N and N is a &quot;medium&quot; sized number so that it is feasible to collect 0(N) unlabeled examples. Assume that the two classifiers are &quot;rote learners&quot;: that is, 1.1 and 12 are defined through look-up tables that list a label for each member of X1 or X2. The problem is a binary classification problem. The problem can be represented as a graph with 2N vertices corresponding to the members of X1 and X2. Each unlabeled pair (x1,i, x2,i) is represented as an edge between nodes corresponding to x1,i and X2,i in the graph. An edge indicates that the two features must have the same label. Given a sufficient number of randomly drawn unlabeled examples (i.e., edges), we will induce two completely connected components that together span the entire graph. Each vertex within a connected component must have the same label — in the binary classification case, we need a single labeled example to identify which component should get which label. (Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case. They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page). The method halves the error rate in comparison to a method using the labeled examples alone. Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem, the assumptions are quite limited. In particular, it may not be possible to learn functions fi (x f2(x2,t) for i = m + 1...n: either because there is some noise in the data, or because it is just not realistic to expect to learn perfect classifiers given the features used for representation. It may be more realistic to replace the second criteria with a softer one, for example (Blum and Mitchell 98) suggest the alternative Alternatively, if Ii and 12 are probabilistic learners, it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners. The question of what soft function to pick, and how to design' algorithms which optimize it, is an open question, but appears to be a promising way of looking at the problem. The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints. At each iteration the algorithm increases the number of rules, while maintaining a high level of agreement between the spelling and contextual decision lists. Inspection of the data shows that at n = 2500, the two classifiers both give labels on 44,281 (49.2%) of the unlabeled examples, and give the same label on 99.25% of these cases. So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree. In the next section we present an alternative approach that builds two classifiers while attempting to satisfy the above constraints as much as possible. The algorithm, called CoBoost, has the advantage of being more general than the decision-list learning alInput: (xi , yi), , (xim, ) ; x, E 2x, yi = +1 Initialize Di (i) = 1/m. Fort= 1,...,T: This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems. We first give a brief overview of boosting algorithms. We then discuss how we adapt and generalize a boosting algorithm, AdaBoost, to the problem of named entity classification. The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel. (We would like to note though that unlike previous boosting algorithms, the CoBoost algorithm presented here is not a boosting algorithm under Valiant's (Valiant 84) Probably Approximately Correct (PAC) model.) This section describes AdaBoost, which is the basis for the CoBoost algorithm. AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper. For a description of the application of AdaBoost to various NLP problems see the paper by Abney, Schapire, and Singer in this volume. The input to AdaBoost is a set of training examples ((xi , yi), , (x„.„ yrn)). Each xt E 2x is the set of features constituting the ith example. For the moment we will assume that there are only two possible labels: each y, is in { —1, +1}. AdaBoost is given access to a weak learning algorithm, which accepts as input the training examples, along with a distribution over the instances. The distribution specifies the relative weight, or importance, of each example — typically, the weak learner will attempt to minimize the weighted error on the training set, where the distribution specifies the weights. The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R), where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction, and numbers close to zero indicate low confidence. The weak hypothesis can abstain from predicting the label of an instance x by setting h(x) = 0. The final strong hypothesis, denoted 1(x), is then the sign of a weighted sum of the weak hypotheses, 1(x) = sign (Vii atht(x)), where the weights at are determined during the run of the algorithm, as we describe below. Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1. Note that Zt is a normalization constant that ensures the distribution Dt+i sums to 1; it is a function of the weak hypothesis ht and the weight for that hypothesis at chosen at the tth round. The normalization factor plays an important role in the AdaBoost algorithm. Schapire and Singer show that the training error is bounded above by Thus, in order to greedily minimize an upper bound on training error, on each iteration we should search for the weak hypothesis ht and the weight at that minimize Z. In our implementation, we make perhaps the simplest choice of weak hypothesis. Each ht is a function that predicts a label (+1 or —1) on examples containing a particular feature xt, while abstaining on other examples: The prediction of the strong hypothesis can then be written as We now briefly describe how to choose ht and at at each iteration. Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive. Zt can be written as follows Following the derivation of Schapire and Singer, providing that W+ > W_, Equ. (4) is minimized by setting Since a feature may be present in only a few examples, W_ can be in practice very small or even 0, leading to extreme confidence values. To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ. (5) and ht into Equ. (4) gives In order to minimize Zt, at each iteration the final algorithm should choose the weak hypothesis (i.e., a feature xt) which has values for W+ and W_ that minimize Equ. (6), with W+ > W_. We now describe the CoBoost algorithm for the named entity problem. Following the convention presented in earlier sections, we assume that each example is an instance pair of the from (xi ,i, x2,) where xj,, E 2x3 , j E 2}. In the namedentity problem each example is a (spelling,context) pair. The first m pairs have labels yi, whereas for i = m + 1, , n the pairs are unlabeled. We make the assumption that for each example, both xi,. and x2,2 alone are sufficient to determine the label yi. The learning task is to find two classifiers : 2x1 { —1, +1} 12 : 2x2 { —1, +1} such that (x1,) = f2(x2,t) = yt for examples i = 1, , m, and f1 (x1,) = f2 (x2,t) as often as possible on examples i = m + 1, ,n. To achieve this goal we extend the auxiliary function that bounds the training error (see Equ. (3)) to be defined over unlabeled as well as labeled instances. Denote by g3(x) = Et crithl(x) , j E {1,2} the unthresholded strong-hypothesis (i.e., f3 (x) = sign(gi (x))). We define the following function: If Zco is small, then it follows that the two classifiers must have a low error rate on the labeled examples, and that they also must give the same label on a large number of unlabeled instances. To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ. (3)), with one term for each classifier. The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples. Put another way, the minimum of Equ. (7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples. Formally, let el (62) be the number of classification errors of the first (second) learner on the training data, and let Eco be the number of unlabeled examples on which the two classifiers disagree. Then, it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco. The algorithm builds two classifiers in parallel from labeled and unlabeled data. As in boosting, the algorithm works in rounds. Each round is composed of two stages; each stage updates one of the classifiers while keeping the other classifier fixed. Denote the unthresholded classifiers after t — 1 rounds by git—1 and assume that it is the turn for the first classifier to be updated while the second one is kept fixed. We first define &quot;pseudo-labels&quot;,-yt, as follows: = Yi t sign(g 0\ 2— kx2,m < i < n Thus the first m labels are simply copied from the labeled examples, while the remaining (n — m) examples are taken as the current output of the second classifier. We can now add a new weak hypothesis 14 based on a feature in X1 with a confidence value al hl and atl are chosen to minimize the function We now define, for 1 <i <n, the following virtual distribution, As before, Ztl is a normalization constant. Equ. (8) can now be rewritten5 as which is of the same form as the function Zt used in AdaBoost. Using the virtual distribution Di (i) and pseudo-labels&quot;y.,„ values for Wo, W± and W_ can be calculated for each possible weak hypothesis (i.e., for each feature x E Xi); the weak hypothesis with minimal value for Wo + 2/WW _ can be chosen as before; and the weight for this weak hypothesis at = ln ww+411:) can be calculated. This procedure is repeated for T rounds while alternating between the two classifiers. The pseudo-code describing the algorithm is given in Fig. 2. The CoBoost algorithm described above divides the function Zco into two parts: Zco = 40 + 40. On each step CoBoost searches for a feature and a weight so as to minimize either 40 or 40. In Input: {(x1,i, Initialize: Vi, j : e(xi) = 0. For t = 1, T and for j = 1, 2: where 4 = exp(-jg'(xj,i)). practice, this greedy approach almost always results in an overall decrease in the value of Zco. Note, however, that there might be situations in which Zco in fact increases. One implementation issue deserves some elaboration. Note that in our formalism a weakhypothesis can abstain. In fact, during the first rounds many of the predictions of Th., g2 are zero. Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function. Each learner is free to pick the labels for these instances. This allow the learners to &quot;bootstrap&quot; each other by filling the labels of the instances on which the other side has abstained so far. The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case. Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98). In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case. AdaBoost.MH maintains a distribution over instances and labels; in addition, each weak-hypothesis outputs a confidence vector with one confidence value for each possible label. We again adopt an approach where we alternate between two classifiers: one classifier is modified while the other remains fixed. Pseudo-labels are formed by taking seed labels on the labeled examples, and the output of the fixed classifier on the unlabeled examples. AdaBoost.MH can be applied to the problem using these pseudolabels in place of supervised examples. For the experiments in this paper we made a couple of additional modifications to the CoBoost algorithm. The algorithm in Fig. (2) was extended to have an additional, innermost loop over the (3) possible labels. The weak hypothesis chosen was then restricted to be a predictor in favor of this label. Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained. This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating — this deserves more theoretical investigation. We also removed the context-type feature type when using the CoBoost approach. This &quot;default&quot; feature type has 100% coverage (it is seen on every example) but a low, baseline precision. When this feature type was included, CoBoost chose this default feature at an early iteration, thereby giving non-abstaining pseudo-labels for all examples, with eventual convergence to the two classifiers agreeing by assigning the same label to almost all examples. Again, this deserves further investigation. Finally, we would like to note that it is possible to devise similar algorithms based with other objective functions than the one given in Equ. (7), such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99). We are currently exploring such algorithms. The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem. A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on unlabeled examples, and observed variables on (seed) labeled examples. The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi). We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}, and the last (n — m) examples are unlabeled. For the purposes of EM, the &quot;observed&quot; data is {(xi, Ya• • • (xrn, Yrn), xfil, and the hidden data is {ym+i y}. The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9). Training under this model involves estimation of parameter values for P(y), P(m) and P(x I y). The maximum likelihood estimates (i.e., parameter values which maximize 10) can not be found analytically, but the EM algorithm can be used to hill-climb to a local maximum of the likelihood function from some initial parameter settings. In our experiments we set the parameter values randomly, and then ran EM to convergence. Given parameter estimates, the label for a test example x is defined as We should note that the model in equation 9 is deficient, in that it assigns greater than zero probability to some feature combinations that are impossible. For example, the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ). Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward. 88,962 (spelling,context) pairs were extracted as training data. 1,000 of these were picked at random, and labeled by hand to produce a test set. We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories. The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively. 123 examples fell into the noise category. Of these cases, 38 were temporal expressions (either a day of the week or month of the year). We excluded these from the evaluation as they can be easily identified with a list of days/months. This left 962 examples, of which 85 were noise. Taking /V, to be the number of examples an algorithm classified correctly (where all gold standard items labeled noise were counted as being incorrect), we calculated two measures of accuracy: See Tab. 2 for the accuracy of the different methods. Note that on some examples (around 2% of the test set) CoBoost abstained altogether; in these cases we labeled the test example with the baseline, organization, label. Fig. (3) shows learning curves for CoBoost. N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree. With each iteration more examples are assigned labels by both classifiers, while a high level of agreement (> 94%) is maintained between them. The test accuracy more or less asymptotes. Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules. In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98). The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function. We are currently exploring other methods that employ similar ideas and their formal properties. Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them. The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage. The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.
Distinguishing Word Senses In Untagged Text This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum—variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. Statistical methods for natural language processing are often dependent on the availability of costly knowledge sources such as manually annotated text or semantic networks. This limits the applicability of such approaches to domains where this hard to acquire knowledge is already available. This paper presents three unsupervised learning algorithms that are able to distinguish among the known senses (i.e., as defined in some dictionary) of a word, based only on features that can be automatically extracted from untagged text. The object of unsupervised learning is to determine the class membership of each observation (i.e. each object to be classified), in a sample without using training examples of correct classifications. We discuss three algorithms, McQuitty's similarity analysis (McQuitty, 1966), Ward's minimum—variance method (Ward, 1963) and the EM algorithm (Dempster, Laird, and Rubin, 1977), that can be used to distinguish among the known senses of an ambiguous word without the aid of disambiguated examples. The EM algorithm produces maximum likelihood estimates of the parameters of a probabilistic model, where that model has been specified in advance. Both Ward's and McQuitty's methods are agglomerative clustering algorithms that form classes of unlabeled observations that minimize their respective distance measures between class members. The rest of this paper is organized as follows. First, we present introductions to Ward's and McQuitty's methods (Section 2) and the EM algorithm (Section 3). We discuss the thirteen words (Section 4) and the three feature sets (Section 5) used in our experiments. We present our experimental results (Section 6) and close with a discussion of related work (Section 7). In general, clustering methods rely on the assumption that classes occupy distinct regions in the feature space. The distance between two points in a multi—dimensional space can be measured using any of a wide variety of metrics (see, e.g. (Devijver and Kittler, 1982)). Observations are grouped in the manner that minimizes the distance between the members of each class. Ward's and McQuitty's method are agglomerative clustering algorithms that differ primarily in how they compute the distance between clusters. All such algorithms begin by placing each observation in a unique cluster, i.e. a cluster of one. The two closest clusters are merged to form a new cluster that replaces the two merged clusters. Merging of the two closest clusters continues until only some specified number of clusters remain. However, our data does not immediately lend itself to a distance—based interpretation. Our features represent part—of—speech (POS) tags, morphological characteristics, and word co-occurrence; such features are nominal and their values do not have scale. Given a POS feature, for example, we could choose noun = 1, verb = 2, adjective = 3, and adverb = 4. That adverb is represented by a larger number than noun is purely coincidental and implies nothing about the relationship between nouns and adverbs. Thus, before we employ either clustering algorithm, we represent our data sample in terms of a dissimilarity matrix. Suppose that we have N observations in a sample where each observation has q features. This data is represented in a N xN dissimilarity matrix such that the value in cell (i, j), where i represents the row number and j represents the column, is equal to the number of features in observations i and j that do not match. For example, in Figure 1 we have four observations. We record the values of three nominal features for each observation. This sample can be represented by the 4 x 4 dissimilarity matrix shown in Figure 2. In the dissimilarity matrix, cells (1,2) and (2,1) have the value 2, indicating that the first and second observations in Figure 1 have different values for two of the three features. A value of 0 indicates that observations i and j are identical. When clustering our data, each observation is represented by its corresponding row (or column) in the dissimilarity matrix. Using this representation, observations that fall close together in feature space are likely to belong to the same class and are grouped together into clusters. In this paper, we use Ward's and McQuitty's methods to form clusters of observations, where each observation is represented by a row in a dissimilarity matrix. In Ward's method, the internal variance of a cluster is the sum of squared distances between each observation in the cluster and the mean observation for that cluster (i.e., the average of all the observations in the cluster). At each step in Ward's method, a new cluster, CKL, with the smallest possible internal variance, is created by merging the two clusters, CK and CL, that have the minimum variance between them. The variance between CK and CL is computed as follows: where TK is the mean observation for cluster CK) NK is the number of observations in CK, and Tr, and NL are defined similarly for CL. Implicit in Ward's method is the assumption that the sample comes from a mixture of normal distributions. While NLP data is typically not well characterized by a normal distribution (see, e.g. (Zipf, 1935), (Pedersen, Kayaalp, and Bruce, 1996)), there is evidence that our data, when represented by a dissimilarity matrix, can be adequately characterized by a normal distribution. However, we will continue to investigate the appropriateness of this assumption. In McQuitty's method, clusters are based on a simple averaging of the feature mismatch counts found in the dissimilarity matrix. At each step in McQuitty's method, a new cluster, CKL, is formed by merging the clusters CK and CL that have the fewest number of dissimilar features between them. The clusters to be merged, CK and CL, are identified by finding the cell (1,k) (or (k,1)), where k 1, that has the minimum value in the dissimilarity matrix. Once the new cluster CKL is created, the dissimilarity matrix is updated to reflect the number of dissimilar features between CKL and all other existing clusters. The dissimilarity between any existing cluster Cr and CKL is computed as: where DKI is the number of dissimilar features between clusters CK and C1 and DLI is similarly defined for clusters CL and C1. This is simply the average number of mismatches between each component of the new cluster and the existing cluster. Unlike Ward's method, McQuitty's method makes no assumptions concerning the distribution of the data sample. The expectation maximization algorithm (Dempster, Laird, and Rubin, 1977), commonly known as the EM algorithm, is an iterative estimation procedure in which a problem with missing data is recast to make use of complete data estimation techniques. In our work, the sense of an ambiguous word is represented by a feature whose value is missing. In order to use the EM algorithm, the parametric form of the model representing the data must be known. In these experiments, we assume that the model form is the Naive Bayes (Duda and Hart, 1973). In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the success of the Naive Bayes model when applied to supervised word—sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. Unfortunately there is little to be done in this case other than reducing the dimensionality of the problem so that fewer parameters are estimated. Second, if the likelihood function is very irregular it may always converge to a local maxima and not find the global maximum. In this case, an alternative is to use the more computationally expensive method of Gibbs Sampling (Geman and Geman, 1984). At the heart of the EM Algorithm lies the Qfunction. This is the expected value of the loglikelihood function for the complete data D = (Y, 8), where Y is the observed data and S is the missing sense value: Here, C is the current value of the maximum likelihood estimates of the model parameters and 02 is the improved estimate that we are seeking; p(Y, 510i) is the likelihood of observing the complete data given the improved estimate of the model parameters. When approximating the maximum of the likelihood function, the EM algorithm starts from a randomly generated initial estimate of C and then replaces 0 by the 0i which maximizes Q(0110). This process is broken down into two steps: expectation (the E-step), and maximization (the M-step). The E-step finds the expected values of the sufficient statistics of the complete model using the current estimates of the model parameters. The M-step makes maximum likelihood estimates of the model parameters using the sufficient statistics from the E-step. These, steps iterate until the parameter estimates 0 and 0i converge. The M-step is usually easy, assuming it is easy for the complete data problem; the E-step is not necessarily so. However, for decomposable models, such as the Naive Bayes, the E-step simplifies to the calculation of the expected counts in the marginal distributions of interdependent features, where the expectation is with respect to 0. The M-step simplifies to the calculation of new parameter estimates from these counts. Further, these expected counts can be calculated by multiplying the sample size N by the probability of the complete data within each marginal distribution given 0 and the observed data within each marginal Yrn. This simplifies to: where count i is the current estimate of the expected count and P(Sm1Y,n) is formulated using 0. For the Naive Bayes model with 3 observable features A, B, C and an unobservable classification feature 5, where 0 = {P(a, s), P(b, s), P(c, s), P(s)} , the E and M-steps are: where s, a, b, and c denote specific values of S, A, B, and C respectively, and P(s1b) and P(s1c) are defined analogously to P(sla). Experiments were conducted to disambiguate 13 different words using 3 different feature sets. In these experiments, each of the 3 unsupervised disambiguation methods is applied to each of the 13 words using each of the 3 feature sets; this defines a total of 117 different experiments. In addition, each experiment was repeated 25 times in order to study the variance introduced by randomly selecting initial parameter estimates, in the case of the EM algorithm, and randomly selecting among equally distant groups when clustering using Ward's and McQuitty's methods. In order to evaluate the unsupervised learning algorithms we use sense—tagged text in these experiments. However, this text is only used to evaluate the accuracy of our methods. The classes discovered by the unsupervised learning algorithms are mapped to dictionary senses in a manner that maximizes their agreement with the sense—tagged text. If the sense—tagged text were not available, as would often be the case in an unsupervised experiment, this mapping would have to be performed manually. The words disambiguated and their sense distributions are shown in Figure 3. All data, with the exception of the data for line, come from the ACL/DCI Wall Street Journal corpus (Marcus, Santorini, and Marcinkiewicz, 1993). With the exception of line, each ambiguous word is tagged with a single sense defined in the Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978). The data for the 12 words tagged using LDOCE senses are described in more detail in (Bruce, Wiebe, and Pedersen, 1996). The line data comes from both the ACL/DCI WSJ corpus and the American Printing House for the Blind corpus. Each occurrence of line is tagged with a single sense defined in WordNet (Miller, 1995). This data is described in more detail in (Leacock, Towell, and Voorhees, 1993). Every experiment utilizes all of the sentences available for each word. The number of sentences available per word is shown as &quot;total count&quot; in Figure 3. We have reduced the sense inventory of these words so that only the two or three most frequent senses are included in the text being disambiguated. For several of the words, there are minority senses that form a very small percentage (i.e., < 5%) of the total sample. Such minority classes are not yet well handled by unsupervised techniques; therefore we do not consider them in this study. We define three different feature sets for use in these experiments. Our objective is to evaluate the effect that different types of features have on the accuracy of unsupervised learning algorithms such as those discussed here. We are particularly interested in the impact of the overall dimensionality of the feature space, and in determining how indicative different feature types are of word senses. Our feature sets are composed of various combinations of the following five types of features. Morphology The feature M represents the morphology of the ambiguous word. For nouns, M is binary indicating singular or plural. For verbs, the value of M indicates the tense of the verb and can have up to 7 possible values. This feature is not used for adjectives. Adjective Senses chief (total count: 1048) highest in rank: 86% most important; main: 14% common: (total count: 1060) 84% as in the phrase 'common stock': belonging to or shared by 2 or more: 8% happening often; usual: 8% last: (total count: 3004) 94% on the occasion nearest in the past: after all others: 6% public: (total count: 715) 68% concerning people in general: concerning the government and people: 19% not secret or private: 13% Noun Senses bill: (total count: 1341) 68% a proposed law under consideration: a piece of paper money or treasury bill: 22% a list of things bought and their price: 10% concern: (total count: 1235) 64% a business; firm: worry; anxiety: 36% drug: (total count: 1127) 57% a medicine; used to make medicine: a habit-forming substance: 43% interest: (total count: 2113) 59% money paid for the use of money: a share in a company or business: 24% readiness to give attention: 17% line: (total count: 1149) 37% a wire connecting telephones: a cord; cable: 32% an orderly series: 30% Verb Senses agree: (total count: 1109) 74% to concede after disagreement: to share the same opinion: 26% close: (total count: 1354) 77% to (cause to) end: to (cause to) stop operation: 23% help: (total count: 1267) 78% to enhance - inanimate object: to assist - human object: 22% include: (total count: 1526) 91% to contain in addition to other parts: to be a part of- human subject: 9% Part—of—Speech Features of the form P Li represent the part—of—speech (POS) of the word i positions to the left of the ambiguous word. PR, represents the POS of the word i positions to the right. In these experiments, we used 4 POS features, PLi PL2, PRI, and PR2 to record the POS of the words 1 and 2 positions to the left and right of the ambiguous word. Each POS feature can have one of 5 possible values: noun, verb, adjective, adverb or other. Co—occurrences Features of the form Ci are binary co-occurrence features. They indicate the presences or absences of a particular content word in the same sentence as the ambiguous word. We use 3 binary co-occurrence features, C1, C2 and C3 to represent the presences or absences of each of the three most frequent content words, C1 being the most frequent content word, C2 the second most frequent and C3 the third. Only sentences containing the ambiguous word were used to establish word frequencies. Frequency based features like this one contain little information about low frequency classes. For words with skewed sense distribution, it is likely that the most frequent content words will be associated only with the dominate sense. As an example, consider the 3 most frequent content words occurring in the sentences that contain chief officer, executive and president. Chief has a majority class distribution of 86% and, not surprisingly, these three content words are all indicative of the dominate sense which is &quot;highest in rank&quot;. The set of content words used in formulating the co—occurrence features are shown in Figure 4. Note that million and company occur frequently. These are not likely to be indicative of a particular sense but more reflect the general nature of the Wall Street Journal corpus. Unrestricted Collocations Features of the form UL; and URi indicate the word occurring in the position i places to the left or right, respectively, of the ambiguous word. All features of this form have 21 possible values. Nineteen correspond to the 19 most frequent words that occur in that fixed position in all of the sentences that contain the particular ambiguous word. There is also a value, (none), that indicates when the position i to the left or right is occupied by a word that is not among the 19 most frequent, and a value, (null), indicating that the position i to the left or right falls outside of the sentence boundary. In these experiments we use 4 unrestricted collocation features, UL2, ULi, URI, and UR2. As an example, the values of these features for concern are as follows: Content Collocations Features of the form CLi and CR1 indicate the content word occurring in the position 1 place to the left or right, respectively, of the ambiguous word. The values of these features are defined much like the unrestricted collocations above, except that these are restricted to the 19 most frequent content words that occur only one position to the left or right of the ambiguous word. To contrast this set of features with the unrestricted collocations, consider concern again. The values of the features representing the 19 most frequent content words 1 position to the left and right are as follows: Feature Sets A, B and C The 3 feature sets used in these experiments are designated A, B and C and are formulated as follows: The dimensionality is the number of possible combinations of feature values and thus the size of the feature space. These values vary since the number of possible values for M varies with the part—of—speech of the ambiguous word. The lower number is associated with adjectives and the higher with verbs. To get a feeling for the adequacy of these feature sets, we performed supervised learning experiments with the interest data using the Naive Bayes model. We disambiguated 3 senses using a 10:1 training—to— test ratio. The average accuracies for each feature set over 100 random trials were as follows: A 80.9%, B 87.7%, and C 82.7%. The window size, the number of values for the POS features, and the number of words considered in the collocation features are kept deliberately small in order to control the dimensionality of the problem. In future work, we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in (Duda and Hart, 1973) and (Gale, Church, and Yarowsky, 1995). Figure 5 shows the average accuracy and standard deviation of disambiguation over 25 random trials for each combination of word, feature set and learning algorithm. Those cases where the average accuracy of one algorithm for a particular feature set is significantly higher than another algorithm, as judged by the t-test (p=.01), are shown in bold face. For each word, the most accurate overall experiment (i.e., algorithm/feature set combination), and those that are not significantly less accurate are underlined. Also included in Figure 5 is the percentage of each sample that is composed of the majority sense. This is the accuracy that can be obtained by a majority classifier; a simple classifier that assigns each ambiguous word to the most frequent sense in a sample. However, bear in mind that in unsupervised experiments the distribution of senses is not generally known. Perhaps the most striking aspect of these results is that, across all experiments, only the nouns are disambiguated with accuracy greater than that of the majority classifier. This is at least partially explained by the fact that, as a class, the nouns have the most uniform distribution of senses. This point will be elaborated on in Section 6.1. While the choice of feature set impacts accuracy, overall it is only to a small degree. We return to this point in Section 6.2. The final result, to be discussed in Section 6.3, is that the differences in the accuracy of these three algorithms are statistically significant both on average and for individual words. Extremely skewed distributions pose a challenging learning problem since the sample contains precious little information regarding minority classes. This makes it difficult to learn their distributions without prior knowledge. For unsupervised approaches, this problem is exacerbated by the difficultly in distinguishing the characteristics of the minority classes from noise. In this study, the accuracy of the unsupervised algorithms was less than that of the majority classifier in every case where the percentage of the majority sense exceeded 68%. However, in the cases where the performance of these algorithms was less than that of the majority classifier, they were often still providing high accuracy disambiguation (e.g., 91% accuracy for last). Clearly, the distribution of classes is not the only factor affecting disambiguation accuracy; compare the performance of these algorithms on bill and public which have roughly the same class distributions. It is difficult to quantify the effect of the distribution of classes on a learning algorithm particularly when using naturally occurring data. In previous unsupervised experiments with interest, using a modified version of Feature Set A, we were able to achieve an increase of 36 percentage points over the accuracy of the majority classifier when the 3 classes were evenly distributed in the sample (Pedersen and Bruce, 1997b). Here, our best performance using a larger sample with a natural distribution of senses is only an increase of 20 percentage points over the accuracy of the majority classifier. Because skewed distributions are common in lexical work (Zipf, 1935), they are an important consideration in formulating disambiguation experiments. In future work, we will investigate procedures for feature selection that are more sensitive to minority classes. Reliance on frequency based features, as used in this work, means that the more skewed the sample is, the more likely it is that the features will be indicative of only the majority class. Despite varying the feature sets, the relative accuracy of the three algorithms remains rather consistent. For 6 of the 13 words there was a single algorithm that was always significantly more accurate than the other two across all features. The EM algorithm was most accurate for last and line with all three feature sets. McQuitty's method was significantly more accurate for chief, common, public, and help regardless of the feature set. Despite this consistency, there were some observable trends associated with changes in feature set. For example, McQuitty's method was significantly more accurate overall in combination with feature set C while the EM algorithm was more accurate with Feature Set A, and the accuracy of Ward's method was the least favorable with Feature Set B. For the nouns, there was no significant difference between Feature Sets A and B when using the EM algorithm. For the verbs there was no significant difference between the three feature sets when using McQuitty's method. The adjectives were significantly more accurate when using McQuitty's method and Feature Set C. One possible explanation for the consistency of results as feature sets varied is that perhaps the features most indicative of word senses are included in all the sets due to the selection methods and the commonality of feature types. These common features may be sufficient for the level of disambiguation achieved here. This explanation seems more plausible for the EM algorithm, where features are weighted, but less so for McQuitty's and Ward's which use a representation that does not allow feature weighting. Based on the average accuracy over part—of—speech categories, the EM algorithm performs with the highest accuracy for nouns while McQuitty's method performs most accurately for verbs and adjectives. This is true regardless of the feature set employed. The standard deviations give an indication of the effect of ties on the clustering algorithms and the effect of the random initialization on the the EM algorithm. In few cases is the standard deviation very small. For the clustering algorithms, a high standard deviation indicates that ties are having some effect on the cluster analysis. This is undesirable and may point to a need to expand the feature set in order to reduce ties. For the EM algorithm, a high standard deviation means that the algorithm is not settling on any particular maxima. Results may become more consistent if the number of parameters that must be estimated was reduced. Figures 6, 7 and 8 show the confusion matrices associated with the disambiguation of concern, inleresi, and help, using Feature Sets A, B, and C, respectively. A confusion matrix shows the number of cases where the sense discovered by the algorithm agrees with the manually assigned sense along the main diagonal; disagreements are shown in the rest of the matrix. In general, these matrices reveal that both the EM algorithm and Ward's method are more biased toward balanced distributions of senses than is McQuitty's method. This may explain the better performance of McQuitty's method in disambiguating those words with the most skewed sense distributions, the adjectives and adverbs. It is possible to adjust the EM algorithm away from this tendency towards discovering balanced distributions by providing prior knowledge of the expected sense distribution. This will be explored in future work. Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create. It seems more reasonable to assume that such text will not Bootstrapping approaches require a small amount of disambiguated text in order to initialize the unsupervised learning algorithm. An early example of such an approach is described in (Hearst, 1991). A supervised learning algorithm is trained with a small amount of manually sense tagged text and applied to a held out test set. Those examples in the test set that are most confidently disambiguated are added to the training sample. A more recent bootstrapping approach is described in (Yarowsky, 1995). This algorithm requires a small number of training examples to serve as a seed. There are a variety of options discussed for automatically selecting seeds; one is to identify collocations that uniquely distinguish between senses. For plant, the collocations manufacturing plant and living plant make such a distinction. Based on 106 examples of manufacturing plant and 82 examples of living plant this algorithm is able to distinguish between two senses of plant for 7,350 examples with 97 percent accuracy. Experiments with 11 other words using collocation seeds result in an average accuracy of 96 percent. While (Yarowsky, 1995) does not discuss distinguishing more than 2 senses of a word, there is no immediate reason to doubt that the &quot;one sense per collocation&quot; rule (Yarowsky, 1993) would still hold for a larger number of senses. In future work we will evaluate using the &quot;one sense per collocation&quot; rule to seed our various methods. This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency. Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), (Schiitze, 1993), (Resnik, 1995a)). An early application of clustering to word—sense disambiguation is described in (Schiitze, 1992). There words are represented in terms of the cooccurrence statistics of four letter sequences. This representation uses 97 features to characterize a word, where each feature is a linear combination of letter four-grams formulated by a singular value decomposition of a 5000 by 5000 matrix of letter fourgram co-occurrence frequencies. The weight associated with each feature reflects all usages of the word in the sample. A context vector is formed for each occurrence of an ambiguous word by summing the vectors of the contextual words (the number of contextual words considered in the sum is unspecified). The set of context vectors for the word to be disambiguated are then clustered, and the clusters are manually sense tagged. The features used in this work are complex and difficult to interpret and it isn't clear that this complexity is required. (Yarowsky, 1995) compares his method to (Schiitze, 1992) and shows that for four words the former performs significantly better in distinguishing between two senses. Other clustering approaches to word—sense disambiguation have been based on measures of semantic distance defined with respect to a semantic network such as WordNet. Measures of semantic distance are based on the path length between concepts in a network and are used to group semantically similar concepts (e.g. (Li, Szpakowicz, and Matwin, 1995)). (Resnik, 1995b) provides an information theoretic definition of semantic distance based on WordNet. (McDonald et al., 1990) apply another clustering approach to word—sense disambiguation (also see (Wilks et al., 1990)). They use co-occurrence data gathered from the machine-readable version of LDOCE to define neighborhoods of related words. Conceptually, the neighborhood of a word is a type of equivalence class. It is composed of all other words that co-occur with the designated word a significant number of times in the LDOCE sense definitions. These neighborhoods are used to increase the number of words in the LDOCE sense definitions, while still maintaining some measure of lexical cohesion. The &quot;expanded&quot; sense definitions are then compared to the context of an ambiguous word, and the sensedefinition with the greatest number of word overlaps with the context is selected as correct. (Guthrie et al., 1991) propose that neighborhoods be subject dependent. They suggest that a word should potentially have different neighborhoods corresponding to the different LDOCE subject code. Subjectspecific neighborhoods are composed of words having at least one sense marked with that subject code. The only other application of the EM algorithm to word—sense disambiguation is described in (Gale, Church, and Yarowsky, 1995). There the EM algorithm is used as part of a supervised learning algorithm to distinguish city names from people's names. A narrow window of context, one or two words to either side, was found to perform better than wider windows. The results presented are preliminary but show an accuracy percentage in the mid-nineties when applied to Dixon, a name found to be quite ambiguous. It should be noted that the EM algorithm relates to a large body of work in speech processing. The Baum—Welch forward—backward algorithm (Baum, 1972) is a specialized form of the EM algorithm that assumes the underlying parametric model is a hidden Markov model. The Baum—Welch forward— backward algorithm has been used extensively in speech recognition (e.g. (Levinson, Rabiner, and Sondhi, 1983), (Kupiec, 1992)), (Jelinek, 1990)). Supervised learning approaches to word—sense disambiguation fall victim to the knowledge acquisition bottleneck. The creation of sense tagged text sufficient to serve as a training sample is expensive and time consuming. This bottleneck is eliminated through the use of unsupervised learning approaches which distinguish the sense of a word based only on features that can be automatically identified. In this study, we evaluated the performance of three unsupervised learning algorithms on the disambiguation of 13 words in naturally occurring text. The algorithms are McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm. Our findings show that each of these algorithms is negatively impacted by highly skewed sense distributions. Our methods and feature sets were found to be most successful in disambiguating nouns rather than adjectives or verbs. Overall, the most successful of our procedures was McQuitty's similarity analysis in combination with a high dimensional feature set. In future work, we will investigate modifications of these algorithms and feature set selection that are more effective on highly skewed sense distributions.
Mixture-Model Adaptation for SMT We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system. Language varies significantly across different genres, topics, styles, etc. This affects empirical models: a model trained on a corpus of car-repair manuals, for instance, will not be well suited to an application in the field of tourism. Ideally, models should be trained on text that is representative of the area in which they will be used, but such text is not always available. This is especially the case for bilingual applications, because parallel training corpora are relatively rare and tend to be drawn from specific domains such as parliamentary proceedings. In this paper we address the problem of adapting a statistical machine translation system by adjusting its parameters based on some information about a test domain. We assume two basic settings. In cross-domain adaptation, a small sample of parallel in-domain text is available, and it is used to optimize for translating future texts drawn from the same domain. In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation. Approaches developed for the two settings can be complementary: an in-domain development corpus can be used to make broad adjustments, which can then be fine tuned for individual source texts. Our method is based on the classical technique of mixture modeling (Hastie et al., 2001). This involves dividing the training corpus into different components, training a model on each part, then weighting each model appropriately for the current context. Mixture modeling is a simple framework that encompasses many different variants, as described below. It is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases. This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (Tillmann and Zhang, 2006; Liang et al., 2006). Techniques for assigning mixture weights depend on the setting. In cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly. In dynamic adaptation, training poses a problem because no reference text is available. Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample). We do not learn mixture weights directly with this method, because there is little hope that these would be well suited to new domains. Instead we attempt to learn how weights should be set as a function of distance. To our knowledge, this approach to dynamic adaptation for SMT is novel, and it is one of the main contributions of the paper. A second contribution is a fairly broad investigation of the large space of alternatives defined by the mixture-modeling framework, using a simple genrebased corpus decomposition. We experimented with the following choices: cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; various text distance metrics; different ways of converting distance metrics into weights; and granularity of the source unit being adapted to. The remainder of the paper is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes. Our baseline is a standard phrase-based SMT system (Koehn et al., 2003). Given a source sentence s, this tries to find the target sentence t� that is the most likely translation of s, using the Viterbi approximation: where alignment a = (91, �t1, j1), ..., (sK, tK, jK); tk are target phrases such that t = �t1 ... tK; sk are source phrases such that s = gj1 ... sjK; and 9k is the translation of the kth target phrase tk. To model p(t, a|s), we use a standard loglinear approach: where each fi(s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and 4-gram language model probabilities logp(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit. Phrase translation model probabilities are features of the form: log p(s|t,a) ≈ EKk=1 log p(gk|�tk). We use two different estimates for the conditional probabilities p(�t|g) and p(g|�t): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(�t|s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s� that matches some ngram in s, only the 30 top-ranked translations t according to p(�t|g) are retained. To derive the joint counts c(g, t) from which p(s|� and p(�t|s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). Our approach to mixture-model adaptation can be summarized by the following general algorithm: from several different domains. Set mixture weights as a function of the distances from corpus components to the current source text. 4. Combine weighted component models into a single global model, and use it to translate as described in the previous section. We now describe each aspect of this algorithm in more detail. We partition the corpus into different genres, defined as being roughly identical to corpus source. This is the simplest way to exploit heterogeneous training material for adaptation. An alternative, which we have not explored, would be to cluster the corpus automatically according to topic. We adapt both language and translation model features within the overall loglinear combination (1). To train translation models on each corpus component, we used a global IBM2 model for word alignment (in order to avoid degradation in alignment quality due to smaller training corpora), then extracted component-specific relative frequencies for phrase pairs. Lexical probabilities were also derived from the global IBM2 model, and were not adapted. The procedure for training component-specific language models on the target halves of each corpus component is identical to the procedure for the global model described in section 2. In addition to the component models, we also used a large static global model. The most commonly-used framework for mixture models is a linear one: where p(x|h) is either a language or translation model; pc(x|h) is a model trained on component c, and λc is the corresponding weight. An alternative, suggested by the form of the global model, is a loglinear combination: where we write αc to emphasize that in this case the mixing parameters are global weights, like the weights on the other features within the loglinear model. This is in contrast to linear mixing, where the combined model p(x|h) receives a loglinear weight, but the weights on the components do not participate in the global loglinear combination. One consequence is that it is more difficult to set linear weights using standard minimum-error training techniques, which assume only a “flat” loglinear model. We used four standard distance metrics to capture the relation between the current source or target text q and each corpus component.1 All are monolingual—they are applied only to source text or only to target text. The tf/idf metric commonly used in information retrieval is defined as cos(vc, vq), where vr and vq are vectors derived from component c and document q, each consisting of elements of the form: −p(w)log pdoc(w), where p(w) is the relative frequency of word w within the component or document, and pdoc(w) is the proportion of components it appears in. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a technique for implicitly capturing the semantic properties of texts, based on the use of Singular Value Decomposition to produce a rankreduced approximation of an original matrix of word and document frequencies. We applied this technique to all documents in the training corpus (as opposed to components), reduced the rank to 100, then calculated the projections of the component and document vectors described in the previous paragraph into the reduced space. Perplexity (Jelinek, 1997) is a standard way of evaluating the quality of a language model on a test text. We define a perplexity-based distance metric pc(q)1/|q|, where pc(q) is the probability assigned to q by an ngram language model trained on component c. The final distance metric, which we call EM, is based on expressing the probability of q as a wordlevel mixture model: p(q) = �|q |Ec dcpc(wi|hi), where q = wl ... w|q|, and pc(w|h) is the ngram probability of w following word sequence h in component c. It is straighforward to use the EM algorithm to find the set of weights �dc, Vc that maximizes the likelihood of q. The weight dc is defined as the distance to component c. For all experiments described below, we used a probability difference threshold of 0.001 as the EM convergence criterion. Our focus in this paper is on adaptation via mixture weights. However, we note that the usual loglinear parameter tuning described in section 2 can also be considered adaptation in the cross-domain setting, because learned preferences for word penalty, relative LM/TM weighting, etc, will reflect the target domain. This is not the case for dynamic adaptation, where, in the absence of an in-domain development corpus, the only information we can hope to glean are the weights on adapted models compared to other features of the system. The method used for adapting mixture weights depends on both the combining framework (loglinear versus linear), and the adaptive setting (crossdomain versus dynamic), as described below. When using a loglinear combining framework as described in section 3.3, mixture weights are set in the same way as the other loglinear parameters when performing cross-domain adaptation. Loglinear mixture models were not used for dynamic adaptation. For both adaptive settings, linear mixture weights were set as a function of the distance metrics described in section 3.4. Given a set of metrics {D1, ... , Dm}, let di,c be the distance from the current text to component c according to metric Di. A simple approach to weighting is to choose a single metric Di, and set the weights in (2) to be proportional to the corresponding distances: Because different distance metrics may capture complementary information, and because optimal weights might be a non-linear function of distance, we also experimented with a linear combination of metrics transformed using a sigmoid function: where Qi reflects the relative predictive power of Di, and the sigmoid parametes ai and bi can be set to selectively suppress contributions from components that are far away. Here we assume that Qi absorbs a normalization constant, so that the Ac’s sum to 1. In this approach, there are three parameters per distance metric to learn: Qi, ai, and bi. In general, these parameters are also specific to the particular model being adapted, ie the LM or the TM. To optimize these parameters, we fixed global loglinear weights at values obtained with Och’s algorithm using representative adapted models based on a single distance metric in (3), then used the Downhill Simplex algorithm (Press et al., 2002) to maximize BLEU score on the development corpus. For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). The two approaches just described avoid conditioning Ac explicitly on c. This is necessary for dynamic adaptation, since any genre preferences learned from the development corpus cannot be expected to generalize. However, it is not necessary for cross-domain adaptation, where the genre of the development corpus is assumed to represent the test domain. Therefore, we also experimented with using Downhill Simplex optimization to directly learn the set of linear weights Ac that yield maximum BLEU score on the development corpus. A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation. In this approach, both the global loglinear weights and, if they are being used, the mixture parameters Qi, ai, bi are set to characterize the test domain as in cross-domain adaptation. When translating, however, distances to the current source text are used in (3) or (4) instead of distances to the indomain development corpus. This obviously limits the metrics used to ones that depend only on source text. All experiments were run on the NIST MT evaluation 2006 Chinese data set. Table 1 summarizes the corpora used. The training corpus was divided into seven components according to genre; in all cases these were identical to LDC corpora, with the exception of the Newswire component, which was amalgamated from several smaller corpora. The target genre for cross-domain adaptation was newswire, for which high-quality training material is available. The cross-domain development set NIST04nw is the newswire subset of the NIST 2004 evaluation set, and the dynamic adaptation development set NIST04-mix is a balanced mixed-genre subset of NIST 2004. The NIST 2005 evaluation set was used for testing cross-domain adaptation, and the NIST 2006 evaluation set (both the “GALE” and “NIST” parts) was used to test dynamic adaptation. Because different development corpora are used for cross-domain and dynamic adaptation, we trained one static baseline model for each of these adaptation settings, on the corresponding development set. All results given in this section are BLEU scores. newswire, sp = speeches, ed = editorial, ng = newsgroup, bn = broadcast news, and bc = broadcast conversation. Table 2 shows a comparison between linear and loglinear mixing frameworks, with uniform weights used in the linear mixture. Both types of mixture model are better than the baseline, but the linear mixture is slightly better than the loglinear mixture. This is quite surprising, because these results are on the development set: the loglinear model tunes its component weights on this set, whereas the linear model only adjusts global LM and TM weights. We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components. None helped, however, and we conclude that the problem is most likely that Och’s algorithm is unable to find a good maximimum in this setting. Due to this result, all experiments we describe below involve linear mixtures only. Table 3 compares the performance of all distance metrics described in section 3.4 when used on their own as defined in (3). The difference between them is fairly small, but appears to be consistent across LM and TM adaptation and (for the LM metrics) across source and target side matching. In general, LM metrics seem to have a slight advantage over the vector space metrics, with EM being the best overall. We focus on this metric for most of the experiments that follow. the NIST04-nw development set. (Entries in the top right corner are missing due to lack of time.) Table 4 shows the performance of the parameterized weighting function described by (4), with source-side EM and LSA metrics as inputs. This is compared to direct weight optimization, as both these techniques use Downhill Simplex for parameter tuning. Unfortunately, neither is able to beat the performance of the normalized source-side EM metric on its own (reproduced on the first line from table 3). In additional tests we verified that this also holds for the test corpus. We speculate that this disappointing result is due to compromises made in order to run Downhill Simplex efficiently, including holding global weights fixed, using only a single starting point, and running with monotone decoding. tion on the NIST04-nw development set. formance of cross domain adaptation (reproduced from table 5 on the second line) is slightly better for the in-domain test set (NIST05), but worse than dynamic adaptation on the two mixed-domain sets. Table 5 shows results for cross-domain adaptation, using the source-side EM metric for linear weighting. Both LM and TM adaptation are effective, with test-set improvements of approximately 1 BLEU point over the baseline for LM adaptation and somewhat less for TM adaptation. Performance also improves on the NIST06 out-of-domain test set (although this set includes a newswire portion as well). However, combined LM and TM adaptation is not better than LM adaptation on its own, indicating that the individual adapted models may be capturing the same information. Table 6 contains results for dynamic adaptation, using the source-side EM metric for linear weighting. In this setting, TM adaptation is much less effective, not significantly better than the baseline; performance of combined LM and TM adaptation is also lower. However, LM adaptation improves over the baseline by up to a BLEU point. The perTable 7 shows results for the hybrid approach described at the end of section 3.5.2: global weights are learned on NIST04-nw, but linear weights are derived dynamically from the current test file. Performance drops slightly compared to pure crossdomain adaptation, indicating that it may be important to have a good fit between global and mixture weights. The results of the final experiment, to determine the effects of source granularity on dynamic adaptation, are shown in table 8. Source-side EM distances are applied to the whole test set, to genres within the set, and to each document individually. Global weights were tuned specifically for each of these conditions. There appears to be little difference among these approaches, although genre-based adaptation perhaps has a slight advantage. Mixture modeling is a standard technique in machine learning (Hastie et al., 2001). It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990). Most previous work on adaptive SMT focuses on the use of IR techniques to identify a relevant subset of the training corpus from which an adapted model can be learned. Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments. Hildebrand et al (1995) describe a similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation. They rely on a perplexity heuristic to determine an optimal size for the relevant subset. Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target-language corpus. This approach has the advantage of not limiting LM adaptation to a parallel corpus, but the disadvantage of requiring two translation passes (one to generate the nbest lists, and another to translate with the adapted model). Ueffing (2006) describes a self-training approach that also uses a two-pass algorithm. A baseline system generates translations that, after confidence filtering, are used to construct a parallel corpus based on the test set. Standard phrase-extraction techniques are then applied to extract an adapted phrase table from the system’s own output. Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters. Each source sentence is then decoded using the language model trained on the cluster that assigns highest likelihood to that sentence. The work we present here is complementary to both the IR approaches and Ueffing’s method because it provides a way of exploiting a preestablished corpus division. This has the potential to allow sentences having little surface similarity to the current source text to contribute statistics that may be relevant to its translation, for instance by raising the probability of rare but pertinent words. Our work can also be seen as extending all previous approaches in that it assigns weights to components depending on their degree of relevance, rather than assuming a binary distinction between relevant and non-relevant components. We have investigated a number of approaches to mixture-based adaptation using genres for Chinese to English translation. The most successful is to weight component models in proportion to maximum-likelihood (EM) weights for the current text given an ngram language model mixture trained on corpus components. This resulted in gains of around one BLEU point. A more sophisticated approach that attempts to transform and combine multiple distance metrics did not yield positive results, probably due to an unsucessful optmization procedure. Other conclusions are: linear mixtures are more tractable than loglinear ones; LM-based metrics are better than VS-based ones; LM adaptation works well, and adding an adapted TM yields no improvement; cross-domain adaptation is optimal, but dynamic adaptation is a good fallback strategy; and source granularity at the genre level is better than the document or test-set level. In future work, we plan to improve the optimization procedure for parameterized weight functions. We will also look at bilingual metrics for crossdomain adaptation, and investigate better combinations of cross-domain and dynamic adaptation.
A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translamodel that uses phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. The alignment template translation model (Och and Ney, 2004) and related phrase-based models advanced the previous state of the art by moving from words to phrases as the basic unit of translation. Phrases, which can be any substring and not necessarily phrases in any syntactic theory, allow these models to learn local reorderings, translation of short idioms, or insertions and deletions that are sensitive to local context. They are thus a simple and powerful mechanism for machine translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an from position i to position j inclusive, and similarly for eji . English sentence e is modeled as: The translation model P(f  |e) “encodes” e into f by the following steps: Other phrase-based models model the joint distribution P(e, f) (Marcu and Wong, 2002) or made P(e) and P(f  |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence correctly into English requires reversing a sequence of five elements. When we run a phrase-based system, Pharaoh (Koehn et al., 2003; Koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [you] [bangjiao]1 [de shaoshu guojia zhiyi] [Australia] [is] [dipl. rels. ]1 [with] [North Korea] [is] [one of the few countries] where we have used subscripts to indicate the reordering of phrases. The phrase-based model is able to order “diplomatic...Korea” correctly (using phrase reordering) and “one...countries” correctly (using a phrase translation), but does not accomplish the necessary inversion of those two groups. A lexicalized phrase-reordering model like that in use in ISI’s system (Och et al., 2004) might be able to learn a better reordering, but simpler distortion models will probably not. We propose a solution to these problems that does not interfere with the strengths of the phrasebased approach, but rather capitalizes on them: since phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well. In order to do this we need hierarchical phrases that consist of both words and subphrases. For example, a hierarchical phrase pair that might help with the above example is: (5) (yu 1 you 2 , have 2 with 1 ) where 1 and 2 are placeholders for subphrases. This would capture the fact that Chinese PPs almost always modify VP on the left, whereas English PPs usually modify VP on the right. Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair. Similarly, (6) ( 1 de 2 , the 2 that 1 ) would capture the fact that Chinese relative clauses modify NPs on the left, whereas English relative clauses modify on the right; and (7) ( 1 zhiyi, one of 1 ) would render the construction zhiyi in English word order. These three rules, along with some conventional phrase pairs, suffice to translate the sentence correctly: (8) [Aozhou] [shi] [[[yu [Bei Han]1 you [bangjiao]2] de [shaoshu guojia]3] zhiyi] [Australia] [is] [one of [the [few countries]3 that [have [dipl. rels. ]2 with [North Korea]1]]] The system we describe below uses rules like this, and in fact is able to learn them automatically from a bitext without syntactic annotation. It translates the above example almost exactly as we have shown, the only error being that it omits the word ‘that’ from (6) and therefore (8). These hierarchical phrase pairs are formally productions of a synchronous context-free grammar (defined below). A move to synchronous CFG can be seen as a move towards syntax-based MT; however, we make a distinction here between formally syntax-based and linguistically syntax-based MT. A system like that of Yamada and Knight (2001) is both formally and linguistically syntax-based: formally because it uses synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank). Our system is formally syntaxbased in that it uses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician’s grammar but often does not. In this respect it resembles Wu’s bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy. Our extraction method is basically the same as that of Block (2000), except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model. In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation. Our model is based on a weighted synchronous CFG (Aho and Ullman, 1969). In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: where X is a nonterminal, γ and α are both strings of terminals and nonterminals, and — is a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α. Rewriting begins with a pair of linked start symbols. At each step, two coindexed nonterminals are rewritten using the two components of a single rule, such that none of the newly introduced symbols is linked to any symbols already present. Thus the hierarchical phrase pairs from our above example could be formalized in a synchronous CFG as: where we have used boxed indices to indicate which occurrences of X are linked by —. Note that we have used only a single nonterminal symbol X instead of assigning syntactic categories to phrases. In the grammar we extract from a bitext (described below), all of our rules use only X, except for two special “glue” rules, which combine a sequence of Xs to form an S: These give the model the option to build only partial translations using hierarchical phrases, and then combine them serially as in a standard phrase-based model. For a partial example of a synchronous CFG derivation, see Figure 1. Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model. The weight of each rule is: where the φi are features defined on rules. For our experiments we used the following features, analogous to Pharaoh’s default feature set: model to learn a preference for longer or shorter derivations, analogous to Koehn’s phrase penalty (Koehn, 2003). The exceptions to the above are the two glue rules, (13), which has weight one, and (14), which has weight (16) w(S _� (S 1 X 2 , S 1 X 2 )) = exp(—λg) the idea being that λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Let D be a derivation of the grammar, and let f(D) and e(D) be the French and English strings generated by D. Let us represent D as a set of triples (r, i, j), each of which stands for an application of a grammar rule r to rewrite a nonterminal that spans f(D)ji on the French side.3 Then the weight of D hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 ,X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i is the product of the weights of the rules used in the translation, multiplied by the following extra factors: where plm is the language model, and exp(−λwp|e|), the word penalty, gives some control over the length of the English output. We have separated these factors out from the rule weights for notational convenience, but it is conceptually cleaner (and necessary for polynomial-time decoding) to integrate them into the rule weights, so that the whole model is a weighted synchronous CFG. The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton. The training process begins with a word-aligned corpus: a set of triples hf, e, ∼i, where f is a French sentence, e is an English sentence, and ∼ is a (manyto-many) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. (2003), which is based on that of Och and Ney (2004). This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence. Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate the phrase translation parameters from the hypothesized distribution. To do this, we first identify initial phrase pairs using the same criterion as previous systems (Och and Ney, 2004; Koehn et al., 2003): Definition 1. Given a word-aligned sentence pair hf, e, ∼i, a rule hfij, ej0 i0 i is an initial phrase pair of hf, e, ∼i iff: Next, we form all possible differences of phrase pairs: Definition 2. The set of rules of hf, e, ∼i is the smallest set satisfying the following: is a rule, where k is an index not used in r. The above scheme generates a very large number of rules, which is undesirable not only because it makes training and decoding very slow, but also because it creates spurious ambiguity—a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation. This can result in nbest lists with very few different translations or feature vectors, which is problematic for the algorithm we use to tune the feature weights. Therefore we filter our grammar according to the following principles, chosen to balance grammar size and performance on our development set: which simplifies the decoder implementation. Moreover, we prohibit nonterminals that are adjacent on the French side, a major cause of spurious ambiguity. 5. A rule must have at least one pair of aligned words, making translation decisions always based on some lexical evidence. Now we must hypothesize weights for all the derivations. Och’s method gives equal weight to all the extracted phrase occurences. However, our method may extract many rules from a single initial phrase pair; therefore we distribute weight equally among initial phrase pairs, but distribute that weight equally among the rules extracted from each. Treating this distribution as our observed data, we use relativefrequency estimation to obtain P(y  |α) and P(α  |y). Our decoder is a CKY parser with beam search together with a postprocessor for mapping French derivations to English derivations. Given a French sentence f, it finds the best derivation (or n best derivations, with little overhead) that generates (f, e) for some e. Note that we find the English yield of the highest-probability single derivation and not necessarily the highest-probability e, which would require a more expensive summation over derivations. We prune the search space in several ways. First, an item that has a score worse than /3 times the best score in the same cell is discarded; second, an item that is worse than the bth best item in the same cell is discarded. Each cell contains all the items standing for X spanning fi/. We choose b and /3 to balance speed and performance on our development set. For our experiments, we set b = 40,/3 = 10−1 for X cells, and b = 15,/3 = 10−1 for S cells. We also prune rules that have the same French side (b = 100). The parser only operates on the French-side grammar; the English-side grammar affects parsing only by increasing the effective grammar size, because there may be multiple rules with the same French side but different English sides, and also because intersecting the language model with the English-side grammar introduces many states into the nonterminal alphabet, which are projected over to the French side. Thus, our decoder’s search space is many times larger than a monolingual parser’s would be. To reduce this effect, we apply the following heuristic when filling a cell: if an item falls outside the beam, then any item that would be generated using a lowerscoring rule or a lower-scoring antecedent item is also assumed to fall outside the beam. This heuristic greatly increases decoding speed, at the cost of some search errors. Finally, the decoder has a constraint that prohibits any X from spanning a substring longer than 10 on the French side, corresponding to the maximum length constraint on initial rules during training. This makes the decoding algorithm asymptotically linear-time. The decoder is implemented in Python, an interpreted language, with C++ code from the SRI Language Modeling Toolkit (Stolcke, 2002). Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30). This is faster than our Python implementation of a standard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems. Our experiments were on Mandarin-to-English translation. We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty. The results of the experiments are summarized in Table 1. The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004a), as publicly distributed. We used the default feature set: language model (same as above), p(f¯  |¯e), p(¯e  |f¯), lexical weighting (both directions), distortion model, word penalty, and phrase penalty. We ran the trainer with its default settings (maximum phrase length 7), and then used Koehn’s implementation of minimumerror-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on our development set, yielding the values shown in Table 2. Finally, we ran the decoder on the test set, pruning the phrase table with b = 100, pruning the chart with b = 100,/3 = 10−5, and limiting distortions to 4. These are the default settings, except for the phrase table’s b, which was raised from 20, and the distortion limit. Both of these changes, made by Koehn’s minimum-error-rate trainer by default, improve performance on the development set. We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules. When filtered for the development set, the grammar has 2.2M rules (see Figure 2 for examples). We then ran the minimum-error rate trainer with our decoder to tune the feature weights, yielding the values shown in Table 2. Note that Ag penalizes the glue rule much less than App does ordinary rules. This suggests that the model will prefer serial combination of phrases, unless some other factor supports the use of hierarchical phrases (e.g., a better language model score). We then tested our system, using the settings described above.4 Our system achieves an absolute improvement of 0.02 over the baseline (7.5% relative), without using any additional training data. This difference is statistically significant (p < 0.01).5 See Table 1, which also shows that the relative gain is higher for higher n-grams. 4Note that we gave Pharaoh wider beam settings than we used on our own decoder; on the other hand, since our decoder’s chart has more cells, its b limits do not need to be as high. The use of hierarchical structures opens the possibility of making the model sensitive to syntactic structure. Koehn et al. (2003) mention German (es gibt, there is) as an example of a good phrase pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy. But in our model, the rule would indeed respect syntactic phrases, because it builds a pair of Ss out of a pair of NPs. Thus, favoring subtrees in our model that are syntactic phrases might provide a fairer way of testing the hypothesis that syntactic phrases are better phrases. This feature adds a factor to (17), { 1 if fij is a constituent 0 otherwise as determined by a statistical tree-substitutiongrammar parser (Bikel and Chiang, 2000), trained on the Penn Chinese Treebank, version 3 (250k words). Note that the parser was run only on the test data and not the (much larger) training data. Rerunning the minimum-error-rate trainer with the new feature yielded the feature weights shown in Table 2. Although the feature improved accuracy on the development set (from 0.314 to 0.322), it gave no statistically significant improvement on the test set. Hierarchical phrase pairs, which can be learned without any syntactically-annotated training data, improve translation accuracy significantly compared with a state-of-the-art phrase-based system. They also facilitate the incorporation of syntactic information, which, however, did not provide a statistically significant gain. Our primary goal for the future is to move towards a more syntactically-motivated grammar, whether by automatic methods to induce syntactic categories, or by better integration of parsers trained on annotated data. This would potentially improve both accuracy and efficiency. Moreover, reducing the grammar size would allow more ambitious training settings. The maximum initial phrase length is currently 10; preliminary experiments show that increasing this limit to as high as 15 does improve accuracy, but requires more memory. On the other hand, we have successfully trained on almost 30M+30M words by tightening the initial phrase length limit for part of the data. Streamlining the grammar would allow further experimentation in these directions. In any case, future improvements to this system will maintain the design philosophy proven here, that ideas from syntax should be incorporated into statistical translation, but not in exchange for the strengths of the phrase-based approach. I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik. This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700. S. D. G.
CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991). Lakoff (1993) argues that rather than being a rare form of creative language, some metaphors are ubiquitous, highly structured, and relevant to cognition. To date, there has been no robust, broadly applicable computational metaphor interpretation system, a gap this article is intended to take a first step toward filling. Most computational models of metaphor depend on hand-coded knowledge bases and work on a few examples. CorMet is designed to work on a larger class of metaphors by extracting knowledge from large corpora without drawing on any handcoded knowledge sources besides WordNet. A method for computationally interpreting metaphorical language would be useful for NLP. Although metaphorical word senses can be cataloged and treated as just another part of the lexicon, this kind of representation ignores regularities in polysemy. A conventional metaphor may have a very large number of linguistic manifestations, which makes it useful to model the metaphor’s underlying mechanisms. CorMet is not capable of interpreting any manifestation of conventional metaphor but is a step toward such a system. CorMet analyzes large corpora of domain-specific documents and learns the selectional preferences of the characteristic verbs of each domain. A selectional preference is a verb’s predilection for a particular type of argument in a particular role. For instance, the object of the verb pour is generally a liquid. Any noun that pour takes as an an object is likely to be intended as a liquid, either metaphorically or literally. CorMet finds conventional metaphors by finding systematic differences in selectional preferences between domains. For instance, if CorMet were to find a sentence like Funds poured into his bank account in a document from the FINANCE domain, it could infer that in that domain, pour has a selection preference for financial assets in its subject. By comparing this selectional preference with pour’s selectional preferences in the LAB domain, CorMet can infer a metaphorical mapping from money to liquids. By finding sets of co-occuring interconcept mappings (like the above mapping and a mapping from investments to containers, for instance), Cormet can articulate the higher-order structure of conceptual metaphors. Note that Cormet is designed to detect higherorder conceptual metaphors by finding some of the sentences embodying some of the interconcept mappings constituting the metaphor of interest but is not designed to be a tool for reliably detecting all instances of a particular metaphor. CorMet’s domain-specific corpora are obtained from the Internet. In this context, a domain is a set of related concepts, and a domain-specific corpus is a set of documents relevant to those concepts. CorMet’s input parameters are two domains between which to search for interconcept mappings and, for each domain, a set of characteristic keywords. CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991), a manually compiled catalog of metaphor. CorMet works on domains that are specific and concrete (e.g., the domain of finance, but not that of actions). CorMet’s discrimination is relatively coarse: It measures trends in selectional preferences across many documents, so common mappings are discernible. CorMet considers the selectional preferences only of verbs, on the theory that they are generally more selectively restrictive than nouns or adjectives. It is worth noting that WordNet, CorMet’s primary knowledge source, implicitly encodes some of the metaphors CorMet is intended to find; Peters and Peters (2000) use WordNet to find many artifact/cognition metaphors. Also, WordNet enumerates some metaphorical senses of some verbs. CorMet does not use any of WordNet’s information about verbs and ignores regularities in the distribution of noun homonyms that could be used to find some metaphors. The article is organized as follows: Section 2 describes the mechanisms by which conventional metaphors are detected. Section 3 walks through CorMet’s process in two examples. Section 4 describes how the system’s performance is evaluated against the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991), and Section 5 covers select related work. Ideally, CorMet could draw on a large quantity of manually vetted, highly representative domain-specific documents. The precompiled corpora available on-line (Kucera 1992; Marcus, Santorini, and Marcinkiewicz 1993) do not span enough subjects. Other on-line data sources include the Internet’s hierarchically structured indices, such as Yahoo’s ontology (www.yahoo.com) and Google’s (www.google.com). Each index entry contains a small number of high-quality links to relevant Web pages, but this is not helpful, because CorMet requires many documents, and those documents need not be of more than moderate quality. Searching the Internet for domain-specific text seems to be the only way to obtain sufficiently large, diverse corpora. CorMet obtains documents by submitting queries to the Google search engine. There are two types of queries: one to fetch any domain-specific documents and another to fetch domain-specific documents that contain a particular verb. The first kind of query consists of a conjunction of from two to five randomly selected domain keywords. Domain keywords are words characteristic of a domain, supplied by the user as an input. For the FINANCE domain, a reasonable set of keywords is stocks, bonds, NASDAQ, Dow, investment, finance. Each query incorporates only a few keywords in order to maximize the number of distinct possible queries. Queries for domain-specific documents containing a particular verb are composed of a conjunction of domain-specific terms and a disjunction of forms of the verb that are more likely to be verbs than other parts of speech. For the verb attack, for instance, acceptable forms are attacked and attacking, but not attack and attacks, which are more likely to be nouns. The syntactic categories in which a word form appears are determined by reference to WordNet. Some queries for the verb attack in the FINANCE domain are: Queries return links to up to 10,000 documents, of which CorMet fetches and analyzes no more than 3,000. In the 13 domains studied, about 75% of these documents are relevant to the domain of interest (as measured through a randomly chosen, handevaluated sample of 100 documents per domain), so the noise is substantial. The documents are processed to remove embedded scripts and HTML tags. The mined documents are parsed with the apple pie parser (Sekine and Grishman 1995). Case frames are extracted from parsed sentences using templates; for instance, (S (NP & OBJ) (VP (were  |was  |got  |get) (VP WORDFORM-PASSIVE)) is used to extract roles for passive, agentless sentences (where WORDFORM-PASSIVE is replaced by a passive form of the verb under analysis). Learning the selectional preferences for a verb in a domain is expensive in terms of time, so it is useful to find a small set of important verbs in each domain. CorMet seeks information about verbs typical of a domain, because these verbs are more likely to figure in metaphors in which that domain is the metaphor’s source. Besiege, for instance, is characteristic of the MILITARY domain and appears in many instances of the MILITARY → MEDICINE mapping, such as The antigens besieged the virus. To find domain-characteristic verbs, CorMet dynamically obtains a large sample of domain-relevant documents, decomposes them into a bag-of-words representation, stems the words with an implementation of the Porter (1980) stemmer, and finds the ratio of occurrences of each word stem to the total number of stems in the domain corpus. The frequency of each stem in the corpus is compared to its frequency in general English (as recorded in an English-language frequency dictionary [Kilgarriff 2003]). The 400 verb stems with the highest relative frequency (computed as a ratio of the stem’s frequency in the domain to its frequency in the English frequency dictionary) are considered characteristic. CorMet treats any word form that may be a verb (according to WordNet) as though it is a verb, which biases CorMet toward verbs with common nominal homonyms. Word stems that have high relative frequency in more than one domain, like e-mail and download, are eliminated on the suspicion that they are more characteristic of documents on the Internet in general than of a substantive domain. Table 1 lists the 20 highest-scoring stems in the LAB and FINANCE domains. There are three constraints on CorMet’s selectional-preference-learning algorithm. First, it must tolerate noise, because complex sentences are often misparsed, and the case frame extractor is error prone. Second, it should be able to work around WordNet’s lacunae. Finally, there should be a reasonable metric for comparing the similarity between selectional preferences. CorMet first uses the selectional-preference-learning algorithm described in Resnik (1993), then clustering over the results. Resnik’s algorithm takes a set of words observed in a case slot (e.g., the subject of pour or the indirect object of give) and finds the WordNet nodes that best characterize the selectional preferences of that slot. (Note that WordNet nodes are treated as categories subcategorizing their descendants.) A case slot has a preference for a WordNet node to the extent that that node, or one of its descendants, is more likely to appear in that case slot than it is to appear at random. An overall measure of the choosiness of a case slot is selectional-preference strength, SR(p), defined as the relative entropy of the posterior probability P(c|p) and the prior probability P(c) (where P(c) is the a priori probability of the appearance of a WordNet node c, or one of its descendants, and P(c|p) is the probability of that node or one of its descendants appearing in a case slot p.) Recall that the relative entropy of two distributions X and Y, D(X||Y), is the inefficiency incurred by using an encoding optimal for Y to encode X. The degree to which a case slot selects for a particular node is measured by selectional association. In effect, the selectional associations divide up the selectional preference strength for a case slot among that slot’s possible fillers. Selectional association is defined as To compute ΛR(p, c), what is needed is a distribution over word classes, but what is observed in the corpus is a distribution over word forms. Resnik’s algorithm works around this problem by approximating a word class distribution from the word form distribution. For each word form observed filling a case slot, credit is divided evenly among all of that word form’s possible senses (and their ancestors in WordNet). Although Resnik’s algorithm makes no explicit attempt at sense disambiguation, greater activation tends to accumulate in those nodes that best characterize a predicate’s selectional preferences. CorMet uses Resnik’s algorithm to learn domain-specific selection preferences. It often finds different selectional preferences for predicates whose preferences should, intuitively, be the same. In the MILITARY domain, the object of assault selects strongly for fortification but not social group, whereas the selectional preferences for the object of attack are the opposite. Taking the cosine of the selectional preferences of these two case slots (one of many possible similarity metrics) gives a surprisingly low score. In order to facilitate more accurate judgments of selectional-preference similarity, CorMet finds clusters of WordNet nodes that, although not as accurate, allow more meaningful comparisons of selectional preferences. Clusters are built using the nearest-neighbor clustering algorithm (Jain, Murty, and Flynn 1999). A predicate’s selectional preferences are represented as vectors whose nth element represents the selectional association of the nth WordNet node for that predicate. The similarity function used is the dot product of the two selectional-preference vectors. Empirically, the level of granularity obtained by running nearest-neighbor clustering twice (i.e., clustering over the sets of nodes constituting selectional preferences, then clustering over the clusters) produces the most conceptually coherent clusters. There are typically fewer than 100 second-order clusters (i.e., clusters of clusters) per domain. In the LAB domain there are 54 second-order clusters, and in the FINANCE domain there are 67. The time complexity of searching for metaphorical interconcept mappings between two domains is proportional to the number of pairs of salient domain objects, so it is more efficient to search over pairs of salient clusters than over the more numerous individual salient nodes. Table 2 shows a MILITARY cluster. These clusters are helpful for finding verbs with similar, but not identical, selectional preferences. Although attack, for instance, does not select for fortification, it does select for other elements of fortification’s cluster, such as building and defensive structure. The fundamental limitation of WordNet with respect to selectional-preference learning is that it fails to exhaust all possible lexical relationships. WordNet can hardly be blamed: The task of recording all possible relationships between all English words is prohibitively large, if not infinite. Nevertheless, there are many words that intuitively should have a common parent but do not. For instance, liquid body substance and water should both be hyponyms of liquid, but in WordNet their shallowest common ancestor is substance. One of the descendants of substance is solid, so there is no single node that represents all liquids. Li and Abe (1998) describe another method of corpus-driven selectional-preference learning that finds a tree cut of WordNet for each case slot. A tree cut is a set of The elements of a cluster of WordNet nodes characteristic of the MILITARY domain. nodes that specifies a partition of the ontology’s leaf nodes, where a node stands for all the leaf nodes descended from it. The method chooses among possible tree cuts according to minimum-description-length criteria. The description length of a tree cut representation is the sum of the size of the tree cut itself (i.e., the minimum number of nodes specifying the partition) and the space required for representing the observed data with that tree cut. For CorMet’s purposes, the problem with this approach is that it is difficult to find clusters of (possibly hypernymically related) nodes representing a selectional preference using its results (because the tree cut includes exactly one node on each path from each leaf node to the root). There are similar objections to similar approaches such as that of Carroll and McCarthy (2000). Polarity is a measure of the directionality and magnitude of structure transfer between two concepts or two domains. Nonzero polarity exists when language characteristic of a concept from one domain is used in a different domain of a different concept. The kind of characteristic language CorMet can detect is limited to verbal selectional preferences. Say CorMet is searching for a mapping between the concepts liquids (characteristic of the LAB domain) and assets (characteristic of the FINANCE domain), as illustrated in Figure 1. There are verbs in LAB that strongly select for liquids, such as pour, flow, and freeze. In FINANCE, these verbs select for assets. In FINANCE there are verbs that strongly select for assets such as spend, invest, and tax. In the LAB domain, these verbs select for nothing in particular. This suggests that liquid is the source concept and asset is the target concept, which implies that LAB and FINANCE are the source and target domains, respectively. CorMet computes the overall polarity between two domains (as opposed to between two concepts) by summing over the polarity between each pair of high-salience concepts from the two domains of interest. Interconcept polarity is defined as follows: Let α be the set of case slots in domain X with the strongest selectional preference for the node cluster A. Let β be the set of case slots in domain Y with the strongest selectional preferences for the node cluster B. The degree of structure flow from A in X to B in Y is computed as the degree to which the predicates α select for the nodes B in Y, or selection strength(Y, α, B). Structure flow in the opposite direction is selection strength(X, β,A). The definition of selection strength(Domain, case slots, node cluster) is the average of the selectionalpreference strengths of the predicates in case slots for the nodes in node cluster in Domain. The polarity for α and β is the difference in the two quantities. If the polarity is near zero, there is not much structure flow and no evidence for a metaphoric mapping. In some cases a difference in selectional preferences between domains does not indicate the presence of a metaphor. To take a fictitious but illustrative example, say Asymmetric structure transfer between LAB and FINANCE. Predicates from LAB that select for liquids are transferred to FINANCE and select for money. On the other hand, predicates from FINANCE that select for money are transferred to LAB and do not select for liquids. that in the LAB domain the subject of sit has a preference for chemists whereas in the FINANCE domain it has a preference for investment bankers. The difference in selectional preferences is caused by the fact that chemists are the kind of person more likely to appear in LAB documents and investment bankers in FINANCE ones. Instances like this are easy to filter out because their polarity is zero. A verb is treated as characteristic of a domain X if it is at least twice as frequent in the domain corpus as it is in general English and it is at least one and a half times as frequent in domain X as in the contrasting domain Y (these ratios were chosen empirically). Pour, for instance, occurs three times as often in FINANCE and twentythree times as often in LAB as it does in general English. Since it is nearly eight times as frequent in LAB as in FINANCE, it is considered characteristic of the former. This heuristic resolves the confusion than can be caused by the ubiquity of certain conventional metaphors—the high density of metaphorical uses of pour in FINANCE could otherwise make it seem as though pour is characteristic of that domain. A verb with weak selectional preferences (e.g., exist) is a bad choice for a characteristic predicate even if it occurs disproportionately often in a domain. Highly selective verbs are more useful because violations of their selectional preferences are more informative. For this reason a predicate’s salience to a domain is defined as its selectional-preference strength times the ratio of its frequency in the domain to its frequency in English. Literal and metaphorical selectional preferences may coexist in the same domain. Consider the selectional preferences of pour in the chemical and financial domains. In the LAB domain, pour is mostly used literally: People pour liquids. There are occasional metaphorical uses (e.g., Funding is pouring into basic proteomics research), but the literal sense is more common. In FINANCE, pour is mostly used metaphorically, although there are occasionally literal uses (e.g., Today oil poured into the new Turkmenistan pipeline). Algorithms 1–3 show pseudocode for finding metaphoric mappings between concepts. comment: Find mappings from concepts in domain1 to concepts in domain2 or vice versa Domain 1 Clusters +— GET BEST CLUSTERS(domain1) Domain 2 Clusters +— GET BEST CLUSTERS(domain2) for each Concept 1 E Domain 1 Clusters for each Concept 2 E Domain 2 Clusters polarity from 1 to 2 +— INTER CONCEPT POLARITY(Concept 1,Concept 2,domain1, domain2) polarity from 2 to 1 +— INTER CONCEPT POLARITY(Concept 2,Concept 1,domain2, domain1) if ABSOLUTE VALUE(polarity from 1 to 2 − polarity from 2 to 1) < C1 then return (0); if polarity from 1 to 2 > C2 and polarity from 2 to 1 > C2 According to the thematic-relation hypothesis (Grubner 1976), many domains are conceived of in terms of physical objects moving along paths between locations in space. In the money domain, assets are mapped to objects and asset holders are mapped to locations. In the idea domain, ideas are mapped to objects, minds are mapped to locations, and communications are mapped to paths. Axioms of inference from the target domain usually become available for reasoning about the source domain, unless there is an aspect of the source domain that specifically contradicts them. For instance, in the domain of material objects, a thing moved from point X to point Y is no longer at X, but in the idea domain, it exists at both locations. Thematically related metaphors may consistently co-occur in the same sentences. For example, the metaphors LIQUID → MONEY and CONTAINERS → INSTITUTIONS often co-occur, as in the sentence Capital flowed into the new company. Conversely, cooccurring metaphors are often components of a single metaphorical conceptualization. A metaphorical mapping is therefore more credible when it is a component of a system of mappings. In CorMet, systematicity measures a metaphorical mapping’s tendency to co-occur with other mappings. The systematicity score for a mapping X is defined as the number of strong, distinct mappings co-occurring with X. This measure goes only a little way toward capturing the extent to which a metaphor exhibits the structure described in the thematic-relations hypothesis, but extending CorMet to find the entities that correspond to objects, locations, and paths is beyond the scope of this article. CorMet computes a confidence measure for each metaphor it discovers. Confidence is a function of three things. The more verbs mediating a metaphor (as attack and assault mediate ENEMY → DISEASE in The antigen attacked the virus and Chemotherapy assaults the tumor), the more credible it is. Strongly unidirectional structure flow from source domain to target makes a mapping more credible. Finally, a mapping is more likely to be correct if it systematically co-occurs with other mappings. The confidence measure should not be interpreted as a probability of correctness: The data available for calibrating such a distribution are inadequate. The weights of each factor, empirically assigned plausible values, are given in Table 3. The confidence measure is intended to wrap all the available evidence about a metaphor’s credibility into one number. A principled way of doing this is desirable, but unfortunately there are not enough data to make meaningful use of machinelearning techniques to find the best set of components and weights. There is substantial arbitrariness in the confidence rating: The components used and the weights they are assigned could easily be different and are best considered guesses that give reasonable results. This section provides a walk-through of the derivation and analysis of the concept mapping LIQUID → MONEY and components of the interconcept mapping WAR → MEDICINE. In the interests of brevity only representative samples of CorMet’s data are shown. See Mason (2002) for a more detailed account. CorMet’s inputs are two domain sets of characteristic keywords for each domain (Table 4). The keywords must characterize a cluster in the space of Internet documents, but CorMet is relatively insensitive to the particular keywords. It is difficult to find keywords characterizing a cluster centering on money alone, so keywords for a more general domain, FINANCE, are provided. It is also difficult to characterize a cluster of documents mostly about liquids. Chemical-engineering articles and hydrographic encyclopedias tend to pertain to the highly technical aspects of liquids instead of their everyday behavior. Documents related to laboratory work are targeted on the theory that most references to liquids in a corpus dedicated to the manipulation and transformation of different states of matter are likely to be literal and will not necessarily be highly technical. Tables 5 and 6 show the top 20 characteristic verbs for LAB and FINANCE, respectively. CorMet finds the selectional preferences of all of the characteristic predicates’ case slots. A sample of the selectional preferences of the top 20 verbs in LAB and FINANCE are shown in Tables 7 and 8, respectively. The leftmost columns of these two tables have the (stemmed form of the) characteristic verb and the thematic role characterized. The right-hand sides have clusters of characteristic nodes. The numbers associated with the nodes are the bits of uncertainty about the identity of a word x resolved by the fact that x fills the given case slot, or P(x +— N) − P(x +— N|case slot(x)) (where x +— N is read as x is N or a hyponym of N). All of the 400 possible mappings between the top 20 concepts (clusters) from the two domains are examined. Each possible mapping is evaluated in terms of polarity, the number of frames instantiating the mapping, and the systematic co-occurrence of that mapping with different, highly salient mappings. The best mappings for LAB x FINANCE are shown in Table 9. Mappings are expressed in abbreviated form for clarity, with only the most recognizable (if not necessarily the most salient) node of each concept displayed. The foremost mapping characterizes money in terms of liquid, the mapping for which the two domains were selected. The second represents a somewhat less intuitive mapping from liquids to institutions. This metaphor is driven primarily by institutions’ capacity to dissolve. Of course, this mapping is incorrect insofar as solids undergo dissolution, not liquids. CorMet made this mistake because of faulty thematic-role identification; it frequently failed to distinguish between the different thematic roles played by the subjects in sentences like The company dissolved and The acid dissolved the compound. The third mapping characterizes communication as a liquid. This was not the mapping the author had in mind when he chose the domains, but it is intuitively plausible: One speaks of information flowing as readily as of money flowing. That this mapping appears in a search not targeted to it reflects this metaphor’s strength. It also illustrates a source of error in inferring the existence of conventional metaphors between domains from the existence of interconcept mappings. The fourth mapping is from containers to organizations. This mapping complements the first one: As liquids flow into containers, so money flows into organizations. Another good mapping, not present here, is money flows into equities and investments. CorMet misses this mapping because, at the level of concepts, money and equities are conflated. This happens because they are near relatives in the WordNet ontology and because there is very high overlap between the predicates selecting for them. Compare the mappings CorMet derived with the Master Metaphor List’s (Lakoff, Espenson, and Schwartz 1991) characterization of the MONEY IS A LIQUID metaphor: The Master Metaphor List also describes INVESTMENTS ARE CONTAINERS FOR MONEY, as exemplified in the following: CorMet has found mappings that can reasonably be construed as corresponding to these metaphors. Compare the mappings from the Master Metaphor List with frames mined by this system and identified as instantiating liquid → income, shown in Table 10. It is important to note that although CorMet can list the case frames that have driven the derivation of a particular high-level mapping, it is designed to discover highlevel mappings, not interpret or even recognize particular instances of metaphorical language. Just as in the Master Metaphor List, there are frames in the CorMet listing in which money and equities are characterized as liquids, are moved as liquids (i.e., pouring earnings and pumping reserves) and change state as liquids (i.e., melting stocks, dissolving stakes, evaporating profits, frozen money). This subsection describes the search for mappings between the MEDICINE and MILITARY domains. The domain keywords for MEDICINE and MILITARY are shown in Table 11. The characteristic verbs of the MILITARY and MEDICINE domains are given in Tables 12 and 13, respectively. Their selectional preferences are given in Tables 14 and 15, respectively. The highest-quality mappings between the MILITARY and MEDICINE domains are shown in Table 16. This pair of domains produces more mappings than the the LAB and FINANCE pair. Many source concepts from the MILITARY domain are mapped to body parts. The heterogeneity of the source concepts seems to be driven by the heterogeneity of possible military targets. Similarly, many source concepts are mapped to drugs. The case frames supporting this mapping suggest that this is because of the heterogeneity of military aggressors (fortifications do not generally fall into this category; this mapping is an error caused by the frame extractor’s frequent confusion of subject and object). These mappings can be interpreted as indicating that things that are attacked map to body parts and things that attack map to drugs. The mapping fortification → illness represents the mapping of targetable strongholds to disease. Illnesses are conceived of as fortifications besieged by treatment. Compare this with the Master Metaphor List’s characterization of TREATING ILLNESS IS FIGHTING A WAR: CorMet’s results can reasonably be interpreted as matching all of the mappings from the Master Metaphor List except winning-is-a-cure and defeat-is-dying. CorMet’s failure to find this mapping is caused by the fact that win, lose, and their synonyms do not have high salience in the MILITARY domain, which may be a reflection of the ubiquity of win and lose outside of that domain. Table 17 shows sample frames from which the body part → {fortification, vehicle, military action, region, skilled worker} mapping was derived. This section describes the evaluation of CorMet against a gold standard, specifically, by determining how many of the metaphors in a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991) can be discovered by CorMet given a characterization of the relevant source and target domains. The final evaluation of the correspondence between the mappings CorMet discovers and the Master Metaphor List entry is necessarily done by hand. This is a highly subjective method of evaluation; a formal, objective evaluation of correctness would be preferable, but at present no such metric is available. The Master Metaphor List is the basis for evaluation because it is composed of manually verified metaphors common in English. The test set is restricted to those elements of the Master Metaphor List with concrete source and target domains. This requirement excludes many important conventional metaphors, such as EVENTS ARE ACTIONS. About a fifth of the Master Metaphor List meets this constraint. This fraction is surprisingly small: It turns out that the bulk of the Master Metaphor List consists of subtle refinements of a few highly abstract metaphors. The concept pairs and corresponding domain pairs for the target metaphors in the Master Metaphor List are given in Table 18. A mapping discovered by CorMet is considered correct if submappings specified in the Master Metaphor List are nearly all present with high salience and incorrect submappings are present with comparatively low salience. The mappings discovered that best represent the targeted metaphors are shown in Table 19. Some of these test cases are marked successes. For instance, ECONOMIC HARM IS PHYSICAL INJURY seems to be captured by the mapping from the loss-3 cluster to the harm-1 cluster. CorMet found reasonable mappings in 10 of 13 cases attempted. This implies 77% accuracy, although in light of the small test and the subjectivity of judgment, this number must not be taken too seriously. Some test cases were disappointing. CorMet found no mapping between THEORY and ARCHITECTURE. This seems to be an artifact of the low-quality corpora obtained for these domains. The documents intended to be relevant to architecture were often about zoning or building policy, not the structure of buildings. For theory, many documents were calls for papers or about university department policy. It is unsurprising that there are no particular mappings between two sets of miscellaneous administrative and policy documents. The weakness of the ARCHITECTURE corpus also prevented CorMet from discovering any BODY → ARCHITECTURE mappings. Accuracy could be improved by refining the process by which domain-specific corpora are obtained to eliminate administrative documents or by requiring documents to have a higher density of domain-relevant terms. Is it meaningful when CorMet finds a mapping, or will it find a mapping between any pair of domains? To answer this question, CorMet was made to search for mappings between randomly selected pairs of domains. Table 20 lists a set of arbitrarily selected domain pairs and the strength of the polarization between them. In all cases, the polarization is zero. This can be interpreted as an encouraging lack of false positives. Another perspective is that CorMet should have found mappings between some of these pairs, such as MEDICINE and SOCIETY, on the theory that societies can be said to sicken, die, or heal. Although this is certainly a valid conventional metaphor, it seems to be less prominent than those metaphors that CorMet did discover. Two of the most broadly effective computational models of metaphor are Fass (1991) and Martin (1990), in both of which metaphors are detected through selectionalpreference violations and interpreted using an ontology. They are distinguished from CorMet in that they work on both novel and conventional metaphors and rely on declarative hand-coded knowledge bases. Fass (1991) describes Met*, a system for interpreting nonliteral language that builds on Wilks (1975) and Wilks (1978). Met* discriminates among metonymic, metaphorical, literal, and anomalous language. It is a component of collative semantics, a semantics for natural language processing that has been implemented in the program meta5 (Fass, 1986, 1987, 1988). Met* treats metonymy as a way of referring to one thing by means of another and metaphor as a way of revealing an interesting relationship between two entities. In Met*, a verb’s selectional preferences are represented as a vector of types. The verb drink’s preference for an animal subject and a liquid object are represented as (animal, drink, liquid). Metaphorical interpretations are made by finding a sense vector in Met*’s knowledge base whose elements are hypernyms of both the preferred argument types and the actual arguments. For example, the car drinks gasoline maps to the vector (car, drink, gasoline). But car is not a hypernym of animal, so Met* searches for a metaphorical interpretation, coming up with (thing, use, energy source). Martin (1990) describes the Metaphor Interpretation, Denotation, and Acquisition System (MIDAS), a computational model of metaphor interpretation. MIDAS has been integrated with the Unix Consultant (UC), a program that answers English questions about using Unix. UC tries to find a literal answer to each question with which it is presented. If violations of literal selectional preference make this impossible, UC calls on MIDAS to search its hierarchical library of conventional metaphors for one that explains the anomaly. If no such metaphor is found, MIDAS tries to generalize a known conventional metaphor by abstracting its components to the most-specific senses that encompass the question’s anomalous language. MIDAS then records the most concrete metaphor descended from the new, general metaphor that provides an explanation for the query’s language. MIDAS is driven by the idea that novel metaphors are derived from known, existing ones. The hierarchical structure of conventional metaphor is a regularity not captured by other computational approaches. Although MIDAS can quickly understand novel metaphors that are the descendants of metaphors in its memory, it cannot interpret compound metaphors or detect intermetaphor relationships besides inheritance. INVESTMENTS → CONTAINERS and MONEY → WATER, for instance, are clearly related, but not in a way that MIDAS can represent. Since not all novel metaphors are descendants of common conventional metaphors, MIDAS’s coverage is limited. MetaBank (Martin 1994) is an empirically derived knowledge base of conventional metaphors designed for use in natural language applications. MetaBank starts with a knowledge base of metaphors based on the Master Metaphor List. MetaBank can search a corpus for one metaphor or scan a large corpus for any metaphorical content. The search for a target metaphor is accomplished by choosing a set of probe words associated with that metaphor and finding sentences with those words, which are then manually sorted as literal, examples of the target metaphor, examples of a different metaphor, unsystematic homonyms, or something else. MetaBank compiles statistics on the frequency of conventional metaphors and the usefulness of the probe words. MetaBank has been used to study container metaphors in a corpus of UNIX-related e-mail and to study metaphor distributions in the Wall Street Journal. Peters and Peters (2000) mine WordNet for patterns of systematic polysemy by finding pairs of WordNet nodes at a relatively high level in the ontology (but still below the root nodes) whose descendants share a set of common word forms. The nodes publication and publisher, for instance, have paper, newspaper, and magazine as common descendants. This is a metonymic relationship; the system can also capture metaphoric relationships, as in the nodes supporting structure and theory, among whose common descendants are (for example) framework, foundation, and base. Peters and Peters’ system found many metaphoric relationships between node pairs that were descendants of the unique beginners artifact and cognition. Goatly (1997) describes a set of linguistic cues of metaphoricality beyond selectional-preference violations, such as metaphorically speaking and, surprisingly, literally. These cues are generally ambiguous (except for metaphorically speaking) but could usefully be incorporated into computational approaches to metaphor. CorMet embodies a method for semiautomatically finding metaphoric mappings between concepts, which can then be used to infer conventionally metaphoric relationships between domains. It can sometimes identify metaphoric language, if it manifests as a common selectional-preference gradient between domains, but is far from being able to recognize metaphoric language in general. CorMet differs from other computational approaches to metaphor in requiring no manually compiled knowledge base besides WordNet. It has successfully found some of the conventional metaphors on the Master Metaphor List. CorMet uses gradients in selectional preferences learned from dynamically mined, domain-specific corpora to identify metaphoric mappings between concepts. It is reasonably accurate despite the noisiness of many of its components. CorMet demonstrates the viability of a computational, corpus-based approach to conventional metaphor but requires more work before it can constitute a viable NLP tool.
Polylingual Topic Models Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. Statistical topic models have emerged as an increasingly useful analysis tool for large text collections. Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006). Much of this work, however, has occurred in monolingual contexts. In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience. In this paper, we present the polylingual topic model (PLTM). We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages). There are many potential applications for polylingual topic models. Although research literature is typically written in English, bibliographic databases often contain substantial quantities of work in other languages. To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages. Such analysis could be significant in tracking international research trends, where language barriers slow the transfer of ideas. Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations. However, the growth of the internet, and in particular Wikipedia, has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before. We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages. In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models. We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content. We also explore how the characteristics of different languages affect topic model performance. The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts. We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages. The internet makes it possible for people all over the world to access documents from different cultures, but readers will not be fluent in this wide variety of languages. By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence. Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007). Tam, Lane and Schultz (Tam et al., 2007) also show improvements in machine translation using bilingual topic models. Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added. We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages. A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here. However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages. They also provide little analysis of the differences between polylingual and single-language topic models. Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts. They find, for example, that English blog posts about the Nintendo Wii often relate to a hack, which cannot be mentioned in Japanese posts due to Japanese intellectual property law. Similarly, posts about whaling often use (positive) nationalist language in Japanese and (negative) environmentalist language in English. The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples. Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German. PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics. This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics. Additionally, PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1, ... , L. In other words, rather than using a single set of topics Φ = {φ1, ... , φT}, as in LDA, there are L sets of language-specific topics, Φ1, ... , ΦL, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl. Anew document tuple w = (w1, ... , wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then, for each language l, a latent topic assignment is drawn for each token in that language: Finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl,Φl) = 11n φlwl |zl . (3) The graphical model is shown in figure 1. Given a corpus of training and test document tuples—W and W', respectively—two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents. These tasks can either be accomplished by averaging over samples of Φ1, . . . , ΦL and αm from P(Φ1, ... , ΦL, αm  |W', β) or by evaluating a point estimate. We take the latter approach, and use the MAP estimate for αm and the predictive distributions over words for Φ1, . . . , ΦL.The probability of held-out document tuples W' given training tuples W is then approximated by Topic assignments for a test document tuple sampling. Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l,n is the current set of topic assignments for all other tokens in the tuple, while (Nt)\l,n is the number of occurrences of topic t in the tuple, excluding zln, the variable being resampled. Our first set of experiments focuses on document tuples that are known to consist of direct translations. In this case, we can be confident that the topic distribution is genuinely shared across all languages. Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model. The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish. These texts consist of roughly a decade of proceedings of the European parliament. For our purposes we use alignments at the speech level rather than the sentence level, as in many translation tasks using this corpus. We also remove the twenty-five most frequent word types for efficiency reasons. The remaining collection consists of over 121 million words. Details by language are shown in Table 1. ES otros otras otro otra parte dem‡s FI muiden toisaalta muita muut muihin muun FR autres autre part cTMt6 ailleurs m6me IT altri altre altro altra dall parte NL andere anderzijds anderen ander als kant PT outros outras outro lado outra noutros SV andra sidan Œ annat ena annan The concentration parameter α for the prior over document-specific topic distributions is initialized to 0.01 T, while the base measure m is initialized to the uniform distribution. Hyperparameters αm are re-estimated every 10 Gibbs iterations. Figure 2 shows the most probable words in all languages for four example topics, from PLTM with 400 topics. The first topic contains words relating to the European Central Bank. This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages. The second topic, concerning children, demonstrates the variability of everyday terminology: although the four Romance languages are closely related, they use etymologically unrelated words for children. (Interestingly, all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.) The third topic demonstrates differences in inflectional variation. English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words, while Greek and Finnish are dominated by inflected variants of the same lexical item. The final topic demonstrates that PLTM effectively clusters “syntactic” words, as well as more semantically specific nouns, adjectives and verbs. Although the topics in figure 2 seem highly focused, it is interesting to ask whether the model is genuinely learning mixtures of topics or simply assigning entire document tuples to single topics. To answer this question, we compute the posterior probability of each topic in each tuple under the trained model. If the model assigns all tokens in a tuple to a single topic, the maximum posterior topic probability for that tuple will be near to 1.0. If the model assigns topics uniformly, the maximum topic probability will be near 1/T. We compute histograms of these maximum topic probabilities for T ∈ {50,100, 200, 400, 800}. For clarity, rather than overlaying five histograms, figure 3 shows the histograms converted into smooth curves using a kernel density estimator.1 Although there is a small bump around 1.0 (for extremely short documents, e.g., “Applause”), values are generally closer to, but greater than, 1/T. Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages. Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple), we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple. For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions. Figure 4 shows the density of these divergences for different numbers of topics. As with the previous figure, there are a small number of documents that contain only one topic in all languages, and thus have zero divergence. These tend to be very short, formulaic parliamentary responses, however. The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that, for each tuple, the model is not simply assigning all tokens in a particular language to a single topic. As the number of topics increases, greater variability in topic distributions causes divergence to increase. Smoothed histograms of inter−language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples. Given a set of training document tuples, PLTM can be used to obtain posterior estimates of Φ', ... , ΦL and αm. The probability of previously unseen held-out document tuples given these estimates can then be computed. The higher the probability of the held-out document tuples, the better the generalization ability of the model. Analytically calculating the probability of a set of held-out document tuples given Φ1, ... , ΦL and αm is intractable, due to the summation over an exponential number of topic assignments for these held-out documents. However, recently developed methods provide efficient, accurate estimates of this probability. We use the “left-to-right” method of (Wallach et al., 2009). We perform five estimation runs for each document and then calculate standard errors using a bootstrap method. Table 2 shows the log probability of held-out data in nats per word for PLTM and LDA, both trained with 200 topics. There is substantial variation between languages. Additionally, the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA. It is important to note, however, that these results do not imply that LDA should be preferred over PLTM—that choice depends upon the needs of the modeler. Rather, these results are intended as a quantitative analysis of the difference between the two models. As the number of topics is increased, the word counts per topic become very sparse in monolingual LDA models, proportional to the size of the vocabulary. Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics. More than 350 topics in the Finnish LDA model have zero tokens assigned to them, and almost all tokens are assigned to the largest 200 topics. English has a larger tail, with non-zero counts in all but 16 topics. In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages. PLTM topics therefore have a higher granularity – i.e., they are more specific. This result is important: informally, we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model. An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages. For example, a journal might publish papers in English, French, German and Italian. No paper is exactly comparable to any other paper, but they are all roughly topically similar. If we wish to perform topic-based bibliometric analysis, it is vital to be able to track the same topics across all languages. One simple way to achieve this topic alignment is to add a small set of comparable document tuples that provide sufficient “glue” to bind the topics together. Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles. In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e., we put each of these documents in a single-document tuple. To do this, we divide the corpus W into two sets of document tuples: a “glue” set G and a “separate” set S such that |G |/ |W |= p. In other words, the proportion of tuples in the corpus that are treated as “glue” (i.e., placed in G) is p. For every tuple in S, we assign each document in that tuple to a new singledocument tuple. By doing this, every document in S has its own distribution over topics, independent of any other documents. Ideally, the “glue” documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently. FR russie tch´etch´enie union avec russe r´egion IT ho presidente mi perch´e relazione votato lang Topics at P = 0.25 DE rußland russland russischen tschetschenien ukraine EN russia russian chechnya cooperation region belarus FR russie tch´etch´enie avec russe russes situation IT russia unione cooperazione cecenia regione russa We train PLTM with 100 topics on corpora with p E 10.01, 0.05, 0.1, 0.25, 0.5}. We use 1000 iterations of Gibbs sampling with Q = 0.01. Hyperparameters αm are re-estimated every 10 iterations. We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation. The lower the divergence, the more similar the distributions are to each other. From the results in figure 4, we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1. Table 3 shows mean JS divergences for each value of p. As expected, JS divergence is greater than that obtained when all tuples are left intact. Divergence drops significantly when the proportion of “glue” tuples increases from 0.01 to 0.25. Example topics for p = 0.01 and p = 0.25 are shown in table 4. At p = 0.01 (1% “glue” documents), German and French both include words relating to Russia, while the English and Italian word distributions appear locally consistent but unrelated to Russia. At p = 0.25, the top words for all four languages are related to Russia. These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents. One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts. Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations. We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008). In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993). We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008). Unlike previous work (Koehn and Knight, 2002), we evaluate all words, not just nouns. We collected bilingual lexica mapping English words to German, Greek, Spanish, French, Italian, Dutch and Swedish. Each lexicon is a set of pairs consisting of an English word and a translated word, 1we, wt}. We do not consider multi-word terms. We expect that simple analysis of topic assignments for sequential words would yield such collocations, but we leave this for future work. For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt, respectively. We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica. Results for K = 1, that is, considering only the single most probable word for each language, are shown in figure 6. Precision at this level is relatively high, above 50% for Spanish, French and Italian with T = 400 and 800. Many of the candidate pairs that were not in the bilingual lexica were valid translations (e.g. EN “comitology” and IT lang Topics at P = 0.01 “comitalogia”) that simply were not in the lexica. We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften,” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3, T = 800, 1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other? The number of such pairs that appear in bilingual lexica is shown on the y-axis. For T = 800, the top English and Spanish words in 448 topics were exact translations of one another. In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus. These aligned document pairs could then be fed into standard machine translation systems as training data. To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language. It is not necessarily clear that PLTM will be effective at identifying translations. In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language. We are therefore interested in determining whether the information in the document-specific topic distributions is sufficient to identify semantically identical documents. We begin by dividing the data into a training set of 69,550 document tuples and a test set of 17,435 document tuples. In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set. We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009). Finally, for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language. We use both Jensen-Shannon divergence and cosine distance. For each document in the query language we rank all documents in the target language and record the rank of the actual translation. Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence. Cosine-based rankings are significantly worse. It is important to note that the length of documents matters. As noted before, many of the documents in the EuroParl collection consist of short, formulaic sentences. Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%. Performance continues to improve with longer documents, most likely due to better topic inference. Results vary by language. Table 5 shows results for all target languages with English as a query language. Again, English generally performs better with Romance languages than Germanic languages. Directly parallel translations are rare in many languages and can be extremely expensive to produce. However, the growth of the web, and in particular Wikipedia, has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora. In this section, we explore two questions relating to comparable text corpora and polylingual topic modeling. First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents. This property is useful for building machine translation systems as well as for human readers who are either learning new languages or analyzing texts in languages they do not know. Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?). We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh, German, Greek, English, Farsi, Finnish, French, Hebrew, Italian, Polish, Russian and Turkish. These versions of Wikipedia were selected to provide a diverse range of language families, geographic areas, and quantities of text. We preprocessed the data by removing tables, references, images and info-boxes. We dropped all articles in non-English languages that did not link to an English article. In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set. For efficiency, we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language. Even with these restrictions, the size of the corpus is 148.5 million words. We present results for a PLTM with 400 topics. 1000 Gibbs sampling iterations took roughly four days on one CPU with current hardware. As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple. We can then average over all such document-document divergences for each pair of languages to get an overall “disagreement” score between languages. Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations. Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages. Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents. To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic. Figure 8 represents the estimated probabilities of topics given a specific language. Competitive cross-country skiing (left) accounts for a significant proportion of the text in Finnish, but barely exists in Welsh and the languages in the Southeastern region. Meanwhile, interest in actors and actresses (center) is consistent across all languages. Finally, historical topics, such as the Byzantine and Ottoman empires (right) are strong in all languages, but show geographical variation: interest centers around the empires. We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages. We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. The authors thank Limin Yao, who was involved in early stages of this project. This work was supported in part by the Center for Intelligent Information Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant number IIS-0326249, and in part by Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103548106, and in part by National Science Foundation under NSF grant #CNS-0619337. Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not necessarily reflect those of the sponsor.
Multiple Aspect Ranking Using the Good Grief Algorithm We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would assign a numeric rank from 1-5 for each of: food quality, service, and ambience. A straightforward approach to this task would be to rank' the text independently for each aspect, using standard ranking techniques such as regression or classification. However, this approach fails to exploit meaningful dependencies between users’ judgments across different aspects. Knowledge of these dependencies can be crucial in predicting accurate ranks, as a user’s opinions on one aspect can influence his or her opinions on others. The algorithm presented in this paper models the dependencies between different labels via the agreement relation. The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction. Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction. The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 'In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance. This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). equal. The Good Grief decoding algorithm predicts a set of ranks – one for each aspect – which maximally satisfy the preferences of the individual rankers and the agreement model. For example, if the agreement model predicts consensus but the individual rankers select ranks (5, 5, 4), then the decoder decides whether to trust the the third ranker, or alter its prediction and output (5, 5, 5) to be consistent with the agreement prediction. To obtain a model well-suited for this decoding, we also develop a joint training method that conjoins the training of multiple aspect models. We demonstrate that the agreement-based joint model is more expressive than individual ranking models. That is, every training set that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates our exploration of a new method for joint multiple aspect ranking. Ranking The ranking, or ordinal regression, problem has been extensivly studied in the Machine Learning and Information Retrieval communities. In this section we focus on two online ranking methods which form the basis of our approach. The first is a model proposed by Crammer and Singer (2001). The task is to predict a rank y E I1, ..., k} for every input x E R'. Their model stores a weight vector w E R' and a vector of increasing boundaries b0 = −00 < b1 < ... < bk−1 < bk = 00 which divide the real line into k segments, one for each possible rank. The model first scores each input with the weight vector: score(x) = w · x. Finally, the model locates score(x) on the real line and returns the appropriate rank as indicated by the boundaries. Formally, the model returns the rank r such that br−1 < score(x) < br. The model is trained with the Perceptron Ranking algorithm (or “PRank algorithm”), which reacts to incorrect predictions on the training set by updating the weight and boundary vectors. The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database. An extension of this model is provided by Basilico and Hofmann (2004) in the context of collaborative filtering. Instead of training a separate model for each user, Basilico and Hofmann train a joint ranking model which shares a set of boundaries across all users. In addition to these shared boundaries, userspecific weight vectors are stored. To compute the score for input x and user i, the weight vectors for all users are employed: where 0 < sim(i, j) < 1 is the cosine similarity between users i and j, computed on the entire training set. Once the score has been computed, the prediction rule follows that of the PRanking model. The model is trained using the PRank algorithm, with the exception of the new definition for the scoring function.2 While this model shares information between the different ranking problems, it fails to explicitly model relations between the rank predictions. In contrast, our algorithm uses an agreement model to learn such relations and inform joint predictions. The goal of our algorithm is to find a rank assignment that is consistent with predictions of individual rankers and the agreement model. To this end, we develop the Good Grief decoding procedure that minimizes the dissatisfaction (grief) of individual components with a joint prediction. In this section, we formally define the grief of each component, and a mechanism for its minimization. We then describe our method for joint training of individual rankers that takes into account the Good Grief decoding procedure. In an m-aspect ranking problem, we are given a training sequence of instance-label pairs (x1, y1), ..., (xt, yt), .... Each instance xt is a feature vector in Rn and the label yt is a vector of m ranks in Ym, where Y = 11, .., k} is the set of possible ranks. The ith component of yt is the rank for the ith aspect, and will be denoted by y[i]t. The goal is to learn a mapping from instances to rank sets, H : X —* Ym, which minimizes the distance between predicted ranks and true ranks. Our m-aspect ranking model contains m+1 components: ((w[1], b[1]), ..., (w[m], b[m]), a). The first m components are individual ranking models, one for each aspect, and the final component is the agreement model. For each aspect i E 1...m, w[i] E Rn is a vector of weights on the input features, and b[i] E Rk−1 is a vector of boundaries which divide the real line into k intervals, corresponding to the k possible ranks. The default prediction of the aspect ranking model simply uses the ranking rule of the PRank algorithm. This rule predicts the rank r such that b[i]r−1 < scorei(x) < b[i]r.3 The value scorei(x) can be defined simply as the dot product w[i]·x, or it can take into account the weight vectors for other aspects weighted by a measure of interaspect similarity. We adopt the definition given in equation 1, replacing the user-specific weight vectors with our aspect-specific weight vectors. 3More precisely (taking into account the possibility of ties): y[i] = min1E{1,..,k}{r : scorei(x) − b[i],. < 01 The agreement model is a vector of weights a E Rn. A value of a · x > 0 predicts that the ranks of all m aspects are equal, and a value of a · x < 0 indicates disagreement. The absolute value Ja · xJ indicates the confidence in the agreement prediction. The goal of the decoding procedure is to predict a joint rank for the m aspects which satisfies the individual ranking models as well as the agreement model. For a given input x, the individual model for aspect i predicts a default rank y[i] based on its feature weight and boundary vectors (w[i], b[i]). In addition, the agreement model makes a prediction regarding rank consensus based on a · x. However, the default aspect predictions 9[1] ... y[m] may not accord with the agreement model. For example, if a · x > 0, but 9[i] = y[j] for some i, j E 1...m, then the agreement model predicts complete consensus, whereas the individual aspect models do not. We therefore adopt a joint prediction criterion which simultaneously takes into account all model components – individual aspect models as well as the agreement model. For each possible prediction r = (r[1], ..., r[m]) this criterion assesses the level of grief associated with the ith-aspect ranking model, gi(x, r[i]). Similarly, we compute the grief of the agreement model with the joint prediction, ga(x, r) (both gi and ga are defined formally below). The decoder then predicts the m ranks which minimize the overall grief:  (2) If the default rank predictions for the aspect models, y = (0[1], ..., y[m]), are in accord with the agreement model (both indicating consensus or both indicating contrast), then the grief of all model components will be zero, and we simply output y. On the other hand, if y indicates disagreement but the agreement model predicts consensus, then we have the option of predicting y and bearing the grief of the agreement model. Alternatively, we can predict some consensus y0 (i.e. with y0[i] = y0[j], Vi, j) and bear the grief of the component ranking models. The decoder H chooses the option with lowest overall grief.4 m Now we formally define the measures of grief used in this criterion. Aspect Model Grief We define the grief of the ithaspect ranking model with respect to a rank r to be the smallest magnitude correction term which places the input’s score into the rth segment of the real line: Agreement Model Grief Similarly, we define the grief of the agreement model with respect to a joint rank r = (r[1], ... , r[m]) as the smallest correction needed to bring the agreement score into accord with the agreement relation between the individual ranks r[1], ... , r[m]: Ranking models Pseudo-code for Good Grief training is shown in Figure 1. This training algorithm is based on PRanking (Crammer and Singer, 2001), an online perceptron algorithm. The training is performed by iteratively ranking each training input x and updating the model. If the predicted rank y is equal to the true rank y, the weight and boundaries vectors remain unchanged. On the other hand, if y = y, then the weights and boundaries are updated to improve the prediction for x (step 4.c in Figure 1). See (Crammer and Singer, 2001) for explanation and analysis of this update rule. Our algorithm departs from PRanking by conjoining the updates for the m ranking models. We achieve this by using Good Grief decoding at each step throughout training. Our decoder H(x) (from equation 2) uses all the aspect component models ponent models are comparable. In practice, we take an uncalibrated agreement model a' and reweight it with a tuning parameter: a = αa'. The value of α is estimated using the development set. We assume that the griefs of the ranking models are comparable since they are jointly trained. as well as the (previously trained) agreement model to determine the predicted rank for each aspect. In concrete terms, for every training instance x, we predict the ranks of all aspects simultaneously (step 2 in Figure 1). Then, for each aspect we make a separate update based on this joint prediction (step 4 in Figure 1), instead of using the individual models’ predictions. Agreement model The agreement model a is assumed to have been previously trained on the same training data. An instance is labeled with a positive label if all the ranks associated with this instance are equal. The rest of the instances are labeled as negative. This model can use any standard training algorithm for binary classification such as Perceptron or SVM optimization. Ranking Models Following previous work on sentiment classification (Pang et al., 2002), we represent each review as a vector of lexical features. More specifically, we extract all unigrams and bigrams, discarding those that appear fewer than three times. This process yields about 30,000 features. Agreement Model The agreement model also operates over lexicalized features. The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi (2002). In addition to unigrams and bigrams, we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review. For example, the presence of “delicious” and “dirty” indicate high contrast, whereas the pair “expensive” and “slow” indicate low contrast. The contrastive distance for a pair of words is computed by considering the difference in relative weight assigned to the words in individually trained PRanking models. In this section, we prove that our model is able to perfectly rank a strict superset of the training corpora perfectly rankable by m ranking models individually. We first show that if the independent ranking models can individually rank a training set perfectly, then our model can do so as well. Next, we show that our model is more expressive by providing a simple illustrative example of a training set which can only be perfectly ranked with the inclusion of an agreement model. First we introduce some notation. For each training instance (xt, yt), each aspect i  1...m, and each rank r  1...k, define an auxiliary variable y[i]t r with y[i]t r = −1 if y[i]t  r and y[i]t r = 1 if y[i]t > r. In words, y[i]t r indicates whether the true rank y[i]t is to the right or left of a potential rank r. Now suppose that a training set (x1, y1), ..., (xT , yT) is perfectly rankable for each aspect independently. That is, for each aspect i  1...m, there exists some ideal model v[i]∗ = (w[i]∗, b[i]∗) such that the signed distance from the prediction to the rth boundary: w[i]∗ · xt − b[i]∗r has the same sign as the auxiliary variable y[i]tr. In other words, the minimum margin over all training instances and ranks, γ = minrt{(w[i]∗ · xt − b[i]∗r)y[i]tr}, is no less than zero. Now for the tth training instance, define an agreement auxiliary variable at, where at = 1 when all aspects agree in rank and at = −1 when at least two aspects disagree in rank. First consider the case where the agreement model a perfectly classifies all training instances: (a · xt)at > 0, t. It is clear that Good Grief decoding with the ideal joint model (w[1]∗, b[1]∗, ..., w[m]∗, b[m]∗, a) will produce the same output as the component ranking models run separately (since the grief will always be zero for the default rank predictions). Now consider the case where the training data is not linearly separable with regard to agreement classification. Define the margin of the worst case error to be β = maxt{|(a·xt) |: (a·xt)at < 0}. If β < γ, then again Good Grief decoding will always produce the default results (since the grief of the agreement model will be at most β in cases of error, whereas the grief of the ranking models for any deviation from their default predictions will be at least γ). On the other hand, if β  γ, then the agreement model errors could potentially disrupt the perfect ranking. However, we need only rescale w∗ := w∗(  + 0 and b∗ := b∗( + E) to ensure that the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error. Thus whenever independent ranking models can perfectly rank a training set, a joint ranking model with Good Grief decoding can do so as well. Now we give a simple example of a training set which can only be perfectly ranked with the addition of an agreement model. Consider a training set of four instances with two rank aspects: We can interpret these inputs as feature vectors corresponding to the presence of “good”, “bad”, and “but not” in the following four sentences: The food was good, but not the ambience. The food was good, and so was the ambience. The food was bad, but not the ambience. The food was bad, and so was the ambience. We can further interpret the first rank aspect as the quality of food, and the second as the quality of the ambience, both on a scale of 1-2. A simple ranking model which only considers the words “good” and “bad” perfectly ranks the food aspect. However, it is easy to see that no single model perfectly ranks the ambience aspect. Consider any model (w, b = (b)). Note that w · x1 < b and w · x2 > b together imply that w3 < 0, whereas w · x3 > b and w · x4 < b together imply that w3 > 0. Thus independent ranking models cannot perfectly rank this corpus. The addition of an agreement model, however, can easily yield a perfect ranking. With a = (0, 0, −5) (which predicts contrast with the presence of the words “but not”) and a ranking model for the ambience aspect such as w = (1, −1, 0), b = (0), the Good Grief decoder will produce a perfect rank. We evaluate our multi-aspect ranking algorithm on a corpus of restaurant reviews available on the website http://www.we8there.com. Reviews from this website have been previously used in other sentiment analysis tasks (Higashinaka et al., 2006). Each review is accompanied by a set of five ranks, each on a scale of 1-5, covering food, ambience, service, value, and overall experience. These ranks are provided by consumers who wrote original reviews. Our corpus does not contain incomplete data points since all the reviews available on this website contain both a review text and the values for all the five aspects. Training and Testing Division Our corpus contains 4,488 reviews, averaging 115 words. We randomly select 3,488 reviews for training, 500 for development and 500 for testing. Parameter Tuning We used the development set to determine optimal numbers of training iterations for our model and for the baseline models. Also, given an initial uncalibrated agreement model a', we define our agreement model to be a = αa' for an appropriate scaling factor α. We tune the value of α on the development set. Corpus Statistics Our training corpus contains 528 among 55 = 3025 possible rank sets. The most frequent rank set (5, 5, 5, 5, 5) accounts for 30.5% of the training set. However, no other rank set comprises more than 5% of the data. To cover 90% of occurrences in the training set, 227 rank sets are required. Therefore, treating a rank tuple as a single label is not a viable option for this task. We also find that reviews with full agreement across rank aspects are quite common in our corpus, accounting for 38% of the training data. Thus an agreementbased approach is natural and relevant. A rank of 5 is the most common rank for all aspects and thus a prediction of all 5’s gives a MAJORITY baseline and a natural indication of task difficulty. Evaluation Measures We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer, 2001; Basilico and Hofmann, 2004). Ranking loss measures the average distance between the true rank and the predicted rank. Formally, given N test instances (x1, y1), ..., (xN, yN) of an m-aspect ranking problem and the corresponding predictions ˆy1, ..., ˆyN, ranking loss is defined as Et,i |y[i]t_ˆy[i]t|. Lower values of this measure cormN respond to a better performance of the algorithm. Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN+DECODE along with various baselines, including the simple MAJORITY baseline mentioned in section 5. The first competitive baseline, PRANK, learns a separate ranker for each aspect using the PRank algorithm. The second competitive baseline, SIM, shares the weight vectors across aspects using a similarity measure (Basilico and Hofmann, 2004). Both of these methods are described in detail in Section 2. In addition, we consider two variants of our algorithm: GG DECODE employs the PRank training algorithm to independently train all component ranking models and only applies Good Grief decoding at test time. GG ORACLE uses Good Grief training and decoding but in both cases is given perfect knowledge of whether or not the true ranks all agree (instead of using the trained agreement model). Our model achieves a rank error of 0.632, compared to 0.675 for PRANK and 0.663 for SIM. Both of these differences are statistically significant at p < 0.002 by a Fisher Sign Test. The gain in performance is observed across all five aspects. Our model also yields significant improvement (p < 0.05) over the decoding-only variant GG DECODE, confirming the importance of joint training. As shown in Figure 2, our model demonstrates consistent improvement over the baselines across all the training rounds. Model Analysis We separately analyze our percomputed separately on cases of actual consensus and actual disagreement. formance on the 210 test instances where all the target ranks agree and the remaining 290 instances where there is some contrast. As Table 2 shows, we outperform the PRANK baseline in both cases. However on the consensus instances we achieve a relative reduction in error of 21.8% compared to only a 1.1% reduction for the other set. In cases of consensus, the agreement model can guide the ranking models by reducing the decision space to five rank sets. In cases of disagreement, however, our model does not provide sufficient constraints as the vast majority of ranking sets remain viable. This explains the performance of GG ORACLE, the variant of our algorithm with perfect knowledge of agreement/disagreement facts. As shown in Table 1, GG ORACLE yields substantial improvement over our algorithm, but most of this gain comes from consensus instances (see Table 2). We also examine the impact of the agreement model accuracy on our algorithm. The agreement model, when considered on its own, achieves classification accuracy of 67% on the test set, compared to a majority baseline of 58%. However, those instances with high confidence |a · x |exhibit substantially higher classification accuracy. Figure 3 shows the performance of the agreement model as a function of the confidence value. The 10% of the data with highest confidence values can be classified by the agreement model with 90% accuracy, and the third of the data with highest confidence can be classified at 80% accuracy. This property explains why the agreement model helps in joint ranking even though its overall accuracy may seem low. Under the Good Grief criterion, the agreement model’s prediction will only be enforced when its grief outweighs that of the ranking models. Thus in cases where the prediction confidence (|a·x|) is relatively low,6 the agreement model will essentially be ignored. We considered the problem of analyzing multiple related aspects of user reviews. The algorithm presented jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. The strength of our algorithm lies in its ability to guide the prediction of individual rankers using rhetorical relations between aspects such as agreement and contrast. Our method yields significant empirical improvements over individual rankers as well as a state-of-the-art joint ranking model. Our current model employs a single rhetorical relation – agreement vs. contrast – to model dependencies between different opinions. As our analy6What counts as “relatively low” will depend on both the value of the tuning parameter α and the confidence of the component ranking models for a particular input x. sis shows, this relation does not provide sufficient constraints for non-consensus instances. An avenue for future research is to consider the impact of additional rhetorical relations between aspects. We also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms. The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168 and grant IIS0415865) and the Microsoft Research Faculty Fellowship. Thanks to Michael Collins, Pawan Deshpande, Jacob Eisenstein, Igor Malioutov, Luke Zettlemoyer, and the anonymous reviewers for helpful comments and suggestions. Thanks also to Vasumathi Raman for programming assistance. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.
A Multi-Pass Sieve for Coreference Resolution Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier’s entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sievebased approaches could be applied to other NLP tasks. Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation is available, even a simple deterministic model can achieve state-of-the-art performance (Haghighi and Klein, 2009). By and large most approaches decide if two mentions are coreferent using a single function over all these features and information local to the two mentions.1 This is problematic for two reasons: (1) lower precision features may overwhelm the smaller number of high precision ones, and (2) local information is often insufficient to make an informed decision. Consider this example: The second attack occurred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps. We propose an unsupervised sieve-like approach to coreference resolution that addresses these is1As we will discuss below, some approaches use an additional component to infer the overall best mention clusters for a document, but this is still based on confidence scores assigned using local information. sues. The approach applies tiers of coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, each model’s decisions are richly informed by sharing attributes across the mentions clustered in earlier tiers. This ensures that each decision uses all of the information available at the time. We implemented all components in our approach using only deterministic models. All our components are unsupervised, in the sense that they do not require training on gold coreference links. The contributions of this work are the following: • We show that a simple scaffolding framework that deploys strong features through tiers of models performs significantly better than a single-pass model. Additionally, we propose several simple, yet powerful, new features. This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To facilitate comparison with most of the recent previous work, we report results using gold mention boundaries. However, our approach does not make any assumptions about the underlying mentions, so it is trivial to adapt it to predicted mention boundaries (e.g., see Haghighi and Klein (2010) for a simple mention detection model). We used the following corpora for development and evaluation: We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. Our sieve framework is implemented as a succession of independent coreference models. We first describe how each model selects candidate mentions, and then describe the models themselves. Given a mention mi, each model may either decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m1, ..., mi−1. We sort candidate antecedents using syntactic information provided by the Stanford parser, as follows: Same Sentence – Candidates in the same sentence are sorted using left-to-right breadth-first traversal of syntactic trees (Hobbs, 1977). Figure 1 shows an example of candidate ordering based on this traversal. The left-to-right ordering favors subjects, which tend to appear closer to the beginning of the sentence and are more probable antecedents. The breadthfirst traversal promotes syntactic salience by ranking higher noun phrases that are closer to the top of the parse tree (Haghighi and Klein, 2009). If the sentence containing the anaphoric mention contains multiple clauses, we repeat the above heuristic separately in each S* constituent, starting with the one containing the mention. Previous Sentence – For all nominal mentions we sort candidates in the previous sentences using rightto-left breadth-first traversal. This guarantees syntactic salience and also favors document proximity. For pronominal mentions, we sort candidates in previous sentences using left-to-right traversal in order to favor subjects. Subjects are more probable antecedents for pronouns (Kertz et al., 2006). For example, this ordering favors the correct candidate (pepsi) for the mention they: [pepsi] says it expects to double [quaker]’s snack food growth rate. after a month-long courtship, [they] agreed to buy quaker oats... In a significant departure from previous work, each model in our framework gets (possibly incomplete) clustering information for each mention from the earlier coreference models in the multi-pass system. In other words, each mention mi may already be assigned to a cluster Cj containing a set of mentions: Cj = {mj1, ... , mj�}; mi E Cj. Unassigned mentions are unique members of their own cluster. We use this information in several ways: Attribute sharing – Pronominal coreference resolution (discussed later in this section) is severely affected by missing attributes (which introduce precision errors because incorrect antecedents are selected due to missing information) and incorrect attributes (which introduce recall errors because correct links are not generated due to attribute mismatch between mention and antecedent). To address this issue, we perform a union of all mention attributes (e.g., number, gender, animacy) in a given cluster and share the result with all cluster mentions. If attributes from different mentions contradict each other we maintain all variants. For example, our naive number detection assigns singular to the mention a group of students and plural to five students. When these mentions end up in the same cluster, the resulting number attributes becomes the set {singular, plural}. Thus this cluster can later be merged with both singular and plural pronouns. Mention selection – Traditionally, a coreference model attempts to resolve every mention in the text, which increases the likelihood of errors. Instead, in each of our models, we exploit the cluster information received from the previous stages by resolving only mentions that are currently first in textual order in their cluster. For example, given the following ordered list of mentions, {mi, m2, m3, m4, m5, m6}, where the superscript indicates cluster id, our model will attempt to resolve only m2 and m4. These two are the only mentions that have potential antecedents and are currently marked as the first mentions in their clusters. The intuition behind this heuristic is two-fold. First, early cluster mentions are usually better defined than subsequent ones, which are likely to have fewer modifiers or are pronouns (Fox, 1993). Several of our models use this modifier information. Second, by definition, first mentions appear closer to the beginning of the document, hence there are fewer antecedent candidates to select from, and fewer opportunities to make a mistake. Search Pruning – Finally, we prune the search space using discourse salience. We disable coreference for first cluster mentions that: (a) are or start with indefinite pronouns (e.g., some, other), or (b) start with indefinite articles (e.g., a, an). One exception to this rule is the model deployed in the first pass; it only links mentions if their entire extents match exactly. This model is triggered for all nominal mentions regardless of discourse salience, because it is possible that indefinite mentions are repeated in a document when concepts are discussed but not instantiated, e.g., a sports bar below: We now describe the coreference models implemented in the sieve. For clarity, we summarize them in Table 1 and show the cumulative performance as they are added to the sieve in Table 2. This model links two mentions only if they contain exactly the same extent text, including modifiers and determiners, e.g., the Shahab 3 ground-ground missile. As expected, this model is extremely precise, with a pairwise precision over 96%. This model links two mentions if any of the conditions below are satisfied: Appositive – the two nominal mentions are in an appositive construction, e.g., [Israel’s Deputy Defense Minister], [Ephraim Sneh] , said ... We use the same syntactic rules to detect appositions as Haghighi and Klein (2009). Predicate nominative – the two mentions (nominal or pronominal) are in a copulative subject-object relation, e.g., [The New York-based College Board] is [a nonprofit organization that administers the SATs and promotes higher education] (Poon and Domingos, 2008). Role appositive – the candidate antecedent is headed by a noun and appears as a modifier in an NP whose head is the current mention, e.g., [[actress] Rebecca Schaeffer]. This feature is inspired by Haghighi and Klein (2009), who triggered it only if the mention is labeled as a person by the NER. We constrain this heuristic more in our work: we allow this feature to match only if: (a) the mention is labeled as a person, (b) the antecedent is animate (we detail animacy detection in Pass 7), and (c) the antecedent’s gender is not neutral. Relative pronoun – the mention is a relative pronoun that modifies the head of the antecedent NP, e.g., [the finance street [which] has already formed in the Waitan district]. Acronym – both mentions are tagged as NNP and one of them is an acronym of the other, e.g., [Agence France Presse] ... [AFP]. We use a simple acronym detection algorithm, which marks a mention as an acronym of another if its text equals the sequence of upper case characters in the other mention. We will adopt better solutions for acronym detection in future work (Schwartz, 2003). Demonym – one of the mentions is a demonym of the other, e.g., [Israel] ... [Israeli]. For demonym detection we use a static list of countries and their gentilic forms from Wikipedia.3 All the above features are extremely precise. As shown in Table 2 the pairwise precision of the sieve after adding these features is over 95% and recall increases 5 points. Linking a mention to an antecedent based on the naive matching of their head words generates a lot of spurious links because it completely ignores possibly incompatible modifiers (Elsner and Charniak, 2010). For example, Yale University and Harvard University have similar head words, but they are obviously different entities. To address this issue, this pass implements several features that must all be matched in order to yield a link: Cluster head match – the mention head word matches any head word in the antecedent cluster. Note that this feature is actually more relaxed than naive head matching between mention and antecedent candidate because it is satisfied when the mention’s head matches the head of any entity in the candidate’s cluster. We constrain this feature by enforcing a conjunction with the features below. Word inclusion – all the non-stop4 words in the mention cluster are included in the set of non-stop words in the cluster of the antecedent candidate. This heuristic exploits the property of discourse that it is uncommon to introduce novel information in later mentions (Fox, 1993). Typically, mentions of the same entity become shorter and less informative as the narrative progresses. For example, the two mentions in ... intervene in the [Florida Supreme Court]’s move ... does look like very dramatic change made by [the Florida court] point to the same entity, but the two mentions in the text below belong to different clusters: The pilot had confirmed ... he had turned onto [the correct runway] but pilots behind him say he turned onto [the wrong runway]. Compatible modifiers only – the mention’s modifiers are all included in the modifiers of the antecedent candidate. This feature models the same discourse property as the previous feature, but it focuses on the two individual mentions to be linked, rather than their entire clusters. For this feature we only use modifiers that are nouns or adjectives. Not i-within-i – the two mentions are not in an iwithin-i construct, i.e., one cannot be a child NP in the other’s NP constituent (Haghighi and Klein, 2009). This pass continues to maintain high precision (91% pairwise) while improving recall significantly (over 6 points pairwise and almost 8 points MUC). Passes 4 and 5 are different relaxations of the feature conjunction introduced in Pass 3, i.e., Pass 4 removes the compatible modifiers only feature, while Pass 5 removes the word inclusion constraint. All in all, these two passes yield an improvement of 1.7 pairwise F1 points, due to recall improvements. Table 2 shows that the word inclusion feature is more precise than compatible modifiers only, but the latter has better recall. This pass relaxes the cluster head match heuristic by allowing the mention head to match any word in the cluster of the candidate antecedent. For example, this heuristic matches the mention Sanders to a cluster containing the mentions {Sauls, the judge, Circuit Judge N. Sanders Sauls}. To maintain high precision, this pass requires that both mention and antecedent be labeled as named entities and the types coincide. Furthermore, this pass implements a conjunction of the above features with word inclusion and not i-within-i. This pass yields less than 1 point improvement in most metrics. With one exception (Pass 2), all the previous coreference models focus on nominal coreference resolution. However, it would be incorrect to say that our framework ignores pronominal coreference in the first six passes. In fact, the previous models prepare the stage for pronominal coreference by constructing precise clusters with shared mention attributes. These are crucial factors for pronominal coreference. Like previous work, we implement pronominal coreference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009). NER label – from the Stanford NER. If we cannot detect a value, we set attributes to unknown and treat them as wildcards, i.e., they can match any other value. This final model raises the pairwise recall of our system almost 22 percentage points, with only an 8 point drop in pairwise precision. Table 2 shows that similar behavior is measured for all other metrics. After all passes have run, we take the transitive closure of the generated clusters as the system output. We present the results of our approach and other relevant prior work in Table 3. We include in the table all recent systems that report results under the same conditions as our experimental setup (i.e., using gold mentions) and use the same corpora. We exclude from this analysis two notable works that report results only on a version of the task that includes finding mentions (Haghighi and Klein, 2010; Stoyanov, 2010). The Haghighi and Klein (2009) numbers have two variants: with semantics (+S) and without (−S). To measure the contribution of our multi-pass system, we also present results from a single-pass variant of our system that uses all applicable features from the multi-pass system (marked as “single pass” in the table). Our sieve model outperforms all systems on two out of the four evaluation corpora (ACE2004ROTH-DEV and ACE2004-NWIRE), on all metrics. On the corpora where our model is not best, it ranks a close second. For example, in ACE2004CULOTTA-TEST our system has a B3 F1 score only .4 points lower than Bengston and Roth (2008) and it outperforms all unsupervised approaches. In MUC6-TEST, our sieve’s B3 F1 score is 1.8 points lower than Haghighi and Klein (2009) +S, but it outperforms a supervised system that used gold named entity labels. Finally, the multi-pass architecture always beats the equivalent single-pass system with its contribution ranging between 1 and 4 F1 points depending on the corpus and evaluation metric. Our approach has the highest precision on all corpora, regardless of evaluation metric. We believe this is particularly useful for large-scale NLP applications that use coreference resolution components, e.g., question answering or information extraction. These applications can generally function without coreference information so it is beneficial to provide such information only when it is highly precise. The sieve model outperforms all other systems on at least two test sets, even though most of the other models are significantly richer. Amongst the comparisons, several are supervised (Bengston and Roth, 2008; Finkel and Manning, 2008; Culotta et al., 2007). The system of Haghighi and Klein (2009) +S uses a lexicon of semantically-compatible noun pairs acquired transductively, i.e., with knowledge of the mentions in the test set. Our system does not rely on labeled corpora for training (like supervised approaches) nor access to corpora during testing (like Haghighi and Klein (2009)). The system that is closest to ours is Haghighi and Klein (2009) −S. Like us, they use a rich set of features and deterministic decisions. However, theirs is a single-pass model with a smaller feature set (no cluster-level, acronym, demonym, or animacy information). Table 3 shows that on the two corpora where results for this system are available, we outperform it considerably on all metrics. To understand if the difference is due to the multi-pass architecture or the richer feature set we compared (Haghighi and Klein, 2009) −S against both our multi-pass system and its single-pass variant. The comparison indicates that both these contributions help: our single-pass system outperforms Haghighi and Klein (2009) consistently, and the multi-pass architecture further improves the performance of our single-pass system between 1 and 4 F1 points, depending on the corpus and evaluation metric. Recent unsupervised coreference work from Haghighi and Klein (2009) included a novel semantic component that matched related head words (e.g., AOL is a company) learned from select wikipedia articles. They first identified articles relevant to the entity mentions in the test set, and then bootstrapped from known syntactic patterns for apposition and predicate-nominatives in order to learn a database of related head pairs. They show impressive gains by using these learned pairs in coreference decisions. This type of learning using test set mentions is often described as transductive. Our work instead focuses on an approach that does not require access to the dataset beforehand. We thus did not include a similar semantic component in our system, given that running a bootstrapping learner whenever a new data set is encountered is not practical and, ultimately, reduces the usability of this NLP component. However, our results show that our sieve algorithm with minimal semantic information still performs as well as the Haghighi and Klein (2009) system with semantics. The sieve architecture offers benefits beyond improved accuracy. Its modular design provides a flexibility for features that is not available in most supervised or unsupervised systems. The sieve allows new features to be seamlessly inserted without affecting (or even understanding) the other components. For instance, once a new high precision feature (or group of features) is inserted as its own stage, it will benefit later stages with more precise clusters, but it will not interfere with their particular algorithmic decisions. This flexibility is in sharp contrast to supervised classifiers that require their models to be retrained on labeled data, and unsupervised systems that do not offer a clear insertion point for new features. It can be difficult to fully understand how a system makes a single decision, but the sieve allows for flexible usage with minimal effort. Table 4 shows the number of incorrect pair-wise links generated by our system on the MUC6-TEST corpus. The table indicates that most of our errors are for nominal mentions. For example, the combined (precision plus recall) number of errors for proper or common noun mentions is three times larger than the number of errors made for pronominal mentions. The table also highlights that most of our errors are recall errors. There are eight times more recall errors than precision errors in our output. This is a consequence of our decision to prioritize highly precise features in the sieve. The above analysis illustrates that our next effort should focus on improving recall. In order to understand the limitations of our current system, we randomly selected 60 recall errors (20 for each mention type) and investigated their causes. Not surprisingly, the causes are unique to each type. For proper nouns, 50% of recall errors are due to mention lengthening, mentions that are longer than their earlier mentions. For example, Washingtonbased USAir appears after USAir in the text, so our head matching components skip it because their high precision depends on disallowing new modifiers as the discourse proceeds. When the mentions were reversed (as is the usual case), they match. The common noun recall errors are very different from proper nouns: 17 of the 20 random examples can be classified as semantic knowledge. These errors are roughly evenly split between recognizing categories of names (e.g., Gitano is an organization name hence it should match the nominal antecedent the company), and understanding hypernym relations like settlements and agreements. Pronoun errors come in two forms. Roughly 40% of these errors are attribute mismatches involving sometimes ambiguous uses of gender and number (e.g., she with Pat Carney). Another 40% are not semantic or attribute-based, but rather simply arise due to the order in which we check potential antecedents. In all these situations, the correct links are missed because the system chooses a closer (incorrect) antecedent. These four highlighted errors (lengthening, semantics, attributes, ordering) add up to 77% of all recall errors in the selected set. In general, each error type is particular to a specific mention type. This suggests that recall improvements can be made by focusing on one mention type without aversely affecting the others. Our sieve-based approach to coreference uniquely allows for such new models to be seamlessly inserted. We presented a simple deterministic approach to coreference resolution that incorporates documentlevel information, which is typically exploited only by more complex, joint learning models. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model’s cluster output. Despite its simplicity, our approach outperforms or performs comparably to the state of the art on several corpora. An additional benefit of the sieve framework is its modularity: new features or models can be inserted in the system with limited understanding of the other features already deployed. Our code is publicly released5 and can be used both as a stand-alone coreference system and as a platform for the development of future systems. The strong performance of our system suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared; likely candidates include relation and event extraction, template slot filling, and author name deduplication. We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government. Many thanks to Jenny Finkel for writing a reimplementation of much of Haghighi and Klein (2009), which served as the starting point for the work reported here. We also thank Nicholas Rizzolo and Dan Roth for helping us replicate their experimental setup, and Heng Ji and Dekang Lin for providing their gender lexicon.
The Kappa Statistic: A Second Look dialogue structure coding scheme. 23(1):13–31. Cicchetti, Domenic V. and Alvan R. Feinstein. 1990. High agreement but low kappa: II. Resolving the paradoxes. of Clinical 43(6):551–558. Cohen, Jacob. 1960. A coefficient of for nominal scales. In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks. In this squib, we highlight issues that affect κ and that the community has largely neglected. First, we discuss the assumptions underlying different computations of the expected agreement component of κ. Second, we discuss how prevalence and bias affect the κ measure. In the last few years, coded corpora have acquired an increasing importance in every aspect of human-language technology. Tagging for many phenomena, such as dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). Carletta (1996) deserves the credit for bringing κ to the attention of computational linguists. κ is computed as P(A) − P(E) 1 − P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1,1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational linguistics literature. First, there are two main ways of computing P(E), the expected agreement, according to whether the distribution of proportions over the categories is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different conceptualizations of the problem. We believe the distinction between the two is often glossed over because in practice the two computations of P(E) produce very similar outcomes in most cases, especially for the highest values of κ. However, first, we will show that they can indeed result in different values of κ, that we will call κCo (Cohen 1960) and κS&C (Siegel and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreement. Moreover, the assumption of equal distributions over the categories masks the exact source of disagreement among the coders. Thus, such an assumption is detrimental if such systematic disagreements are to be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). Second, κ is affected by skewed distributions of categories (the prevalence problem) and by the degree to which the coders disagree (the bias problem). That is, for a fixed P(A), the values of κ vary substantially in the presence of prevalence, bias, or both. We will conclude by suggesting that κCo is a better choice than κS&C in those studies in which the assumption of equal distributions underlying κS&C does not hold: the vast majority, if not all, of discourse- and dialogue-tagging efforts. However, as κCo suffers from the bias problem but κS&C does not, κS&C should be reported too, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the first, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&C may straddle one of the significant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrences as Accept, and another 55 as Ack. They disagree on 25 occurrences, which one coder labels as Ack, and the other as Accept. In Figure 1, this example is encoded by the top contingency table on the left (labeled Example 1) and the agreement table on the right. The contingency table directly mirrors our description. The agreement table is an N × m matrix, where N is the number of items in the data set and m is the number of labels that can be assigned to each object; in our example, N = 150 and m = 2. Each entry nij is the number of codings of label j to item i. The agreement table in Figure 1 shows that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through 125 as Ack by both coders, and 126 to 150 differ in their labels. 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a statistic called alpha. Krippendorff computes P(E) (called 1 − De in his terminology) with a sampling-without-replacement methodology. The computations of P(E) and of 1 − De show that the difference is negligible: Cohen’s contingency tables (left) and Siegel and Castellan’s agreement table (right). Agreement tables lose information. When the coders disagree, we cannot reconstruct which coder picked which category. Consider Example 2 in Figure 1. The two coders still disagree on 25 occurrences of Okay. However, one coder now labels 10 of those as Accept and the remaining 15 as Ack, whereas the other labels the same 10 as Ack and the same 15 as Accept. The agreement table does not change, but the contingency table does. Turning now to computing P(E), Figure 2 shows, for Example 1, Cohen’s computation of P(E) on the left, and Siegel and Castellan’s computation on the right. We include the computations of KCo and KS&C as the last step. For both Cohen and Siegel and Castellan, P(A) = 125/150 = 0.8333. The observed agreement P(A) is computed as the proportion of items the coders agree on to the total number of items; N is the number of items, and k the number of coders (N = 150 and k = 2 in our example). Both KCo and KS&C are highly significant at the p = 0.5 ∗ 10−5 level (significance is computed for KCo and KS&C according to the formulas in Cohen [1960] and Siegel and Castellan [1988], respectively). The difference between KCo and KS&C in Figure 2 is just under 1%, however, the results of the two K computations straddle the value 0.67, which for better or worse has been adopted as a cutoff in computational linguistics. This cutoff is based on the assessment of K values in Krippendorff (1980), which discounts K < 0.67 and allows tentative conclusions when 0.67 < K < 0.8 and definite conclusions when K ≥ 0.8. Krippendorff’s scale has been adopted without question, even though Krippendorff himself considers it only a plausible standard that has emerged from his and his colleagues’ work. In fact, Carletta et al. (1997) use words of caution against adopting Krippendorff’s suggestion as a standard; the first author has also raised the issue of how to assess K values in Di Eugenio (2000). If Krippendorff’s scale is supposed to be our standard, the example just worked out shows that the different computations of P(E) do affect the assessment of intercoder agreement. If less-strict scales are adopted, the discrepancies between the two K computations play a larger role, as they have a larger effect on smaller values of K. For example, Rietveld and van Hout (1993) consider 0.20 < K < 0.40 as indicating fair agreement, and 0.40 < K < 0.60 as indicating moderate agreement. Suppose that two coders are coding 100 occurrences of Okay. The two coders label 40 occurrences as Accept and 25 as Ack. The remaining 35 are labeled as Ack by one coder and as Accept by the other (as in Example 6 in Figure 4); KCo = 0.418, but KS&C = 0.27. These two values are really at odds. Step 1. For each category j, compute the overall proportion pj,l of items assigned to j by each coder l. In a contingency table, each row and column total divided by N corresponds to one such proportion for the corresponding coder. Assumption of equal distributions among coders (Siegel and Castellan) Step 1. For each category j, compute pj, the overall proportion of items assigned to j. In an agreement table, the column totals give the total counts for each category j, hence: Step 3. P(E), the likelihood of coders’ accidentally assigning the same category to a given item, is The computation of P(E) and κ according to Cohen (left) and to Siegel and Castellan (right). In the computational linguistics literature, r. has been used mostly to validate coding schemes: Namely, a “good” value of r. means that the coders agree on the categories and therefore that those categories are “real.” We noted previously that assessing what constitutes a “good” value for r. is problematic in itself and that different scales have been proposed. The problem is compounded by the following obvious effect on r. values: If P(A) is kept constant, varying values for P(E) yield varying values of r.. What can affect P(E) even if P(A) is constant are prevalence and bias. The prevalence problem arises because skewing the distribution of categories in the data increases P(E). The minimum value P(E) = 1/m occurs when the labels are equally distributed among the m categories (see Example 4 in Figure 3). The maximum value P(E) = 1 occurs when the labels are all concentrated in a single category. But for a given value of P(A), the larger the value of P(E), the lower the value of r.. Example 3 and Example 4 in Figure 3 show two coders agreeing on 90 out of 100 occurrences of Okay, that is, P(A) = 0.9. However, r. ranges from −0.048 to 0.80, and from not significant to significant (the values of r.S&C for Examples 3 and 4 are the same as the values of r.Co).3 The differences in r. are due to the difference in the relative prevalence of the two categories Accept and Ack. In Example 3, the distribution is skewed, as there are 190 Accepts but only 10 Acks across the two coders; in Example 4, the distribution is even, as there are 100 Accepts and 100 Acks, respectively. These results do not depend on the size of the sample; that is, they are not due to the fact Contingency tables illustrating the bias effect on κCo. Example 3 and Example 4 are small. As the computations of P(A) and P(E) are based on proportions, the same distributions of categories in a much larger sample, say, 10,000 items, will result in exactly the same κ values. Although this behavior follows squarely from κ’s definition, it is at odds with using κ to assess a coding scheme. From both Example 3 and Example 4 we would like to conclude that the two coders are in substantial agreement, independent of the skewed prevalence of Accept with respect to Ack in Example 3. The role of prevalence in assessing κ has been subject to heated discussion in the medical literature (Grove et al. 1981; Berry 1992; Goldman 1992). The bias problem occurs in κCo but not κS&C. For κCo, P(E) is computed from each coder’s individual probabilities. Thus, the less two coders agree in their overall behavior, the fewer chance agreements are expected. But for a given value of P(A), decreasing P(E) will increase κCo, leading to the paradox that κCo increases as the coders become less similar, that is, as the marginal totals diverge in the contingency table. Consider two coders coding the usual 100 occurrences of Okay, according to the two tables in Figure 4. In Example 5, the proportions of each category are very similar among coders, at 55 versus 60 Accept, and 45 versus 40 Ack. However, in Example 6 coder 1 favors Accept much more than coder 2 (75 versus 40 occurrences) and conversely chooses Ack much less frequently (25 versus 60 occurrences). In both cases, P(A) is 0.65 and κS&C is stable at 0.27, but κCo goes from 0.27 to 0.418. Our initial example in Figure 1 is also affected by bias. The distribution in Example 1 yielded κCo = 0.6724 but κS&C = 0.6632. If the bias decreases as in Example 2, κCo becomes 0.6632, the same as κS&C. The issue that remains open is which computation of κ to choose. Siegel and Castellan’s κS&C is not affected by bias, whereas Cohen’s κCo is. However, it is questionable whether the assumption of equal distributions underlying κS&C is appropriate for coding in discourse and dialogue work. In fact, it appears to us that it holds in few if any of the published discourse- or dialogue-tagging efforts for which κ has been computed. It is, for example, appropriate in situations in which item i may be tagged by different coders than item j (Fleiss 1971). However, κ assessments for discourse and dialogue tagging are most often performed on the same portion of the data, which has been annotated by each of a small number of annotators (between two and four). In fact, in many cases the analysis of systematic disagreements among annotators on the same portion of the data (i.e., of bias) can be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). To use κCo but to guard against bias, Cicchetti and Feinstein (1990) suggest that κCo be supplemented, for each coding category, by two measures of agreement, positive and negative, between the coders. This means a total of 2m additional measures, which we believe are too many to gain a general insight into the meaning of the specific κCo value. Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliability be reported as three numbers: κCo and two adjustments of κCo, one with bias removed, the other with prevalence removed. The value of κCo adjusted for bias turns out to be ... κS&C. Adjusted for prevalence, κCo yields a measure that is equal to 2P(A) − 1. The results for Example 1 should then be reported as κCo = 0.6724, κS&C = 0.6632, 2P(A)−1 = 0.6666; those for Example 6 as κCo = 0.418, κS&C = 0.27, and 2P(A)−1 = 0.3. For both Examples 3 and 4, 2P(A) − 1 = 0.8. Collectively, these three numbers appear to provide a means of better judging the meaning of κ values. Reporting both κ and 2P(A) − 1 may seem contradictory, as 2P(A) − 1 does not correct for expected agreement. However, when the distribution of categories is skewed, this highlights the effect of prevalence. Reporting both κCo and κS&C does not invalidate our previous discussion, as we believe κCo is more appropriate for discourse- and dialogue-tagging in the majority of cases, especially when exploiting bias to improve coding (Wiebe, Bruce, and O’Hara 1999). This work is supported by grant N00014-00-1-0640 from the Office of Naval Research. Thanks to Janet Cahn and to the anonymous reviewers for comments on earlier drafts.
Dependency Treelet Translation: Syntactically Informed Phrasal SMT We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. Over the past decade, we have witnessed a revolution in the field of machine translation (MT) toward statistical or corpus-based methods. Yet despite this success, statistical machine translation (SMT) has many hurdles to overcome. While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04). State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words. Arbitrary reordering of words is allowed within memorized phrases, but typically only a small amount of phrase reordering is allowed, modeled in terms of offset positions at the string level. This reordering model is very limited in terms of linguistic generalizations. For instance, when translating English to Japanese, an ideal system would automatically learn largescale typological differences: English SVO clauses generally become Japanese SOV clauses, English post-modifying prepositional phrases become Japanese pre-modifying postpositional phrases, etc. A phrasal SMT system may learn the internal reordering of specific common phrases, but it cannot generalize to unseen phrases that share the same linguistic structure. In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne...pas except in the context of specific intervening words. The hope in the SMT community has been that the incorporation of syntax would address these issues, but that promise has yet to be realized. One simple means of incorporating syntax into SMT is by re-ranking the n-best list of a baseline SMT system using various syntactic models, but Och et al. (04) found very little positive impact with this approach. However, an n-best list of even 16,000 translations captures only a tiny fraction of the ordering possibilities of a 20 word sentence; re-ranking provides the syntactic model no opportunity to boost or prune large sections of that search space. Inversion Transduction Grammars (Wu, 97), or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar. To make this process computationally efficient, however, some severe simplifying assumptions are made, such as using a single non-terminal label. This results in the model simply learning a very high level preference regarding how often nodes should switch order without any contextual information. Also these translation models are intrinsically word-based; phrasal combinations are not modeled directly, and results have not been competitive with the top phrasal SMT systems. Along similar lines, Alshawi et al. (2000) treat translation as a process of simultaneous induction of source and target dependency trees using headtransduction; again, no separate parser is used. Yamada and Knight (01) employ a parser in the target language to train probabilities on a set of operations that convert a target language tree to a source language string. This improves fluency slightly (Charniak et al., 03), but fails to significantly impact overall translation quality. This may be because the parser is applied to MT output, which is notoriously unlike native language, and no additional insight is gained via source language analysis. Lin (04) translates dependency trees using paths. This is the first attempt to incorporate large phrasal SMT-style memorized patterns together with a separate source dependency parser and SMT models. However the phrases are limited to linear paths in the tree, the only SMT model used is a maximum likelihood channel model and there is no ordering model. Reported BLEU scores are far below the leading phrasal SMT systems. MSR-MT (Menezes & Richardson, 01) parses both source and target languages to obtain a logical form (LF), and translates source LFs using memorized aligned LF patterns to produce a target LF. It utilizes a separate sentence realization component (Ringger et al., 04) to turn this into a target sentence. As such, it does not use a target language model during decoding, relying instead on MLE channel probabilities and heuristics such as pattern size. Recently Aue et al. (04) incorporated an LF-based language model (LM) into the system for a small quality boost. A key disadvantage of this approach and related work (Ding & Palmer, 02) is that it requires a parser in both languages, which severely limits the language pairs that can be addressed. In this paper we propose a novel dependency treebased approach to phrasal SMT which uses treebased ‘phrases’ and a tree-based ordering model in combination with conventional SMT models to produce state-of-the-art translations. Our system employs a source-language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from a parallel sentence-aligned corpus. We begin by parsing the source text to obtain dependency trees and word-segmenting the target side, then applying an off-the-shelf word alignment component to the bitext. The word alignments are used to project the source dependency parses onto the target sentences. From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree. A unique feature is that we allow treelets with a wildcard root, effectively allowing mappings for siblings in the dependency tree. This allows us to model important phenomena, such as not ... ne...pas. We also train a variety of statistical models on this aligned dependency tree corpus, including a channel model and an order model. To translate an input sentence, we parse the sentence, producing a dependency tree for that sentence. We then employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models that are combined in a log-linear framework as in (Och, 03). This approach offers the following advantages over string-based SMT systems: Instead of limiting learned phrases to contiguous word sequences, we allow translation by all possible phrases that form connected subgraphs (treelets) in the source and target dependency trees. This is a powerful extension: the vast majority of surface-contiguous phrases are also treelets of the tree; in addition, we gain discontiguous phrases, including combinations such as verb-object, article-noun, adjective-noun etc. regardless of the number of intervening words. Another major advantage is the ability to employ more powerful models for reordering source language constituents. These models can incorporate information from the source analysis. For example, we may model directly the probability that the translation of an object of a preposition in English should precede the corresponding postposition in Japanese, or the probability that a pre-modifying adjective in English translates into a post-modifier in French. We require a source language dependency parser that produces unlabeled, ordered dependency trees and annotates each source word with a partof-speech (POS). An example dependency tree is shown in Figure 1. The arrows indicate the head annotation, and the POS for each candidate is listed underneath. For the target language we only require word segmentation. To obtain word alignments we currently use GIZA++ (Och & Ney, 03). We follow the common practice of deriving many-to-many alignments by running the IBM models in both directions and combining the results heuristically. Our heuristics differ in that they constrain manyto-one alignments to be contiguous in the source dependency tree. A detailed description of these heuristics can be found in Quirk et al. (04). Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence. Oneto-one alignments project directly to create a target tree isomorphic to the source. Many-to-one alignments project similarly; since the ‘many’ source nodes are connected in the tree, they act as if condensed into a single node. In the case of one-to-many alignments we project the source node to the rightmost2 of the ‘many’ target words, and make the rest of the target words dependent on it. 2 If the target language is Japanese, leftmost may be more appropriate. startup properties and options propriétés et options de démarrage Unaligned target words3 are attached into the dependency structure as follows: assume there is an unaligned word tj in position j. Let i < j and k > j be the target positions closest to j such that ti depends on tk or vice versa: attach tj to the lower of ti or tk. If all the nodes to the left (or right) of position j are unaligned, attach tj to the left-most (or right-most) word that is aligned. The target dependency tree created in this process may not read off in the same order as the target string, since our alignments do not enforce phrasal cohesion. For instance, consider the projection of the parse in Figure 1 using the word alignment in Figure 2a. Our algorithm produces the dependency tree in Figure 2b. If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string: de démarrage appears in the wrong place. A second reattachment pass corrects this situation. For each node in the wrong order, we reattach it to the lowest of its ancestors such that it is in the correct place relative to its siblings and parent. In Figure 2c, reattaching démarrage to et suffices to produce the correct order. From the aligned pairs of dependency trees we extract all pairs of aligned source and target treelets along with word-level alignment linkages, up to a configurable maximum size. We also keep treelet counts for maximum likelihood estimation. Phrasal SMT systems often use a model to score the ordering of a set of phrases. One approach is to penalize any deviation from monotone decoding; another is to estimate the probability that a source phrase in position i translates to a target phrase in position j (Koehn et al., 03). We attempt to improve on these approaches by incorporating syntactic information. Our model assigns a probability to the order of a target tree given a source tree. Under the assumption that constituents generally move as a whole, we predict the probability of each given ordering of modifiers independently. That is, we make the following simplifying assumption (where c is a function returning the set of nodes modifying t): Furthermore, we assume that the position of each child can be modeled independently in terms of a head-relative position: Figure 3a demonstrates an aligned dependency tree pair annotated with head-relative positions; Figure 3b presents the same information in an alternate tree-like representation. We currently use a small set of features reflecting very local information in the dependency tree to model P(pos(m,t)  |S, T): As an example, consider the children of propriété in Figure 3. The head-relative positions One can also include features of siblings to produce a Markov ordering model. However, we found that this had litt of its modifiers la and Cancel are -1 and +1, respectively. Thus we try to predict as follows: Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03). The MLE model effectively captures non-literal phrasal translations such as idioms, but suffers fr om data sparsity. The wordto-word model does not typically suffer from data sparsity, but prefers more literal translations. Given a set of treelet translation pairs that cover a given input dependency tree and produce a target dependency tree, we model the probability of source given target as the product of the individual treelet translation probabilities: we assume a uniform probability distribution over the decompositions of a tree into treelets. Target Model: Given an ordered target language dependency tree, it is trivial to read off the surface string. We evaluate this string using a trigram model with modified Kneser-Ney smoothing. Miscellaneous Feature Functions: The log-linear framework allows us to incorporate other feature functions as ‘models’ in the translation process. For instance, using fewer, larger treelet translation pairs often provides better translations, since they capture more context and allow fewer possibilities for search and model error. Therefore we add a feature function that counts the number of phrases used. We also add a feature that counts the number of target words; this acts as an insertion/deletion bonus/penalty. The challenge of tree-based decoding is that the traditional left-to-right decoding approach of string-based systems is inapplicable. Additional challenges are posed by the need to handle treelets—perhaps discontiguous or overlapping— and a combinatorially explosive ordering space. Our decoding approach is influenced by ITG (Wu, 97) with several important extensions. First, we employ treelet translation pairs instead of single word translations. Second, instead of modeling rearrangements as either preserving source order or swapping source order, we allow the dependents of a node to be ordered in any arbitrary manner and use the order model described in section 2.4 to estimate probabilities. Finally, we use a log-linear framework for model combination that allows any amount of other information to be modeled. We will initially approach the decoding problem as a bottom up, exhaustive search. We define the set of all possible treelet translation pairs of the subtree rooted at each input node in the following manner: A treelet translation pair x is said to match the input dependency tree S iff there is some connected subgraph S’ that is identical to the source side of x. We say that x covers all the nodes in S’ and is rooted at source node s, where s is the root of matched subgraph S’. We first find all treelet translation pairs that match the input dependency tree. Each matched pair is placed on a list associated with the input node where the match is rooted. Moving bottomup through the input dependency tree, we compute a list of candidate translations for the input subtree rooted at each node s, as follows: Consider in turn each treelet translation pair x rooted at s. The treelet pair x may cover only a portion of the input subtree rooted at s. Find all descendents s' of s that are not covered by x, but whose parent s&quot; is covered by x. At each such node s&quot; look at all interleavings of the children of s&quot; specified by x, if any, with each translation t' from the candidate translation list5 of each child s'. Each such interleaving is scored using the models previously described and added to the candidate translation list for that input node. The resultant translation is the best scoring candidate for the root input node. As an example, see the example dependency tree in Figure 4a and treelet translation pair in 4b. This treelet translation pair covers all the nodes in 4a except the subtrees rooted at software and is. We first compute (and cache) the candidate translation lists for the subtrees rooted at software and is, then construct full translation candidates by attaching those subtree translations to installés in all possible ways. The order of sur relative to installés is fixed; it remains to place the translated subtrees for the software and is. Note that if c is the count of children specified in the mapping and r is the count of subtrees translated via recursive calls, then there are (c+r+1)!/(c+1)! orderings. Thus (1+2+1)!/(1+1)! = 12 candidate translations are produced for each combination of translations of the software and is. Converting this exhaustive search to dynamic programming relies on the observation that scoring a translation candidate at a node depends on the following information from its descendents: the order model requires features from the root of a translated subtree, and the target language model is affected by the first and last two words in each subtree. Therefore, we need to keep the best scoring translation candidate for a given subtree for each combination of (head, leading bigram, trailing bigram), which is, in the worst case, O(V5), where V is the vocabulary size. The dynamic programming approach therefore does not allow for great savings in practice because a trigram target language model forces consideration of context external to each subtree. To eliminate unnecessary ordering operations, we first check that a given set of words has not been previously ordered by the decoder. We use an order-independent hash table where two trees are considered equal if they have the same tree structure and lexical choices after sorting each child list into a canonical order. A simpler alternate approach would be to compare bags-ofwords. However since our possible orderings are bound by the induced tree structure, we might overzealously prune a candidate with a different tree structure that allows a better target order. The following optimizations do not preserve optimality, but work well in practice. Instead of keeping the full list of translation candidates for a given input node, we keep a topscoring subset of the candidates. While the decoder is no longer guaranteed to find the optimal translation, in practice the quality impact is minimal with a list size 10 (see Table 5.6). Variable-sized n-best lists: A further speedup can be obtained by noting that the number of translations using a given treelet pair is exponential in the number of subtrees of the input not covered by that pair. To limit this explosion we vary the size of the n-best list on any recursive call in inverse proportion to the number of subtrees uncovered by the current treelet. This has the intuitive appeal of allowing a more thorough exploration of large treelet translation pairs (that are likely to result in better translations) than of smaller, less promising pairs. Channel model scores and treelet size are powerful predictors of translation quality. Heuristically pruning low scoring treelet translation pairs before the search starts allows the decoder to focus on combinations and orderings of high quality treelet pairs. translation pairs rooted at that node, as ranked first by size, then by MLE channel model score, then by Model 1 score. The impact of this optimization is explored in Table 5.6. The complexity of the ordering step at each node grows with the factorial of the number of children to be ordered. This can be tamed by noting that given a fixed pre- and post-modifier count, our order model is capable of evaluating a single ordering decision independently from other ordering decisions. One version of the decoder takes advantage of this to severely limit the number of ordering possibilities considered. Instead of considering all interleavings, it considers each potential modifier position in turn, greedily picking the most probable child for that slot, moving on to the next slot, picking the most probable among the remaining children for that slot and so on. The complexity of greedy ordering is linear, but at the cost of a noticeable drop in BLEU score (see Table 5.4). Under default settings our system tries to decode a sentence with exhaustive ordering until a specified timeout, at which point it falls back to greedy ordering. We evaluated the translation quality of the system using the BLEU metric (Papineni et al., 02) under a variety of configurations. We compared against two radically different types of systems to demonstrate the competitiveness of this approach: We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data (e.g., support articles, product documentation). We selected a cleaner subset of this data by eliminating sentences with XML or HTML tags as well as very long (>160 characters) and very short (<40 characters) sentences. We held out 2,000 sentences for development testing and parameter tuning, 10,000 sentences for testing, and 250 sentences for lambda training. We ran experiments on subsets of the training data ranging from 1,000 to 300,000 sentences. Table 4.1 presents details about this dataset. We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser developed at Microsoft Research able to produce syntactic analyses at varying levels of depth (Heidorn, 02). For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed surface words. For word alignment, we used GIZA++, following a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions. We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above. The target language model was trained using only the French side of the corpus; additional data may improve its performance. Finally we trained lambdas via Maximum BLEU (Och, 03) on 250 held-out sentences with a single reference translation, and tuned the decoder optimization parameters (n-best list size, timeouts etc) on the development test set. The same GIZA++ alignments as above were used in the Pharaoh decoder. We used the heuristic combination described in (Och & Ney, 03) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 03). Except for the order model (Pharaoh uses its own ordering approach), the same models were used: MLE channel model, Model 1 channel model, target language model, phrase count, and word count. Lambdas were trained in the same manner (Och, 03). MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data. MSR-MT does not use lambdas or a target language model. We present BLEU scores on an unseen 10,000 sentence test set using a single reference translation for each sentence. Speed numbers are the end-to-end translation speed in sentences per minute. All results are based on a training set size of 100,000 sentences and a phrase size of 4, except Table 5.2 which varies the phrase size and Table 5.3 which varies the training set size. Results for our system and the comparison systems are presented in Table 5.1. Pharaoh monotone refers to Pharaoh with phrase reordering disabled. The difference between Pharaoh and the Treelet system is significant at the 99% confidence level under a two-tailed paired t-test. Table 5.2 compares Pharaoh and the Treelet system at different phrase sizes. While all the differences are statistically significant at the 99% confidence level, the wide gap at smaller phrase sizes is particularly striking. We infer that whereas Pharaoh depends heavily on long phrases to encapsulate reordering, our dependency treebased ordering model enables credible performance even with single-word ‘phrases’. We conjecture that in a language pair with large-scale ordering differences, such as English-Japanese, even long phrases are unlikely to capture the necessary reorderings, whereas our tree-based ordering model may prove more robust. Table 5.3 compares the same systems at different training corpus sizes. All of the differences are statistically significant at the 99% confidence level. Noting that the gap widens at smaller corpus sizes, we suggest that our tree-based approach is more suitable than string-based phrasal SMT when translating from English into languages or domains with limited parallel data. We also ran experiments varying different system parameters. Table 5.4 explores different ordering strategies, Table 5.5 looks at the impact of discontiguous phrases and Table 5.6 looks at the impact of decoder optimizations such as treelet pruning and n-best list size. We presented a novel approach to syntacticallyinformed statistical machine translation that leverages a parsed dependency tree representation of the source language via a tree-based ordering model and treelet phrase extraction. We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes. Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis. While this is a natural starting point due to its wellunderstood nature and commonly available tools, we feel that this is not the most effective representation for syntax in MT. Dependency analysis, in contrast to constituency analysis, tends to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and is better suited to lexicalized models, such as the ones presented in this paper. The most important contribution of our system is a linguistically motivated ordering approach based on the source dependency tree, yet this paper only explores one possible model. Different model structures, machine learning techniques, and target feature representations all have the potential for significant improvements. Currently we only consider the top parse of an input sentence. One means of considering alternate possibilities is to build a packed forest of dependency trees and use this in decoding translations of each input sentence. As noted above, our approach shows particular promise for language pairs such as EnglishJapanese that exhibit large-scale reordering and have proven difficult for string-based approaches. Further experimentation with such language pairs is necessary to confirm this. Our experience has been that the quality of GIZA++ alignments for such language pairs is inadequate. Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.
A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. The field of research in natural language processing (NLP) applications for L2 language is constantly growing. This is largely driven by the ex panding population of L2 English speakers, whose varying levels of ability may require different types of NLP tools from those designed primarily for native speakers of the language. These include applications for use by the individual and within instructional contexts. Among the key tools are error-checking applications, focusing particularly on areas which learners find the most challenging. Prepositions and determiners are known to be oneof the most frequent sources of error for L2 En glish speakers, a finding supported by our analysisof a small error-tagged corpus we created (determiners 17% of errors, prepositions 12%). There fore, in developing a system for automatic error detection in L2 writing, it seems desirable to focus on these problematic, and very common, parts of speech (POS).This paper gives a brief overview of the prob lems posed by these POS and of related work. We c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. then present our proposed approach on both L1 and L2 data and discuss the results obtained so far. 2.1 Prepositions. Prepositions are challenging for learners because they can appear to have an idiosyncratic behaviour which does not follow any predictable pattern even across nearly identical contexts. For example, we say I study in Boston but I study at MIT; or He is independent of his parents, but dependent on his son. As it is hard even for L1 speakers to articulatethe reasons for these differences, it is not surprising that learners find it difficult to master preposi tions. 2.2 Determiners. Determiners pose a somewhat different problem from prepositions as, unlike them, their choice is more dependent on the wider discourse contextthan on individual lexical items. The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context. For example, wecan say boys like sport or the boys like sport, depending on whether we are making a general state ment about all boys or referring to a specific group.Equally, both she ate an apple and she ate the ap ple are grammatically well-formed sentences, butonly one may be appropriate in a given context, de pending on whether the apple has been mentioned previously. Therefore, here, too, it is very hard tocome up with clear-cut rules predicting every pos sible kind of occurrence. 169 Although in the past there has been some research on determiner choice in L1 for applications such as generation and machine translation output, work to date on automatic error detection in L2 writing hasbeen fairly limited. Izumi et al (2004) train a maximum entropy classifier to recognise various er rors using contextual features. They report results for different error types (e.g. omission - precision 75.7%, recall 45.67%; replacement - P 31.17%, R 8%), but there is no break-down of results byindividual POS. Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy. Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter. They report 80% precision and 30% recall. Finally, Gamon etal. (2008) use a complex system including a decision tree and a language model for both preposi tion and determiner errors, while Yi et al (2008)propose a web count-based system to correct de terminer errors (P 62%, R 41%).The work presented here displays some similar ities to the papers mentioned above in its use of a maximum entropy classifier and a set of features.However, our feature set is more linguistically sophisticated in that it relies on a full syntactic analysis of the data. It includes some semantic compo nents which we believe play a role in correct class assignment. determiners 4.1 Feature set. The approach proposed in this paper is based on the belief that although it is difficult to formulatehard and fast rules for correct preposition and determiner usage, there is enough underlying regularity of characteristic syntactic and semantic con texts to be able to predict usage to an acceptabledegree of accuracy. We use a corpus of grammat ically correct English to train a maximum entropyclassifier on examples of correct usage. The classifier can therefore learn to associate a given preposition or determiner to particular contexts, and re liably predict a class when presented with a novel instance of a context for one or the other. The L1 source we use is the British National Head noun ?apple? Number singular Noun type count Named entity? no WordNet category food, plant Prep modification? yes, ?on? Object of Prep? no Adj modification? yes, ?juicy? Adj grade superlative POS ?3 VV, DT, JJS, IN, DT, NN Table 1: Determiner feature set for Pick the juiciest apple on the tree. POS modified verb Lexical item modified ?drive? WordNet Category motion Subcat frame pp to POS of object noun Object lexical item ?London? Named entity? yes, type = location POS ?3 NNP, VBD, NNP Grammatical relation iobj Table 2: Preposition feature set for John drove to London.Corpus (BNC) as we believe this offers a represen tative sample of different text types. We represent training and testing items as vectors of values for linguistically motivated contextual features. Our feature vectors include 18 feature categories for determiners and 13 for prepositions; the main ones are illustrated in Table 1 and Table 2 respectively. Further determiner features note whether the nounis modified by a predeterminer, possessive, nu meral, and/or a relative clause, and whether it ispart of a ?there is. . . ? phrase. Additional preposi tion features refer to the grade of any adjectives or adverbs modified (base, comparative, superlative) and to whether the items modified are modified by more than one PP 1 . In De Felice and Pulman (2007), we described some of the preprocessing required and offered some motivation for this approach. As for ourchoice of features, we aim to capture all the ele ments of a sentence which we believe to have an effect on preposition and determiner choice, and which can be easily extracted automatically - this is a key consideration as all the features derivedrely on automatic processing of the text. Grammatical relations refer to RASP-style grammatical re lations between heads and complements in which the preposition occurs (see e.g. (Briscoe et al, 1 A full discussion of each feature, including motivation for its inclusion and an assessment of its contribution to the model, is found in De Felice (forthcoming). 170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)). Semantic word type information is takenfrom WordNet lexicographer classes, 40 broad se mantic categories which all nouns and verbs in WordNet belong to 2 (e.g. ?verb of motion?, ?noun denoting food?), while the POStags are from the Penn Treebank tagset - we note the POS of three words either side of the target word 3 . For each. occurrence of a preposition or determiner in the corpus, we obtain a feature vector consisting ofthe preposition or determiner and its context, de scribed in terms of the features noted above. 5.1 Prepositions. At the moment, we restrict our analysis to the nine most frequent prepositions in the data: at, by, for, from, in, of, on, to, and with, to ensure a sufficient amount of data for training. This gives a training dataset comprising 8,898,359 instances. We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance. Before testing our model on learner data, it is important to ascertain that it can correctlyassociate prepositions to a given context in gram matical, well-edited data. We therefore tested themodel on a section of the BNC not used in train ing, section J. Our best result to date is 70.06% accuracy (test set size: 536,193). Table 3 relates our results to others reported in the literature on comparable tasks. The baseline refers to always choosing the most frequent option, namely of.We can see that our model?s performance com pares favourably to the best results in the literature, although direct comparisons are hard to draw sincedifferent groups train and test on different preposi tion sets and on different types of data (British vs. American English, BNC vs. news reports, and so 2 No word sense disambiguation was performed at this stage. 3 In NPs with a null determiner, the target is the head noun. 4 Developed by James Curran. Proportion of training data Precision Recall of 27.83% (2,501,327) 74.28% 90.47% to 20.64% (1,855,304) 85.99% 81.73% in 17.68% (1,589,718) 60.15% 67.60% for 8.01% (720,369) 55.47% 43.78% on 6.54% (587,871) 58.52% 45.81% with 6.03% (541,696) 58.13% 46.33% at 4.72% (424,539) 57.44% 52.12% by 4.69% (421,430) 63.83% 56.51% from 3.86% (347,105) 59.20% 32.07% Table 4: L1 results - individual prepositions on). Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be. The figure reported here refers to the latter task,as it is the most similar to the one we are evalu ating. Additionally, Chodorow et al also discusssome modifications to their model which can in crease accuracy; the result noted here is the one more directly comparable to our own approach. 5.1.1 Further discussion To fully assess the model?s performance on the L1data, it is important to consider factors such as performance on individual prepositions, the relation ship between training dataset size and accuracy, and the kinds of errors made by the model.Table 4 shows the classifier?s performance on in dividual prepositions together with the size of their training datasets. At first glance, a clear correlationappears between the amount of data seen in training and precision and recall, as evidenced for ex ample by of or to, for which the classifier achievesa very high score. In other cases, however, the cor relation is not so clear-cut. For example by has one of the smallest data sets in training but higher scores than many of the other prepositions, whilefor is notable for the opposite reason, namely hav ing a large dataset but some of the lowest scores. The absence of a definite relation between dataset size and performance suggests that theremight be a cline of ?learnability? for these prepo sitions: different prepositions? contexts may be more or less uniquely identifiable, or they mayhave more or fewer senses, leading to less confusion for the classifier. One simple way of verify ing the latter case is by looking at the number of senses assigned to the prepositions by a resource 171 Target prep Confused with at by for from in of on to with at xx 4.65% 10.82% 2.95% 36.83% 19.46% 9.17% 10.28% 5.85% by 6.54% xx 8.50% 2.58% 41.38% 19.44% 5.41% 10.04% 6.10% for 8.19% 3.93% xx 1.91% 25.67% 36.12% 5.60% 11.29% 7.28% from 6.19% 4.14% 6.72% xx 26.98% 26.74% 7.70% 16.45% 5.07% in 7.16% 9.28% 10.68% 3.01% xx 43.40% 10.92% 8.96% 6.59% of 3.95% 2.00% 18.81% 3.36% 40.21% xx 9.46% 14.77% 7.43% on 5.49% 3.85% 8.66% 2.29% 32.88% 27.92% xx 12.20% 6.71% to 9.77% 3.82% 11.49% 3.71% 24.86% 27.95% 9.43% xx 8.95% with 3.66% 4.43% 12.06% 2.24% 28.08% 26.63% 6.81% 16.10% xx Table 5: Confusion matrix for L1 data - prepositions such as the Oxford English Dictionary. However, we find no good correlation between the two as the preposition with the most senses is of (16), and that with the fewest is from (1), thus negating the idea that fewer senses make a preposition easierto learn. The reason may therefore be found else where, e.g. in the lexical properties of the contexts. A good picture of the model?s errors can be had by looking at the confusion matrix in Table 5,which reports, for each preposition, what the clas sifier?s incorrect decision was. Analysis of these errors may establish whether they are related to thedataset size issue noted above, or have a more lin guistically grounded explanation.From the table, the frequency effect appears evi dent: in almost every case, the three most frequentwrong choices are the three most frequent prepo sitions, to, of, and in, although interestingly not inthat order, in usually being the first choice. Conversely, the less frequent prepositions are less of ten suggested as the classifier?s choice. This effectprecludes the possibility at the moment of draw ing any linguistic conclusions. These may only be gleaned by looking at the errors for the three more frequent prepositions. We see for example that there seems to be a strong relation between of and for, the cause of which is not immediately clear: perhaps they both often occur within noun phrases(e.g. book of recipes, book for recipes). More pre dictable is the confusion between to and from, andbetween locative prepositions such as to and at, al though the effect is less strong for other potentially confusable pairs such as in and at or on. Table 6 gives some examples of instances where the classifier?s chosen preposition differs from thatfound in the original text. In most cases, the clas sifier?s suggestion is also grammatically correct, Classifier choice Correct phrase demands of the sector demands for. condition for development condition of. travel to speed travel at. look at the USA look to. .Table 6: Examples of classifier errors on preposi tion L1 task Author Accuracy Baseline 59.83% Han et al 06 83.00% Gamon et al 08 86.07% Turner and Charniak 07 86.74% Our model 92.15% Table 7: Classifier performance - L1 determiners but the overall meaning of the phrases changes somewhat. For example, while the demands of the sector are usually made by the sector itself, the demands for the sector suggest that someoneelse may be making them. These are subtle dif ferences which it may be impossible to capture without a more sophisticated understanding of the wider context. The example with travel, on the other hand, yields an ungrammatical result. We assume thatthe classifier has acquired a very strong link be tween the lexical item travel and the preposition tothat directs it towards this choice (cf. also the ex ample of look at/to). This suggests that individual lexical items play an important role in preposition choice along with other more general syntactic and semantic properties of the context. 172 %of training data Prec. Recall a 9.61% (388,476) 70.52% 53.50% the 29.19% (1,180,435) 85.17% 91.51% null 61.20% (2,475,014) 98.63% 98.79% Table 8: L1 results - individual determiners 5.2 Determiners. For the determiner task, we also consider only the three most frequent cases (a, the, null), which gives us a training dataset consisting of 4,043,925 instances. We achieve accuracy of 92.15% on theL1 data (test set size: 305,264), as shown in Table 7. Again, the baseline refers to the most fre quent class, null. The best reported results to date on determiner selection are those in Turner and Charniak (2007). Our model outperforms their n-gram languagemodel approach by over 5%. Since the two approaches are not tested on the same data this com parison is not conclusive, but we are optimistic that there is a real difference in accuracy since the type of texts used are not dissimilar. As in the case of the prepositions, it is interesting to see whether this high performance is equally distributed across thethree classes; this information is reported in Ta ble 8. Here we can see that there is a very strongcorrelation between amount of data seen in training and precision and recall. The indefinite arti cle?s lower ?learnability?, and its lower frequency appears not to be peculiar to our data, as it is also found by Gamon et al among others.The disparity in training is a reflection of the dis tribution of determiners in the English language. Perhaps if this imbalance were addressed, the model would more confidently learn contexts of use for a, too, which would be desirable in view of using this information for error correction. On theother hand, this would create a distorted represen tation of the composition of English, which maynot be what we want in a statistical model of lan guage. We plan to experiment with smaller scale, more similar datasets to ascertain whether the issue is one of training size or of inherent difficulty in learning about the indefinite article?s occurrence.In looking at the confusion matrix for determin ers (Table 9), it is interesting to note that for theclassifier?s mistakes involving a or the, the erroneous choice is in the almost always the other de terminer rather than the null case. This suggeststhat the frequency effect is not so strong as to over Target det Confused with a the null a xx 92.92% 7.08% the 80.66% xx 19.34% null 14.51% 85.49% xx Table 9: Confusion matrix for L1 determiners ride any true linguistic information the model has acquired, otherwise the predominant choice wouldalways be the null case. On the contrary, these results show that the model is indeed capable of distinguishing between contexts which require a determiner and those which do not, but requires fur ther fine tuning to perform better in knowing which of the two determiner options to choose. Perhaps the introduction of a discourse dimension might assist in this respect. We plan to experiment withsome simple heuristics: for example, given a se quence ?Determiner Noun?, has the noun appeared in the preceding few sentences? If so, we might expect the to be the correct choice rather than a. 6.1 Working with L2 text. To evaluate the model?s performance on learner data, we use a subsection of the Cambridge Learner Corpus (CLC) 5 . We envisage our model to. eventually be of assistance to learners in analysingtheir writing and identifying instances of preposi tion or determiner usage which do not correspond to what it has been trained to expect; the more probable instance would be suggested as a more appropriate alternative. In using NLP tools and techniques which have been developed with and for L1 language, a loss of performance on L2 data is to be expected. These methods usually expect grammatically well-formed input; learner text is often ungrammatical, misspelled, and different in content and structure from typical L1 resources such as the WSJ and the BNC. 6.2 Prepositions. For the preposition task, we extract 2523 instances of preposition use from the CLC (1282 correct, 1241 incorrect) and ask the classifier to mark them 5 The CLC is a computerised database of contemporary written learner English (currently over 25m words). It wasdeveloped jointly by Cambridge ESOL and Cambridge Uni versity Press. The Cambridge Error Coding System has been developed and applied manually to the data by Cambridge University Press. 173 Instance type Accuracy Correct 66.7% Incorrect 70%Table 10: Accuracy on L2 data - prepositions. Ac curacy on incorrect instances refers to the classifier successfully identifying the preposition in the text as not appropriate for that context. as correct or incorrect. The results from this taskare presented in Table 10. These first results sug gest that the model is fairly robust: the accuracy rate on the correct data, for example, is not much lower than that on the L1 data. In an application designed to assist learners, it is important to aim to reduce the rate of false alarms - cases where the original is correct, but the model flags an error - toa minimum, so it is positive that this result is com paratively high. Accuracy on error identification is at first glance even more encouraging. However, ifwe look at the suggestions the model makes to re place the erroneous preposition, we find that theseare correct only 51.5% of the time, greatly reduc ing its usefulness. 6.2.1 Further discussion A first analysis of the classifier?s decisions and itserrors points to various factors which could be im pairing its performance. Spelling mistakes in theinput are one of the most immediate ones. For ex ample, in the sentence I?m Franch, responsable on the computer services, the classifier is not able to suggest a correct alternative to the erroneous on:since it does not recognise the adjective as a misspelling of responsible, it loses the information associated with this lexical feature, which could po tentially determine the preposition choice. A more complex problem arises when poor grammar in the input misleads the parser so thatthe information it gives for a sentence is incor rect, especially as regards PP attachment. In this example, I wold like following equipment to my speech: computer, modem socket and microphone, the missing the leads the parser to treat following as a verb, and believes it to be the verb to which the preposition is attached. It therefore suggests from as a correction, which is a reasonable choice given the frequency of phrases such as to follow from. However, this was not what the PP was meant to modify: impaired performance from the parser could be a significant negative factor in the model?s performance. It would be interesting to test themodel on texts written by students of different lev els of proficiency, as their grammar may be more error-free and more likely to be parsed correctly. Alternatively, we could modify the parser so as to skip cases where it requires several attempts before producing a parse, as these more challenging casescould be indicative of very poorly structured sentences in which misused prepositions are depen dent on more complex errors.A different kind of problem impacting our accu racy scores derives from those instances where theclassifier selects a preposition which can be cor rect in the given context, but is not the correct one in that particular case. In the example I received a beautiful present at my birthday, the classifier identifies the presence of the error, and suggests the grammatically and pragmatically appropriate correction for. The corpus annotators, however, indicate on as the correct choice. Since we use their annotations as the benchmark against which to evaluate the model, this instance is counted as the classifier being wrong because it disagrees with the annotators. A better indication of the model?sperformance may be to independently judge its de cisions, to avoid being subject to the annotators?bias. Finally, we are beginning to look at the rela tions between preposition errors and other types oferror such as verb choice, and how these are anno tated in the data. An overview of the classifier?s error patterns forthe data in this task shows that they are largely similar to those observed in the L1 data. This sug gests that the gap in performance between L1 and L2 is due more to the challenges posed by learner text than by inherent shortcomings in the model, and therefore that the key to better performance is likely to lie in overcoming these problems. In future work we plan to use L2 data where someof the spelling errors and non-preposition or deter miner errors have been corrected so that we can see which of the other errors are worth focussing on first. 6.3 Determiners. Our work on determiner error correction is still in the early stages. We follow a similar procedure to the prepositions task, selecting a number of both correct and incorrect instances. On the former (set size 2000) accuracy is comparable to that on L1data: 92.2%. The danger of false alarms, then, ap pears not to be as significant as for the prepositions 174 task. On the incorrect instances (set size ca. 1200), however, accuracy is less than 10%. Preliminary error analysis shows that the modelis successful at identifying cases of misused deter miner, e.g. a for the or vice versa, doing so in overtwo-thirds of cases. However, by far the most fre quent error type for determiners is not confusion between indefinite and definite article, but omitting an article where one is needed. At the moment, themodel detects very few of these errors, no doubt in fluenced by the preponderance of null cases seen in training. Furthermore, some of the issues raised earlier in discussing the application of NLP tools to L2 language hold for this task, too. In addition to those, though, in this task more than for prepositions we believe that differences intext type between the training texts - the BNC and the testing material - learner essays - has a sig nificant negative effect on the model. In this task,the lexical items play a crucial role in class assign ment. If the noun in question has not been seen in training, the classifier may be unable to make an informed choice. Although the BNC comprises a wide variety of texts, there may not be a sufficient number covering topics typical of learner essays, such as ?business letters? or ?postcards to penpals?.Also, the BNC was created with material from almost 20 years ago, and learners writing in contem porary English may use lexical items which are notvery frequently seen in the BNC. A clear exam ple of this discrepancy is the noun internet, which requires the definite article in English, but not inseveral other languages, leading to countless sen tences such as I saw it in internet, I booked it on internet, and so on. This is one of the errors themodel never detects: a fact which is not surpris ing when we consider that this noun occurs only four times in the whole of the training data. It may be therefore necessary to consider using alternative sources of training data to overcome this problem and improve the classifier?s performance. In developing this model, our first aim was not to create something which learns like a human, butsomething that works in the best and most effi cient possible way. However, it is interesting to see whether human learners and classifiers display similar patterns of errors in preposition choice.This information has twofold value: as well as being of pedagogical assistance to instructors of En glish L2, were the classifier to display student-like error patterns, insights into ?error triggers? could be derived from the L2 pedagogical literature to improve the classifier. The analysis of the typesof errors made by human learners yields some insights which might be worthy of further investi gation. A clear one is the confusion between the three locative and temporal prepositions at, in, and on (typical sentence: The training programme will start at the 1st August). This type of error is made often by both learners and the model on both types of data, suggesting that perhaps further attentionto features might be necessary to improve discrim ination between these three prepositions.There are also interesting divergences. For ex ample, a common source of confusion in learners is between by and from, as in I like it becauseit?s from my favourite band. However, this confu sion is not very frequent in the model, a difference which could be explained either by the fact that, as noted above, performance on from is very low and so the classifier is unlikely to suggest it, or that in training the contexts seen for by are sufficiently distinctive that the classifier is not misled like the learners. Finally, a surprising difference comes from looking at what to is confused with. The model often suggests at where to would be correct. This is perhaps not entirely unusual as both can occur with locative complements (one can go to a placeor be at a place) and this similarity could be con fusing the classifier. Learners, however, although they do make this kind of mistake, are much more hampered by the confusion between for and to, as in She was helpful for me or This is interesting for you. In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense. We can hypothesise that the classifier is less distracted by these cases because the effect of the lexical features is stronger. A more detailed discussion of the issues arising from the comparison of confusion pairs cannot be had here. However, in noting both divergences and similarities between the two learners, human and machine, we may be able to derive useful insights into the way the learning processes operate, and what factors could be more or less important for them. 175 This paper discussed a contextual feature based approach to the automatic acquisition of models of use for prepositions and determiners, whichachieve an accuracy of 70.06% and 92.15% re spectively, and showed how it can be applied to anerror correction task for L2 writing, with promis ing early results. There are several directions that can be pursued to improve accuracy on both types of data. The classifier can be further fine-tuned to acquire more reliable models of use for the two POS. We can also experiment with its confidencethresholds, for example allowing it to make an other suggestion when its confidence in its first choice is low. Furthermore, issues relating to the use of NLP tools with L2 data must be addressed, such as factoring out spelling or other errors in the data, and perhaps training on text types which are more similar to the CLC. In the longer term, we also envisage mining the information implicit inour training data to create a lexical resource de scribing the statistical tendencies observed. AcknowledgementsWe wish to thank Stephen Clark and Laura Rimell for stim ulating discussions and the anonymous reviewers for their helpful comments. We acknowledge Cambridge University Press?s assistance in accessing the Cambridge Learner Corpusdata. Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.
Concept Discovery From Text Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality. Broad-coverage lexical resources such as WordNet are extremely useful in applications such as Word Sense Disambiguation (Leacock, Chodorow, Miller 1998) and Question Answering (Pasca and Harabagiu 2001). However, they often include many rare senses while missing domain-specific senses. For example, in WordNet, the words dog, computer and company all have a sense that is a hyponym of person. Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person. On the other hand, WordNet misses the user-interface object sense of the word dialog (as often used in software manuals). One way to deal with these problems is to use a clustering algorithm to automatically induce semantic classes (Lin and Pantel 2001). Many clustering algorithms represent a cluster by the centroid of all of its members (e.g., K means) (McQueen 1967) or by a representative element (e.g., K-medoids) (Kaufmann and Rousseeuw 1987). When averaging over all elements in a cluster, the centroid of a cluster may be unduly influenced by elements that only marginally belong to the cluster or by elements that also belong to other clusters. For example, when clustering words, we can use the contexts of the words as features and group together the words that tend to appear in similar contexts. For instance, U.S. state names can be clustered this way because they tend to appear in the following contexts: (List A) ___ appellate court campaign in ___ ___ capital governor of ___ ___ driver's license illegal in ___ ___ outlaws sth. primary in ___ ___'s sales tax senator for ___ If we create a centroid of all the state names, the centroid will also contain features such as: (List B) ___'s airport archbishop of ___ ___'s business district fly to ___ ___'s mayor mayor of ___ ___'s subway outskirts of ___ because some of the state names (like New York and Washington) are also names of cities. Using a single representative from a cluster may be problematic too because each individual element has its own idiosyncrasies that may not be shared by other members of the cluster. In this paper, we propose a clustering algo rithm, CBC (Clustering By Committee), in which the centroid of a cluster is constructed by averaging the feature vectors of a subset of the cluster members. The subset is viewed as a committee that determines which other elements belong to the cluster. By carefully choosing committee members, the features of the centroid tend to be the more typical features of the target class. For example, our system chose the following committee members to compute the centroid of the state cluster: Illinois, Michigan, Minnesota, Iowa, Wisconsin, Indiana, Nebraska and Vermont. As a result, the centroid contains only features like those in List A. Evaluating clustering results is a very difficult task. We introduce a new evaluation methodol ogy that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Clustering algorithms are generally categorized as hierarchical and partitional. In hierarchical agglomerative algorithms, clusters are constructed by iteratively merging the most similar clusters. These algorithms differ in how they compute cluster similarity. In single-link clustering, the similarity between two clusters is the similarity between their most similar members while complete-link clustering uses the similarity between their least similar members. Average-link clustering computes this similarity as the average similarity between all pairs of elements across clusters. The complexity of these algorithms is O(n2logn), where n is the number of elements to be clustered (Jain, Murty, Flynn 1999). Chameleon is a hierarchical algorithm that employs dynamic modeling to improve clustering quality (Karypis, Han, Kumar 1999). When merging two clusters, one might consider the sum of the similarities between pairs of elements across the clusters (e.g. average-link clustering). A drawback of this approach is that the existence of a single pair of very similar elements might unduly cause the merger of two clusters. An alternative considers the number of pairs of elements whose similarity exceeds a certain threshold (Guha, Rastogi, Kyuseok 1998). However, this may cause undesirable mergers when there are a large number of pairs whose similarities barely exceed the threshold. Chameleon clustering combines the two approaches. K-means clustering is often used on large data sets since its complexity is linear in n, the number of elements to be clustered. K-means is a family of partitional clustering algorithms that iteratively assigns each element to one of K clusters according to the centroid closest to it and recomputes the centroid of each cluster as the average of the cluster?s elements. K-means has complexity O(K?T?n) and is efficient for many clustering tasks. Because the initial centroids are randomly selected, the resulting clusters vary in quality. Some sets of initial centroids lead to poor convergence rates or poor cluster quality. Bisecting K-means (Steinbach, Karypis, Kumar 2000), a variation of K-means, begins with a set containing one large cluster consisting of every element and iteratively picks the largest cluster in the set, splits it into two clusters and replaces it by the split clusters. Splitting a cluster consists of applying the basic K-means algorithm ? times with K=2 and keeping the split that has the highest average element centroid similarity. Hybrid clustering algorithms combine hierarchical and partitional algorithms in an attempt to have the high quality of hierarchical algorithms with the efficiency of partitional algorithms. Buckshot (Cutting, Karger, Pedersen, Tukey 1992) addresses the problem of randomly selecting initial centroids in K-means by combining it with average-link clustering. Buckshot first applies average-link to a random sample of n elements to generate K clusters. It then uses the centroids of the clusters as the initial K centroids of K-means clustering. The sample size counterbalances the quadratic running time of average-link to make Buckshot efficient: O(K?T?n + nlogn). The parameters K and T are usually considered to be small numbers. Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, ?threaten with __? is a context. If the word handgun occurred in this context, the context is a feature of handgun. The value of the feature is the pointwise mutual information (Manning and Sch?tze 1999 p.178) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information between c and w is defined as: ( ) ( ) ( ) N jF N wF N wF cw j c i i c mi ??? =, where N = ( )?? i j i jF is the total frequency counts of all words and their contexts. A well known problem with mutual information is that it is biased towards infrequent words/features. We therefore multiplied miw,c with a discounting factor: ( ) ( ) ( ) ( ) ( ) ( ) 11 +??? i j ci i j ci c c jF,wFmin jF,wFmin wF wF We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: ( ) ?? = c cw c cw c cwcw ji ji ji mimi mimi w,wsim 22 CBC consists of three phases. In Phase I, we compute each element?s top-k similar elements. In our experiments, we used k = 20. In Phase II, we construct a collection of tight clusters, where the elements of each cluster form a committee. The algorithm tries to form as many committees as possible on the condition that each newly formed committee is not very similar to any existing committee. If the condition is violated, the committee is simply discarded. In the final phase of the algorithm, each element is assigned to its most similar cluster. 4.1. Phase I: Find top-similar elements. Computing the complete similarity matrix between pairs of elements is obviously quadratic. However, one can dramatically reduce the running time by taking advantage of the fact that the feature vector is sparse. By indexing the features, one can retrieve the set of elements that have a given feature. To compute the top similar words of a word w, we first sort w?s features according to their mutual information with w. We only compute pairwise similarities between w and the words that share a high mutual information feature with w. 4.2. Phase II: Find committees. The second phase of the clustering algorithm recursively finds tight clusters scattered in the similarity space. In each recursive step, the algorithm finds a set of tight clusters, called committees, and identifies residue elements that are not covered by any committee. We say a committee covers an element if the element?s similarity to the centroid of the committee exceeds some high similarity threshold. The algorithm then recursively attempts to find more committees among the residue elements. The output of the algorithm is the union of all committees found in each recursive step. The details of Phase II are presented in Figure 1. In Step 1, the score reflects a preference for bigger and tighter clusters. Step 2 gives preference to higher quality clusters in Step 3, where a cluster is only kept if its similarity to all previously kept clusters is below a fixed threshold. In our experiments, we set ?1 = 0.35. Input: A list of elements E to be clustered, a similarity database S from Phase I, thresh olds ?1 and ?2. Step 1: For each element e ? E Cluster the top similar elements of e from S using average-link clustering. For each cluster discovered c compute the following score: |c| ? avgsim(c), where |c| is the number of elements in c and avgsim(c) is the average pairwise simi larity between elements in c. Store the highest-scoring cluster in a list L. Step 2: Sort the clusters in L in descending order of their scores. Step 3: Let C be a list of committees, initially empty. For each cluster c ? L in sorted order Compute the centroid of c by averaging the frequency vectors of its elements and computing the mutual information vector of the centroid in the same way as we did for individual elements. If c?s similarity to the centroid of each committee previously added to C is be low a threshold ?1, add c to C. Step 4: If C is empty, we are done and return C. Step 5: For each element e ? E If e?s similarity to every committee in C is below threshold ?2, add e to a list of resi dues R. Step 6: If R is empty, we are done and return C. Otherwise, return the union of C and the output of a recursive call to Phase II us ing the same input except replacing E with R. Output: A list of committees. Figure 1. Phase II of CBC. Step 4 terminates the recursion if no committee is found in the previous step. The residue elements are identified in Step 5 and if no residues are found, the algorithm terminates; otherwise, we recursively apply the algorithm to the residue elements. Each committee that is discovered in this phase defines one of the final output clusters of the algorithm. 4.3. Phase III: Assign elements to clusters. In Phase III, every element is assigned to the cluster containing the committee to which it is most similar. This phase resembles K-means in that every element is assigned to its closest centroid. Unlike K-means, the number of clusters is not fixed and the centroids do not change (i.e. when an element is added to a cluster, it is not added to the committee of the cluster). Many cluster evaluation schemes have been proposed. They generally fall under two categories: ? comparing cluster outputs with manually generated answer keys (hereon referred to as classes); or ? embedding the clusters in an application and using its evaluation measure. An example of the first approach considers the average entropy of the clusters, which measures the purity of the clusters (Steinbach, Karypis, and Kumar 2000). However, maximum purity is trivially achieved when each element forms its own cluster. An example of the second approach evaluates the clusters by using them to smooth probability distributions (Lee and Pereira 1999). Like the entropy scheme, we assume that there is an answer key that defines how the elements are supposed to be clustered. Let C be a set of clusters and A be the answer key. We define the editing distance, dist(C, A), as the number of operations required to make C consistent with A. We say that C is consistent with A if there is a one to one mapping between clusters in C and the classes in A such that for each cluster c in C, all elements of c belong to the same class in A. We allow two editing operations: ? merge two clusters; and ? move an element from one cluster to another. Let B be the baseline clustering where each element is its own cluster. We define the quality of a set of clusters C as follows: ( ) ( )ABdist ACdist , ,1? Suppose the goal is to construct a clustering consistent with the answer key. This measure can be interpreted as the percentage of operations saved by starting from C versus starting from the baseline. We aim to construct a clustering consistent with A as opposed to a clustering identical to A because some senses in A may not exist in the corpus used to generate C. In our experiments, we extract answer classes from WordNet. The word dog belongs to both the Person and Animal classes. However, in the newspaper corpus, the Person sense of dog is at best extremely rare. There is no reason to expect a clustering algorithm to discover this sense of dog. The baseline distance dist(B, A) is exactly the number of elements to be clustered. We made the assumption that each element belongs to exactly one cluster. The transforma tion procedure is as follows: 1. Suppose there are m classes in the answer. key. We start with a list of m empty sets, each of which is labeled with a class in the answer key. 2. For each cluster, merge it with the set. whose class has the largest number of elements in the cluster (a tie is broken arbitrarily). 3. If an element is in a set whose class is not. the same as one of the element?s classes, move the element to a set where it be longs. dist(C, A) is the number of operations performed using the above transformation rules on C. a b e c d e a c d b e b a c d e a b c d e A) B) C) D) E) Figure 2. An example of applying the transformation rules to three clusters. A) The classes in the answer key; B) the clusters to be transformed; C) the sets used to reconstruct the classes (Rule 1); D) the sets after three merge operations (Step 2); E) the sets after one move operation (Step 3). Figure 2 shows an example. In D) the cluster containing e could have been merged with either set (we arbitrarily chose the second). The total number of operations is 4. We generated clusters from a news corpus using CBC and compared them with classes extracted from WordNet (Miller 1990). 6.1. Test Data. To extract classes from WordNet, we first estimate the probability of a random word belonging to a subhierarchy (a synset and its hyponyms). We use the frequency counts of synsets in the SemCor corpus (Landes, Leacock, Tengi 1998) to estimate the probability of a subhierarchy. Since SemCor is a fairly small corpus, the frequency counts of the synsets in the lower part of the WordNet hierarchy are very sparse. We smooth the probabilities by assuming that all siblings are equally likely given the parent. A class is then defined as the maximal subhierarchy with probability less than a threshold (we used e-2). We used Minipar 1 (Lin 1994), a broad coverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500 words/second on a PIII-750 with 512MB memory. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3. The test set is constructed by intersecting the words in WordNet with the nouns in the corpus whose total mutual information with all of its contexts exceeds a threshold m. Since WordNet has a low coverage of proper names, we removed all capitalized nouns. We constructed two test sets: S13403 consisting of 13403 words (m = 250) and S3566 consisting of 3566 words (m = 3500). We then removed from the answer classes the words that did not occur in the test sets. Table 1 summa rizes the test sets. The sizes of the WordNet classes vary a lot. For S13403 there are 99 classes that contain three words or less and the largest class contains 3246 words. For S3566, 78 classes have three or less words and the largest class contains 1181 words. 1Available at www.cs.ualberta.ca/~lindek/minipar.htm. 6.2. Cluster Evaluation. We clustered the test sets using CBC and the clustering algorithms of Section 2 and applied the evaluation methodology from the previous section. Table 2 shows the results. The columns are our editing distance based evaluation measure. Test set S3566 has a higher score for all algorithms because it has a higher number of average features per word than S13403. For the K-means and Buckshot algorithms, we set the number of clusters to 250 and the maximum number of iterations to 8. We used a sample size of 2000 for Buckshot. For the Bisecting K-means algorithm, we applied the basic K-means algorithm twice (? = 2 in Section 2) with a maximum of 8 iterations per split. Our implementation of Chameleon was unable to complete clustering S13403 in reasonable time due to its time complexity. Table 2 shows that K-means, Buckshot and Average-link have very similar performance. CBC outperforms all other algorithms on both data sets. 6.3. Manual Inspection. Let c be a cluster and wn(c) be the WordNet class that has the largest intersection with c. The precision of c is defined as: Table 1. A description of the test sets in our experiments. DATA SET TOTAL WORDS m Average # of Features TOTAL CLASSES S13403 13403 250 740.8 202 S3566 3566 3500 2218.3 150 DATA SET TOTAL WORDS M Avg. Features per Word 13403 250 740.8 3566 3500 2218.3 Table 2. Cluster quality (%) of several clustering algorithms on the test sets. ALGORITHM S13403 S3566 CBC 60.95 65.82 K-means (K=250) 56.70 62.48 Buckshot 56.26 63.15 Bisecting K-means 43.44 61.10 Chameleon n/a 60.82 Average-link 56.26 62.62 Complete-link 49.80 60.29 Single-link 20.00 31.74 ( ) ( ) c cwnc cprecision ?= CBC discovered 943 clusters. We sorted them according to their precision. Table 3 shows five of the clusters evenly distributed according to their precision ranking along with their Top-15 features with highest mutual-information. The words in the clusters are listed in descending order of their similarity to the cluster centroid. For each cluster c, we also include wn(c). The underlined words are in wn(c). The first cluster is clearly a cluster of firearms and the second one is of pests. In WordNet, the word pest is curiously only under the person hierarchy. The words stopwatch and houseplant do not belong to the clusters but they have low similarity to their cluster centroid. The third cluster represents some kind of control. In WordNet, the legal power sense of jurisdiction is not a hyponym of social control as are supervision, oversight and governance. The fourth cluster is about mixtures. The words blend and mix as the event of mixing are present in WordNet but not as the result of mixing. The last cluster is about consumers. Here is the consumer class in WordNet 1.5: addict, alcoholic, big spender, buyer, client, concert-goer, consumer, customer, cutter, diner, drinker, drug addict, drug user, drunk, eater, feeder, fungi, head, heroin addict, home buyer, junkie, junky, lush, nonsmoker, patron, policy holder, purchaser, reader, regular, shopper, smoker, spender, subscriber, sucker, taker, user, vegetarian, wearer In our cluster, only the word client belongs to WordNet?s consumer class. The cluster is ranked very low because WordNet failed to consider words like patient, tenant and renter as consumers. Table 3 shows that even the lowest ranking CBC clusters are fairly coherent. The features associated with each cluster can be used to classify previously unseen words into one or more existing clusters. Table 4 shows the clusters containing the word cell that are discovered by various clustering algorithms from S13403. The underlined words represent the words that belong to the cell class in WordNet. The CBC cluster corresponds almost exactly to WordNet?s cell class. K-means and Buckshot produced fairly coherent clusters. The cluster constructed by Bisecting K-means is obviously of inferior quality. This is consistent with the fact that Bisecting K-means has a much lower score on S13403 compared to CBC, K means and Buckshot. Table 3. Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutual information and the WordNet classes that have the largest intersection with each cluster. RANK MEMBERS TOP-15 FEATURES wn(c) 1 handgun, revolver, shotgun, pistol, rifle, machine gun, sawed-off shotgun, submachine gun, gun, automatic pistol, automatic rifle, firearm, carbine, ammunition, magnum, cartridge, automatic, stopwatch __ blast, barrel of __ , brandish __, fire __, point __, pull out __, __ discharge, __ fire, __ go off, arm with __, fire with __, kill with __, open fire with __, shoot with __, threaten with __ artifact / artifact 236 whitefly, pest, aphid, fruit fly, termite, mosquito, cockroach, flea, beetle, killer bee, maggot, predator, mite, houseplant, cricket __ control, __ infestation, __ larvae, __ population, infestation of __, specie of __, swarm of __ , attract __, breed __, eat __, eradicate __, feed on __, get rid of __, repel __, ward off __ animal / animate being / beast / brute / creature / fauna 471 supervision, discipline, oversight, control, governance, decision making, jurisdiction breakdown in __, lack of __ , loss of __, assume __, exercise __, exert __, maintain __, retain __, seize __, tighten __, bring under __, operate under __, place under __, put under __, remain under __ act / human action / human activity 706 blend, mix, mixture, combination, juxtaposition, combine, amalgam, sprinkle, synthesis, hybrid, melange dip in __, marinate in __, pour in __, stir in __, use in __, add to __, pour __, stir __, curious __, eclectic __, ethnic __, odd __, potent __, unique __, unusual __ group / grouping 941 employee, client, patient, applicant, tenant, individual, participant, renter, volunteer, recipient, caller, internee, enrollee, giver benefit for __, care for __, housing for __, benefit to __, service to __, filed by __, paid by __, use by __, provide for __, require for --, give to __, offer to __, provide to __, disgruntled __, indigent __ worker We presented a clustering algorithm, CBC, for automatically discovering concepts from text. It can handle a large number of elements, a large number of output clusters, and a large sparse feature space. It discovers clusters using well scattered tight clusters called committees. In our experiments, we showed that CBC outperforms several well known hierarchical, partitional, and hybrid clustering algorithms in cluster quality. For example, in one experiment, CBC outperforms K-means by 4.25%. By comparing the CBC clusters with WordNet classes, we not only find errors in CBC, but also oversights in WordNet. Evaluating cluster quality has always been a difficult task. We presented a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Acknowledgements The authors wish to thank the reviewers for their helpful comments. This research was partly supported by Natural Sciences and Engineering Research Council of Canada grant OGP121338 and scholarship PGSB207797.
The Generative Lexicon In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories outline a theory of lexical semantics embodying a notion of well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is perthan assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop theory of Structure, representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the lexical knowledge base through a theory of inheritance. provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole. In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole. I believe we have reached an interesting turning point in research, where linguistic studies can be informed by computational tools for lexicology as well as an appreciation of the computational complexity of large lexical databases. Likewise, computational research can profit from an awareness of the grammatical and syntactic distinctions of lexical items; natural language processing systems must account for these differences in their lexicons and grammars. The wedding of these disciplines is so important, in fact, that I believe it will soon be difficult to carry out serious computational research in the fields of linguistics and NLP without the help of electronic dictionaries and computational lexicographic resources (cf. Walker et al. [forthcoming] and Boguraev and Briscoe [1988]). Positioned at the center of this synthesis is the study of word meaning, lexical semantics, which is currently witnessing a revival. In order to achieve a synthesis of lexical semantics and NLP, I believe that the lexical semantics community should address the following questions: Before addressing these questions, I would like to establish two basic assumptions that will figure prominently in my suggestions for a lexical semantics framework. The first is that, without an appreciation of the syntactic structure of a language, the study of lexical semantics is bound to fail. There is no way in which meaning can be completely divorced from the structure that carries it. This is an important methodological point, since grammatical distinctions are a useful metric in evaluating competing semantic theories. The second point is that the meanings of words should somehow reflect the deeper, conceptual structures in the system and the domain it operates in. This is tantamount to stating that the semantics of natural language should be the image of nonlinguistic conceptual organizing principles (whatever their structure). Computational lexical semantics should be guided by the following principles. First, a clear notion of semantic well-formedness will be necessary to characterize a theory of possible word meaning. This may entail abstracting the notion of lexical meaning away from other semantic influences. For instance, this might suggest that discourse and pragmatic factors should be handled differently or separately from the semantic contributions of lexical items in composition.' Although this is not a necessary assumption and may in fact be wrong, it may help narrow our focus on what is important for lexical semantic descriptions. Secondly, lexical semantics must look for representations that are richer than thematic role descriptions (Gruber 1965; Fillmore 1968). As argued in Levin and Rappaport (1986), named roles are useful at best for establishing fairly general mapping strategies to the syntactic structures in language. The distinctions possible with thetaroles are much too coarse-grained to provide a useful semantic interpretation of a sentence. What is needed, I will argue, is a principled method of lexical decomposition. This presupposes, if it is to work at all, (1) a rich, recursive theory of semantic composition, (2) the notion of semantic well-formedness mentioned above, and (3) an appeal to several levels of interpretation in the semantics (Scha 1983). Thirdly, and related to the point above, the lexicon is not just verbs. Recent work has done much to clarify the nature of verb classes and the syntactic constructions that each allows (Levin 1985, 1989). Yet it is not clear whether we are any closer to understanding the underlying nature of verb meaning, why the classes develop as they do, and what consequences these distinctions have for the rest of the lexicon and grammar. The curious thing is that there has been little attention paid to the other lexical categories (but see Miller and Johnson-Laird [1976], Miller and Fellbaum [1991], and Fass [1988]). That is, we have little insight into the semantic nature of adjectival predication, and even less into the semantics of nominals. Not until all major categories have been studied can we hope to arrive at a balanced understanding of the lexicon and the methods of composition. Stepping back from the lexicon for a moment, let me say briefly what I think the position of lexical research should be within the larger semantic picture. Ever since the earliest attempts at real text understanding, a major problem has been that of controlling the inferences associated with the interpretation process. In other words, how deep or shallow is the understanding of a text? What is the unit of well-formedness when doing natural language understanding; the sentence, utterance, paragraph, or discourse? There is no easy answer to this question because, except for the sentence, these terms are not even formalizable in a way that most researchers would agree on. It is my opinion that the representation of the context of an utterance should be viewed as involving many different generative factors that account for the way that language users create and manipulate the context under constraints, in order to be understood. Within such a theory, where many separate semantic levels (e.g. lexical semantics, compositional semantics, discourse structure, temporal structure) have independent interpretations, the global interpretation of a &quot;discourse&quot; is a highly flexible and malleable structure that has no single interpretation. The individual sources of semantic knowledge compute local inferences with a high degree of certainty (cf. Hobbs et al. 1988; Charniak and Goldman 1988). When integrated together, these inferences must be globally coherent, a state that is accomplished by processes of cooperation among separate semantic modules. The basic result of such a view is that semantic interpretation proceeds in a principled fashion, always aware of what the source of a particular inference is, and what the certainty of its value is. Such an approach allows the reasoning process to be both tractable and computationally efficient. The representation of lexical semantics, therefore, should be seen as just one of many levels in a richer characterization of contextual structure. Given what I have said, let us examine the questions presented above in more detail. First, let us turn to the issue of methodology. How can we determine the soundness of our method? Are new techniques available now that have not been adequately explored? Very briefly, one can summarize the most essential techniques assumed by the field, in some way, as follows (see, for example Cruse [1986]): Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. is not dependent on the syntactic context (cf. Katz and Fodor 1963; Karttunen 1971, 1974; Seuren 1985). This is illustrated in Example 3, where a killing always entails a dying. Example 3 When the same lexical item may carry different entailments in different contexts, we say that the entailments are sensitive to the syntactic contexts; for example, forget in Example 4, Example 4 a. John forgot that he locked the door. b. John forgot to lock the door. Example 4a has a factive interpretation of forget that 4b does not carry: in fact, 4b is counterfactive. Other cases of contextual specification involve aspectual verbs such as begin and finish as shown in Example 5. Example 5 The exact meaning of the verb finish varies depending on the object it selects, assuming for these examples the meanings finish smoking or finish drinking. While female behaves as a simple intersective modifier in 8b, certain modifiers such as alleged in 8a cannot be treated as simple attributes; rather, they create an intensional context for the head they modify. An even more difficult problem for compositionality arises from phrases containing frequency adjectives (cf. Stump 1981), as shown in 8c and 8d. Example 8 The challenge here is that the adjective doesn't modify the nominal head, but the entire proposition containing it (cf. Partee [1985] for discussion). A similar difficulty arises with the interpretation of scalar predicates such as fast in Example 9. Both the scale and the relative interpretation being selected for depends on the noun that the predicate is modifying. a. a fast typist: one who types quickly b. a fast car: one which can move quickly c. a fast waltz: one with a fast tempo Such data raise serious questions about the principles of compositionality and how ambiguity should be accounted for by a theory of semantics. This just briefly characterizes some of the techniques that have been useful for arriving at pre-theoretic notions of word meaning. What has changed over the years are not so much the methods themselves as the descriptive details provided by each test. One thing that has changed, however — and this is significant — is the way computational lexicography has provided stronger techniques and even new tools for lexical semantics research: see Atkins (1987) for sense discrimination tasks; Amsler (1985), Atkins et al. (forthcoming) for constructing concept taxonomies; Wilks et al. (1988) for establishing semantic relatedness among word senses; and Boguraev and Pustejovsky (forthcoming) for testing new ideas about semantic representations. Turning now to the question of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the taxonomic descriptions that have recently been made of verb classes are far superior to the classifications available twenty years ago (see Levin [1985] for review). Using mainly the descriptive vocabulary of Talmy (1975, 1985) and Jackendoff (1983), fine and subtle distinctions are drawn that were not captured in the earlier, primitives-based approach of Schank (1972, 1975) or the frame semantics of Fillmore (1968). As an example of the verb classifications developed by various researchers (and compiled by the MIT Lexicon Project; see Levin [1985, 1989]), consider the grammatical alternations in the example sentences below (cf. Dowty 1991). These three pairs show how the semantics of transitive motion verbs (e.g. run into) is similar in some respects to reciprocal verbs such as meet. The important difference, however, is that the reciprocal interpretation requires that both subject and object be animate or moving; hence 12b is ill-formed. (cf. Levin 1989; Dowty 1991). Another example of how diathesis reveals the underlying semantic differences between verbs is illustrated in Examples 13 and 14 below. A construction called the conative (see Hale and Keyser [1986] and Levin [1985]) involves adding the preposition at to the verb, changing the verb meaning to an action directed toward an object. a. Mary cut the bread. b. Mary cut at the bread. a. Mary broke the bread. b. *Mary broke at the bread. What these data indicate is that the conative is possible only with verbs of a particular semantic class; namely, verbs that specify the manner of an action that results in a change of state of an object. As useful and informative as the research on verb classification is, there is a major shortcoming with this approach. Unlike the theories of Katz and Fodor (1963), Wilks (1975a), and Quillian (1968), there is no general coherent view on what the entire lexicon will look like when semantic structures for other major categories are studied. This can be essential for establishing a globally coherent theory of semantic representation. On the other hand, the semantic distinctions captured by these older theories were often too coarse-grained. It is clear, therefore, that the classifications made by Levin and her colleagues are an important starting point for a serious theory of knowledge representation. I claim that lexical semantics must build upon this research toward constructing a theory of word meaning that is integrated into a linguistic theory, as well as interpreted in a real knowledge representation system. In this section I turn to the question of whether current theories have changed the way we look at representation and lexicon design. The question here is whether the representations assumed by current theories are adequate to account for the richness of natural language semantics. It should be pointed out here that a theory of lexical meaning will affect the general design of our semantic theory in several ways. If we view the goal of a semantic theory as being able to recursively assign meanings to expressions, accounting for phenomena such as synonymy, antonymy, polysemy, metonymy, etc., then our view of compositionality depends ultimately on what the basic lexical categories of the language denote. Conventional wisdom on this point paints a picture of words behaving as either active functors or passive arguments (Montague 1974). But we will see that if we change the way in which categories can denote, then the form of compositionality itself changes. Therefore, if done correctly, lexical semantics can be a means to reevaluate the very nature of semantic composition in language. In what ways could lexical semantics affect the larger methods of composition in semantics? I mentioned above that most of the careful representation work has been done on verb classes. In fact, the semantic weight in both lexical and compositional terms usually falls on the verb. This has obvious consequences for how to treat lexical ambiguity. For example, consider the verb bake in the two sentences below. Atkins, Kegl, and Levin (1988) demonstrate that verbs such as bake are systematically ambiguous, with both a change-of-state sense (15a) and a create sense (15b). A similar ambiguity exists with verbs that allow the resulative construction, shown in Examples 16 and 17, and discussed in Dowty (1979), Jackendoff (1983), and Levin and Rapoport (1988). These two verbs differ in their semantic representations, where run in 18a means goto-by-means-of-running, while in 18b it means simply move-by-running (cf. Jackendoff 1983). The methodology described above for distinguishing word senses is also assumed by those working in more formal frameworks. For example, Dowty (1985) proposes multiple entries for control and raising verbs, and establishes their semantic equivalence with the use of meaning postulates. That is, the verbs in Examples 19 and 20 are lexically distinct but semantically related by rules.3 Given the conventional notions of function application and composition, there is little choice but to treat all of the above cases as polysemous verbs. Yet, something about the systematicity of such ambiguity suggests that a more general and simpler explanation should be possible. By relaxing the conditions on how the meaning of a complex expression is derived from its parts, I will, in fact, propose a very straightforward explanation for these cases of logical polysemy. In this section, I will outline what I think are the basic requirements for a theory of computational semantics. I will present a conservative approach to decomposition, where lexical items are minimally decomposed into structured forms (or templates) rather than sets of features. This will provide us with a generative framework for the composition of lexical meanings, thereby defining the well-formedness conditions for semantic expressions in a language. We can distinguish between two distinct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a method would tell us the minimal semantic configuration of a lexical item. Furthermore, it should tell us the compositional properties of a word, just as a grammar informs us of the specific syntactic behavior of a certain category. What we are led to, therefore, is a generative theory of word meaning, but one very different from the generative semantics of the 1970s. To explain why I am suggesting that lexical decomposition proceed in a generative fashion rather than the traditional exhaustive approach, let me take as a classic example, the word closed as used in Example 21 (see Lakoff 1970). a. The door is closed. b. The door closed. c. John closed the door. Lakoff (1970), Jackendoff (1972), and others have suggested that the sense in 21c must incorporate something like cause-to-become-not-open for its meaning. Similarly, a verb such as give specifies a transfer from one person to another, e.g., cause-to-have. Most decomposition theories assume a set of primitives and then operate within this set to capture the meanings of all the words in the language. These approaches can be called exhaustive since they assume that with a fixed number of primitives, complete definitions of lexical meaning can be given. In the sentences in 21, for example, close is defined in terms of the negation of a primitive, open. Any method assuming a fixed number of primitives, however, runs into some well-known problems with being able to capture the full expressiveness of natural language. These problems are not, however, endemic to all decomposition approaches. I would like to suggest that lexical (and conceptual) decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, let us assume a fixed number of generative devices that can be seen as constructing semantic expressions.' Just as a formal language is described more in terms of the productions in the grammar than its accompanying vocabulary, a semantic language is definable by the rules generating the structures for expressions rather than the vocabulary of primitives itself.6 How might this be done? Consider the sentences in Example 21 again. A minimal decomposition on the word closed is that it introduces an opposition of terms: closed and not-closed. For the verbal forms in 21b and 21c, both terms in this opposition are predicated of different subevents denoted by the sentences. In 21a, this opposition is left implicit, since the sentence refers to a single state. Any minimal analysis of the semantics of a lexical item can be termed a generative operation, since it operates on the predicate(s) already literally provided by the word. This type of analysis is essentially Aristotle's principle of opposition (cf. Lloyd 1968), and it will form the basis of one level of representation for a lexical item. The essential opposition denoted by a predicate forms part of what I will call the qualia structure of that lexical item. Briefly, the qualia structure of a word specifies four aspects of its meaning: I will call these aspects of a word's meaning its Constitutive Role, Formal Role, Telic Role, and its Agentive Role, respectively.' This minimal semantic distinction is given expressive force when combined with a theory of event types. For example, the predicate in 21a denotes the state of the door being closed. No opposition is expressed by this predicate. In 21b and 21c, however, the opposition is explicitly part of the meaning of the predicate. Both these predicates denote what I will call transitions. The intransitive use of close in 21b makes no mention of the causer, yet the transition from not-closed to closed is still entailed. In 2k, the event that brings about the closed state of the door is made more explicit by specifying the actor involved. These differences constitute what I call the event structure of a lexical item. Both the opposition of predicates and the specification of causation are part of a verb's semantics, and are structurally associated with slots in the event template for the word. As we will see in the next section, there are different inferences associated with each event type, as well as different syntactic behaviors (cf. Grimshaw 1990 and Pustejovsky 1991). Because the lexical semantic representation of a word is not an isolated expression, but is in fact linked to the rest of the lexicon, in Section 7, I suggest how the global integration of the semantics for a lexical item is achieved by structured inheritance through the different qualia associated with a word. I call this the lexical inheritance structure for the word. Finally, we must realize that part of the meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called the argument structure for a lexical item. I will build on Grimshaw's recent proposals (Grimshaw 1990) for how to define the mapping from the lexicon to syntax. to a particular vocabulary of primitives, a lexical semantics should provide a method for the decomposition and composition of lexical items. 7 Some of these roles are reminiscent of descriptors used by various computational researchers, such as Wilks (1975b), Hayes (1979), and Hobbs et al. (1987). Within the theory outlined here, these roles determine a minimal semantic description of a word that has both semantic and grammatical consequences. This provides us with an answer to the question of what levels of semantic representation are necessary for a computational lexical semantics. In sum, I will argue that lexical meaning can best be captured by assuming the following levels of representation. These four structures essentially constitute the different levels of semantic expressiveness and representation that are needed for a computational theory of lexical semantics. Each level contributes a different kind of information to the meaning of a word. The important difference between this highly configurational approach to lexical semantics and feature-based approaches is that the recursive calculus defined for word meaning here also provides the foundation for a fully compositional semantics for natural language and its interpretation into a knowledge representation model. A logical starting point for our investigations into the meaning of words is what has been called the functional structure or argument structure associated with verbs. What originally began as the simple listing of the parameters or arguments associated with a predicate has developed into a sophisticated view of the way arguments are mapped onto syntactic expressions (for example, the f-structure in Lexical Functional Grammar [Bresnan 1982] and the Projection Principle in Government-Binding Theory [Chomsky 1981]). One of the most important contributions has been the view that argument structure is highly structured independent of the syntax. Williams's (1981) distinction between external and internal arguments and Grimshaw's proposal for a hierarchically structured representation (Grimshaw 1990) provide us with the basic syntax for one aspect of a word's meaning. The argument structure for a word can be seen as a minimal specification of its lexical semantics. By itself, it is certainly inadequate for capturing the semantic characterization of a lexical item, but it is a necessary component. As mentioned above, the theory of decomposition being outlined here is based on the central idea that word meaning is highly structured, and not simply a set of semantic features. Let us assume this is the case. Then the lexical items in a language will essentially be generated by the recursive principles of our semantic theory. One level of semantic description involves an event-based interpretation of a word or phrase. I will call this level the event structure of a word (cf. Pustejovsky 1991; Moens and Steedman 1988). The event structure of a word is one level of the semantic specification for a lexical item, along with its argument structure, qualia structure, and inheritance structure. Because it is recursively defined on the syntax, it is also a property of phrases and sentences.' I will assume a sortal distinction between three classes of events: states (es), processes (eP), and transitions (eT ) . Unlike most previous sortal classifications for events, I will adopt a subeventual analysis or predicates, as argued in Pustejovsky (1991) and independently proposed in Croft (1991). In this view, an event sort such as eT may be decomposed into two sequentially structured subevents, (e&quot;, ss). Aspects of the proposal will be introduced as needed in the following discussion. In Section 5, I demonstrated how most of the lexical semantics research has concentrated on verbal semantics. This bias influences our analyses of how to handle ambiguity and certain noncompositional structures. Therefore, the only way to relate the different senses for the verbs in the examples below was to posit separate entries. 8 This proposal is an extension of ideas explored by Bach (1986), Higginbotham (1985), and Allen (1984). For a full discussion, see Pustejovsky (1988, 1991). See Tenny (1987) for a proposal on how aspectual distinctions are mapped to the syntax. A similar philosophy has lead linguists to multiply word senses in constructions involving Control and Equi-verbs, where different syntactic contexts necessitate different semantic types.' Normally, compositionality in such structures simply refers to the application of the functional element, the verb, to its arguments. Yet, such examples indicate that in order to capture the systematicity of such ambiguity, something else is at play, where a richer notion of composition is operative. What then accounts for the polysemy of the verbs in the examples above? The basic idea I will pursue is the following. Rather than treating the expressions that behave as arguments to a function as simple, passive objects, imagine that they are as active in the semantics as the verb itself. The product of function application would be sensitive to both the function and its active argument. Something like this is suggested in Keenan and Faltz (1985), as the Meaning—Form Correlation Principle. I will refer to such behavior as cocompositionality (see below). What I have in mind can best be illustrated by returning to the examples in 28. a. John baked the potato. b. John baked the cake. Rather than having two separate word senses for a verb such as bake, suppose there is simply one, a change-of-state reading. Without going into the details of the analysis, let us assume that bake can be lexically specified as denoting a process verb, and is minimally represented as Example 29.&quot; Lexical Semantics for bake:11 In order to explain the shift in meaning of the verb, we need to specify more clearly what the lexical semantics of a noun is. I have argued above that lexical semantic theory must make a logical distinction between the following qualia roles: the constitutive, formal, telic, and agentive roles. Now let us examine these roles in more detail. One can distinguish between potato and cake in terms of how they come about; the former 9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations. Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting operations to capture sense relatedness. We return to this topic below. 10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the language are typed for a particular event-sort, and thematic roles are treated as partial functions over the event (cf. Dowty 1989 and Chierchia 1989). 11 More precisely, the process el' should reflect that it is the substance contained in the object x that is affected. See footnote 20 for explanation. is a natural kind, while the latter is an artifact. Knowledge of an object includes not just being able to identify or refer, but more specifically, being able to explain how an artifact comes into being, as well as what it is used for; the denotation of an object must identify these roles. Thus, any artifact can be identified with the state of being that object, relative to certain predicates. As is well known from work on event semantics and Aktionsarten, it is a general property of processes that they can shift their event type to become a transition event (cf. Hinrichs 1985; Moens and Steedman 1987; and Krifka 1987). This particular fact about event structures, together with the semantic distinction made above between the two object types, provides us with an explanation for what I will refer to as the logical polysemy of verbs such as bake. As illustrated in Example 30a, when the verb takes as its complement a natural kind such as potato, the resulting semantic interpretation is unchanged; i.e., a process reading of a state-change. This is because the noun does not &quot;project&quot; an event structure of its own. That is, relative to the process of baking, potato does not denote an event-type.12 What is it, then, about the semantics of cake that shifts this core meaning of bake from a state-change predicate to its creation sense? As just suggested, this additional meaning is contributed by specific lexical knowledge we have about artifacts, and cake in particular; namely, there is an event associated with that object's &quot;coming into being,&quot; in this case the process of baking. Thus, just as a verb can select for an argument-type, we can imagine that an argument is itself able to select the predicates that govern it. I will refer to such constructions as cospecifications. Informally, relative to the process bake, the noun cake carries the selectional information that it is a process of &quot;baking&quot; that brings it about.13 We can illustrate this schematically in Example 31, where the complement effectively acts like a &quot;stage-level&quot; event predicate (cf. Carlson 1977) relative to the process event-type of the verb (i.e. a function from processes to transitions, <P , T>).14 The change in meaning in 31 comes not from the semantics of bake, but rather in composition with the complement of the verb, at the level of the entire verb phrase. The &quot;creation&quot; sense arises from the semantic role of cake that specifies it is an artifact (see below for discussion). Thus, we can derive both word senses of verbs like bake by putting some of the semantic weight on the NP. This view suggests that, in such cases, the verb itself is not polysemous. Rather, the sense of &quot;create&quot; is part of the meaning of cake by virtue of it being an artifact. The verb appears polysemous because certain complements add to the basic meaning by virtue of what they denote. We return to this topic below, There are several interesting things about such collocations. First, because the complement &quot;selects&quot; the verb that governs it (by virtue of knowledge of what is done to the object), the semantics of the phrase is changed. The semantic &quot;connectedness,&quot; as it were, is tighter when cospecification obtains. In such cases, the verb is able to successfully drop the dative PP argument, as shown below in (1). When the complement does not select the verb governing it, dative-drop is ungrammatical as seen in (2) (although there are predicates selected by these nouns; e.g. keep a secret, read a book, and play a record). Ia. Romeo gave the lecture. b.Hamlet mailed a letter. For discussion see Pustejovsky (in press). and provide a formal treatment for how the nominal semantics is expressed in these examples. Similar principles seem to be operating in the resultative constructions in Examples 23 and 24; namely, a systematic ambiguity is the result of principles of semantic composition rather than lexical ambiguity of the verbs. For example, the resultative interpretations for the verbs hammer in 23(b) and wipe in 24(b) arise from a similar operation, where both verbs are underlyingly specified with an event type of process. The adjectival phrases flat and clean, although clearly stative in nature, can also be interpreted as stage-level event predicates (cf. Dowty 1979). Notice, then, how the resultative construction requires no additional word sense for the verb, nor any special semantic machinery for the resultative interpretation to be available. Schematically, this is shown in Example 32. In fact, this analysis explains why it is that only process verbs participate in the resultative construction, and why the resultant phrase (the adjectival phrase) must be a subset of the states, namely, stage-level event predicates. Because the meaning of the sentence in 32 is determined by both function application of hammer to its arguments and function application of flat to the event-type of the verb, this is an example of cocompositionality (cf. Pustejovsky [forthcoming] for discussion). Having discussed some of the behavior of logical polysemy in verbs, let us continue our discussion of lexical ambiguity with the issue of metonymy. Metonymy, where a subpart or related part of an object &quot;stands for&quot; the object itself, also poses a problem for standard denotational theories of semantics. To see why, imagine how our semantics could account for the &quot;reference shifts&quot; of the complements shown in Example 33.16 Example 33 The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to the normal interpretations? I suggest that these are cases of semantic type coercion (cf. Pustejovsky 1989a), where the verb has coerced the meaning of a term phrase into a different semantic type. Briefly, type coercion can be defined as follows:17 As these examples illustrate, the syntactic argument to a verb is not always the same logical argument in the semantic relation. Although superficially similar to cases of general metonymy (cf. Lakoff and Johnson 1982; Nunberg 1978), there is an interesting systematicity to such shifts in meaning that we will try to characterize below as logical metonymy. The sentences in 34 illustrate the various syntactic consequences of metonymy and coercion involving experiencer verbs, while those in 35 show the different metonymic extensions possible from the causing event in a killing. The generalization here is that when a verb selects an event as one of its arguments, type coercion to an event will permit a limited range of logical metonymies. For example, in sentences 34(a,b,c,d,f,h), the entire event is directly referred to, while in 34(e,g) only a participant from the coerced event reading is directly expressed. Other examples of coercion include &quot;concealed questions&quot; 36 and &quot;concealed exclamations&quot; 37 (cf. Grimshaw 1979; Elliott 1974). That is, although the italicized phrases syntactically appear as NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that are not typical semantic properties for nouns in linguistics; e.g., artifact, natural kind. In Pustejovsky (1989b) and Pustejovsky and Anick (1988), I suggest that there is a system of relations that characterizes the semantics of nominals, very much like the argument structure of a verb. I called this the Qualia Structure, inspired by Aristotle's theory of explanation and ideas from Moravcsik (1975). Essentially, the qualia structure of a noun determines its meaning as much as the list of arguments determines a verb's meaning. The elements that make up a qualia structure include notions such as container, space, surface, figure, artifact, and so on.&quot; As stated earlier, there are four basic roles that constitute the qualia structure for a lexical item. Here I will elaborate on what these roles are and why they are useful. They are given in Example 38, where each role is defined, along with the possible values that these roles may assume. When we combine the qualia structure of a NP with the argument structure of a verb, we begin to see a richer notion of compositionality emerging, one that looks very much like object-oriented approaches to programming (cf. Ingria and Pustejovsky 1990). To illustrate these structures at play, let us consider a few examples. Assume that the decompositional semantics of a nominal includes a specification of its qualia structure: For example, a minimal semantic description for the noun novel will include values for each of these roles, as shown in Example 40, where *x* can be seen as a distinguished variable, representing the object itself. This structures our basic knowledge about the object: it is a narrative; typically in the form of a book; for the purpose of reading (whose event type is a transition); and is an artifact created by a transition event of writing. Observe how this structure differs minimally, but significantly, from the qualia structure for the noun dictionary in Example 41. Notice the differences in the values for the constitutive and telic roles. The purpose of a dictionary is an activity of referencing, which has an event structure of a process. I will now demonstrate that such structured information is not only useful for nouns, but necessary to account for their semantic behavior. I suggested earlier, that for cases such as 33, repeated below, there was no need to posit a separate lexical entry for each verb, where the syntactic and semantic types had to be represented explicitly. Example 42 Rather, the verb was analyzed as coercing its complement to the semantic type it expected. To illustrate this, consider 42(c). The type for begin within a standard typed intensional logic is <VP , <NP , S>>, and its lexical semantics is similar to that of other subject control verbs (cf. Klein and Sag [1985] for discussion). Assuming an event structure such as that of Krifka (1987) or Pustejovsky (1991), we can convert this lexical entry into a representation consistent with a logic making use of event-types (or sorts) by means of the following meaning postulate.2° VPVX1 El [Pa (xi ) (XII) 2ea [P (Xi ) (xn)(ea)il This allows us to type the verb begin as taking a transition event as its first argument, represented in Example 45. Because the verb requires that its first argument be of type transition the complement in 33(c) will not match without some sort of shift. It is just this kind of context where the complement (in this case a novel) is coerced to another type. The coercion dictates to the complement that it must conform to its type specification and the qualia roles may 20 It should be pointed out that the lexical structure for the verb bake given above in 30 and 31 can more properly be characterized as a process acting on various qualia of the arguments. in fact have values matching the correct type. For purposes of illustration, the qualia structure for novel from 41 can be represented as the logical expression in Example 46. The coercion operation on the complement in the above examples can be seen as a request to find any transition event associated with the noun. As we saw above, the qualia structure contains just this kind of information. We can imagine the qualia roles as partial functions from a noun denotation into its subconstituent denotations. For our present purposes, we abbreviate these functions as QF, Qc, QT, QA. When applied, they return the value of a particular qualia role. For example, the purpose of a novel is for reading it, shown in 47(a), while the mode of creating a novel is by writing it, represented in 47(b). As the expressions in 47 suggest, there are, in fact, two obvious interpretations for this sentence in 42(c). a. John began to read a novel. b. John began to write a novel. One of these is selected by the coercing verb, resulting in a complement that has a event-predicate interpretation, without any syntactic transformations (cf. Pustejovsky [1989a] for details).21 The derivation in 49(a) and the structure in 49(b) show the effects of this coercion on the verb's complement, using the telic value of nove1.22 21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and various contextual influences. But 1 maintain that there are only a finite number of default interpretations available in such constructions. These form part of the lexical semantics of the noun. Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al. (1990). 22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that each expression a may have available to it, a set of shifting operators, which we call Ea, which operate over an expression, changing its type and denotation. By making reference to these operators directly in the rule of function application, we can treat the functor polymorphically, as illustrated below. The fact that this is not a unique interpretation of the elliptical event predicate is in some ways irrelevant to the notion of type coercion. That there is some event involving the complement is required by the lexical semantics of the governing verb and the rules of type well-formedness, and although there are many ways to act on a novel, I argue that certain relations are &quot;privileged&quot; in the lexical semantics of the noun. It is not the role of a lexical semantic theory to say what readings are preferred, but rather which are available.23 Assuming the semantic selection given above for begin is correct, we would predict that, because of the process event-type associated with the telic role for dictionary, there is only one default interpretation for the sentence in 50; namely, the agentive event of &quot;compiling.&quot; 23 There are interesting differences in complement types between finish and complete. The former takes both NP and a gerundive VP, while the latter takes only an NP (cf. for example, Freed [1979] for discussion). Ia. John finished the book. b. John finished writing the book. 2a. John completed the book. b. *John completed writing the book. The difference would indicate that, contrary to some views (e.g. Wierzbicka [19881 and Dixon [19911), lexical items need to carry both syntactic and semantic selectional information to determine the range of complements they may take. Notice here also that complete tends to select the agentive role value for its complement and not the telic role. The scope of semantic selection is explored at length in Pustejovsky (forthcoming). Not surprisingly, when the noun in complement position has no default interpretation within an event predicate — as given by its qualia structure — the resulting sentence is extremely odd. a. *Mary began a rock. b. ? ?John finished the flower. The semantic distinctions that are possible once we give semantic weight to lexical items other than verbs are quite wide-ranging. The next example I will consider concerns scalar modifiers, such as fast, that modify different predicates depending on the head they modify. If we think of certain modifiers as modifying only a subset of the qualia for a noun, then we can view fast as modifying only the telic role of an object. This allows us to go beyond treating adjectives such as fast as intersective modifiers — for example, as Ax[cari (x) A fast' (x)]. Let us assume that an adjective such as fast is a member of the general type (N, N), but can be subtyped as applying to the Telic role of the noun being modified. That is, it has as its type, ([N Telic], N). This gives rise directly to the different interpretations in Example 52. These interpretations are all derived from a single word sense for fast. Because the lexical semantics for this adjective indicates that it modifies the telic role of the noun, it effectively acts as an event predicate rather than an attribute over the entire noun denotation, as illustrated in Example 53 for fast motorway (cf. Pustejovsky and Boguraev [1991] for discussion). As our final example of how the qualia structure contributes to the semantic interpretation of a sentence, observe how the nominals window and door in Examples 54 and 55 carry two interpretations (cf. Lakoff [1987] and Pustejovsky and Anick [19881): a. John crawled through the window. b. The window is closed. Each noun appears to have two word senses: a physical object denotation and an aperture denotation. Pustejovsky and Anick (1988) characterize the meaning of such &quot;Double Figure-Ground&quot; nominals as inherently relational, where both parameters are logically part of the meaning of the noun. In terms of the qualia structure for this class of nouns, the formal role takes as its value the Figure of a physical object, while the constitutive role assumes the Invert-Figure value of an aperture.24 The foregrounding or backgrounding of a nominal's qualia is very similar to argument structure-changing operations for verbs. That is, in 55(a), paint applies to the formal role of the door, while in 55(b), through will apply to the constitutive interpretation of the same NP. The ambiguity with such nouns is a logical one, one that is intimately linked to the semantic representation of the object itself. The qualia structure, then, is a way of capturing this logical polysemy. In conclusion, it should be pointed out that the entire lexicon is organized around such logical ambiguities, which Pustejovsky and Anick (1988) call Lexical Conceptual Paradigms. Pustejovsky (forthcoming) distinguishes the following systems and the paradigms that lexical items fall into: Example 57 Such paradigms provide a means for accounting for the systematic ambiguity that may exist for a lexical item. For example, a noun behaving according to paradigm 57(a) 24 There are many such classes of nominals, both two-dimensional such as those mentioned in the text, and three-dimensional, such as &quot;room,&quot; &quot;fireplace,&quot; and &quot;pipe.&quot; They are interesting semantically, because they are logically ambiguous, referring to either the object or the aperture, but not both. Boguraev and Pustejovsky (forthcoming) show how these logical polysemies are in fact encoded in dictionary definitions for these words. exhibits a logical polysemy involving packaging or grinding operators; e.g., haddock or lamb (cf. Copestake and Briscoe [1991] for details). In previous sections, I discussed lexical ambiguity and showed how a richer view of lexical semantics allows us to view a word's meaning as being flexible, where word senses could arise generatively by composition with other words. The final aspect of this flexibility deals with the logical associations a word has in a given context; that is, how this semantic information is organized as a global knowledge base. This involves capturing both the inheritance relations between concepts and, just as importantly, how the concepts are integrated into a coherent expression in a given sentence. I will assume that there are two inheritance mechanisms at work for representing the conceptual relations in the lexicon: fixed inheritance and projective inheritance. The first includes the methods of inheritance traditionally assumed in Al and lexical research (e.g. Roberts and Goldstein 1977; Brachman and Schmolze 1985; Bobrow and Winograd 1977); that is, a fixed network of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be limited to a description of projective inheritance and the notion of &quot;degrees of prototypicality&quot; of predication. I will argue that such degrees of salience or coherence relations can be explained in structural terms by examining a network of related lexical items.&quot; I will illustrate the distinction between these mechanisms by considering the two sentences in Example 58, and their relative prototypicality. a. The prisoner escaped last night. b. The prisoner ate dinner last night. Both of these sentences are obviously well-formed syntactically, but there is a definite sense that the predication in 58(a) is &quot;tighter&quot; or more prototypical than that in 58(b). What would account for such a difference? Intuitively, we associate prisoner with an escaping event more strongly than an eating event. Yet this is not information that comes from a fixed inheritance structure, but is rather usually assumed to be cornmonsense knowledge. In what follows, however, I will show that such distinctions can be captured within a theory of lexical semantics by means of generating ad hoc categories. First, we give a definition for the fixed inheritance structure of a lexical item (cf. Touretzky 1986). Let Q and P be concepts in our model of lexical organization. Then: Definition A sequence (Qi , PO is an inheritance path, which can be read as the conjunction of ordered pairs {(xi, y,) Ii < i < n}. Furthermore, following Touretsky, from this we can define the set of concepts that lie on an inheritance path, the conclusion space. Definition The conclusion space of a set of sequences (I) is the set of all pairs (Q, P) such that a sequence (Q, , P) appears in 1. From these two definitions we can define the traditional is-a relation, relating the above pairs by a generalization operator, <G,&quot; as well as other relations that I will not discuss.27 Let us suppose that, in addition to these fixed relational structures, our semantics allows us to dynamically create arbitrary concepts through the application of certain transformations to lexical meanings. For example, for any predicate, Q — e.g. the value of a qualia role — we can generate its opposition, -02 (cf. Pustejovsky 1991). By relating these two predicates temporally we can generate the arbitrary transition events for this opposition (cf. Wright 1963): Similarly, by operating over other qualia role values we can generate semantically related concepts. I will call any operator that performs such an operation a projective transformation, and define them below: Definition A projective transformation, 7r, on a predicate Qi generates a predicate, Q2, such that 7r(Q1) = Q2, where Q2 cl (I) . The set of transformations includes: negation, <, temporal precedence, >, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). The projective conclusion space, P('DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicate Q: as: P(43R) = {(P(Q1)P(Q)) I(Qi, • • • , Qn) 430. From this resulting representation, we can generate a relational structure that can be considered the set of ad hoc categories and relations associated with a lexical item (cf. Barselou 1983). Using these definitions, let us return to the sentences in Example 58. I will assume that the noun prisoner has a qualia structure such as that shown in 60. Using the representation in 60 above, I now trace part of the derivation of the projective conclusion space for prisoner. Inheritance structures are defined for each qualia role of an element. In the case above, values are specified for only two roles. For each role, R, we apply a projective transformation 7 onto the predicate Q that is the value of that role. For example, from the telic role of prisoner we can generalize (e.g. drop the conjunct) to the concept of being confined. From this concept, we can apply the negation operator, generating the predicate opposition of not-confined and confined. To this, we apply the two temporal operators, < and >, generating two states: free before capture and free after capture. Finally, to these concepts, if we apply the operator act, varying who is responsible for the resulting transition event, we generate the concepts: turn in, capture, escape, and release. Projecting on Telic Role of prisoner: These relations constitute the projective conclusion space for the telic role of prisoner relative to the application of the transformations mentioned above. Similar operations on the formal role will generate concepts such as die and kill. Generating such structures for all items in a sentence during analysis, we can take those graphs that result in no contradictions to be the legitimate semantic interpretations of the entire sentence. Let us now return to the sentences in Example 58. It is now clear why these two sentences differ in their prototypicality (or the relevance conditions on their predication). The predicate eat is not within the space of related concepts generated from the semantics of the NP the prisoner; escape, however, did fall within the projective conclusion space for the Telic role of prisoner, as shown in Example 63. We can therefore use such a procedure as one metric for evaluating the &quot;proximity&quot; of a predication (Quillian 1968; Hobbs 1982). In the examples above, the difference in semanticality can now be seen as a structural distinction between the semantic representations for the elements in the sentence. In this section, I have shown how the lexical inheritance structure of an item relates, in a generative fashion, the decompositional structure of a word to a much larger set of concepts that are related in obvious ways. What we have not addressed, however, is how the fixed inheritance information of a lexical item is formally derivable during composition. This issue is explicitly addressed in Briscoe et al. (1990) as well as Pustejovsky and Briscoe (1991). In this paper I have outlined a framework for lexical semantic research that I believe can be useful for both computational linguists and theoretical linguists alike. I argued against the view that word meanings are fixed and inflexible, where lexical ambiguity must be treated by multiple word entries in the lexicon. Rather, the lexicon can be seen as a generative system, where word senses are related by logical operations defined by the well-formedness rules of the semantics. In this view, much of the lexical ambiguity of verbs and prepositions is eliminated because the semantic load is spread more evenly throughout the lexicon to the other lexical categories. I described a language for structuring the semantic information carried by nouns and adjectives, termed Qualia structure, as well as the rules of composition that allow this information to be incorporated into the semantic interpretation of larger expressions, including explicit methods for type coercion. Finally, I discussed how these richer lexical representations can be used to generate projective inheritance structures that connect the conceptual information associated with lexical items to the global conceptual lexicon. This suggests a way of accounting for relations such as coherence and the prototypicality of a predication. Although much of what I have presented here is incomplete and perhaps somewhat programmatic, I firmly believe this approach can help clarify the nature of word meaning and compositionality in natural language, and at the same time bring us closer to understanding the creative use of word senses. I would like to thank the following for comments on earlier drafts of this paper: Peter Anick, Sabine Bergler, Bran Boguraev, Ted Briscoe, Noam Chomsky, Bob Ingria, George Miller, Sergei Nirenburg, and Rich Thomason.
Bayesian Learning of a Tree Substitution Grammar Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy. Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars. In particular, learning procedures are not able to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG. Since different TSG derivations can produce the same parse tree, learning procedures must guess the derivations, the number of which is exponential in the tree size. This compels heuristic methods of subtree extraction, or maximum likelihood estimators which tend to extract large subtrees that overfit the training data. These problems are common in natural language processing tasks that search for a hidden segmentation. Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009). In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work. TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as where each r is a rule used in the derivation. Under a regular CFG, each parse tree uniquely idenfifies a derivation. In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it. This disconnect between parses and derivations complicates both inference and learning. The inference (parsing) task for TSGs is NP-hard (Sima’an, 1996), and in practice the most probable parse is approximated (1) by sampling from the derivation forest or (2) from the top k derivations. Grammar learning is more difficult as well. CFGs are usually trained on treebanks, especially the Wall Street Journal (WSJ) portion of the Penn Treebank. Once the model is defined, relevant events can simply be counted in the training data. In contrast, there are no treebanks annotated with TSG derivations, and a treebank parse tree of n nodes is ambiguous among 2n possible derivations. One solution would be to manually annotate a treebank with TSG derivations, but in addition to being expensive, this task requires one to know what the grammar actually is. Part of the thinking motivating TSGs is to let the data determine the best set of subtrees. One approach to grammar-learning is DataOriented Parsing (DOP), whose strategy is to simply take all subtrees in the training data as the grammar (Bod, 1993). Bod (2001) did this, approximating “all subtrees” by extracting from the Treebank 400K random subtrees for each subtree height ranging from two to fourteen, and compared the performance of that grammar to that of a heuristically pruned “minimal subset” of it. The latter’s performance was quite good, achieving 90.8% F1 score1 on section 23 of the WSJ. This approach is unsatisfying in some ways, however. Instead of heuristic extraction we would prefer a model that explained the subtrees found in the grammar. Furthermore, it seems unlikely that subtrees with ten or so lexical items will be useful on average at test time (Bod did not report how often larger trees are used, but did report that including subtrees with up to twelve lexical items improved parser performance). We expect there to be fewer large subtrees than small ones. Repeating Bod’s grammar extraction experiment, this is indeed what we find when comparing these two grammars (Figure 1). In summary, we would like a principled (modelbased) means of determining from the data which set of subtrees should be added to our grammar, and we would like to do so in a manner that prefers smaller subtrees but permits larger ones if the data warrants it. This type of requirement is common in NLP tasks that require searching for a hidden segmentation, and in the following sections we apply it to learning a TSG from the Penn Treebank. prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here. Our training data is a set of parse trees T that we assume was produced by an unknown TSG g with probability Pr(T |g). Using Bayes’ rule, we can compute the probability of a particular hypothesized grammar as Pr(g) is a distribution over grammars that expresses our a priori preference for g. We use a set of Dirichlet Process (DP) priors (Ferguson, 1973), one for each nonterminal X E N, the set of nonterminals in the grammar. A sample from a DP is a distribution over events in an infinite sample space (in our case, potential subtrees in a TSG) which takes two parameters, a base measure and a concentration parameter: The base measure GX defines the probability of a subtree t as the product of the PCFG rules r E t that constitute it and a geometric distribution Pr$ over the number of those rules, thus encoding a preference for smaller subtrees.3 The parameter α contributes to the probability that previously unseen subtrees will be sampled. All DPs share parameters p$ and α. An entire grammar is then given as g = {gX : X E N}. We emphasize that no head information is used by the sampler. Rather than explicitly consider each segmentation of the parse trees (which would define a TSG and its associated parameters), we use a collapsed Gibbs sampler to integrate over all possible grammars and sample directly from the posterior. This is based on the Chinese Restaurant Process (CRP) representation of the DP. The Gibbs sampler is an iterative procedure. At initialization, each parse tree in the corpus is annotated with a specific derivation by marking each node in the tree with a binary flag. This flag indicates whether the subtree rooted at that node (a height one CFG rule, at minimum) is part of the subtree containing its parent. The Gibbs sampler considers every non-terminal, non-root node c of each parse tree in turn, freezing the rest of the training data and randomly choosing whether to join the subtrees above c and rooted at c (outcome h1) or to split them (outcome h2) according to the probability ratio φ(h1)/(φ(h1) + φ(h2)), where φ assigns a probability to each of the outcomes (Figure 2). Let sub(n) denote the subtree above and including node n and sub(n) the subtree rooted at n; o is a binary operator that forms a single subtree from two adjacent ones. The outcome probabilities are: where t = sub(c) o sub(c). Under the CRP, the subtree probability θ(t) is a function of the current state of the rest of the training corpus, the appropriate base measure Groot(t), and the concentration parameter α: where zt is the multiset of subtrees in the frozen portion of the training corpus sharing the same root as t, and countzt(t) is the count of subtree t among them. We used the standard split for the Wall Street Journal portion of the Treebank, training on sections 2 to 21, and reporting results on sentences with no more than forty words from section 23. We compare with three other grammars. We note two differences in our work that explain the large difference in scores for the minimal grammar from those reported by Bod: (1) we did not implement the smoothed “mismatch parsing”, which permits lexical leaves of subtrees to act as wildcards, and (2) we approximate the most probable parse with the top single derivation instead of the top 1,000. Rule probabilities for all grammars were set with relative frequency. The Gibbs sampler was initialized with the spinal grammar derivations. We construct sampled grammars in two ways: by summing all subtree counts from the derivation states of the first i sampling iterations together with counts from the Treebank CFG rules (denoted (α, p$,≤i)), and by taking the counts only from iteration i (denoted (α, p$, i)). Our standard CKY parser and Gibbs sampler were both written in Perl. TSG subtrees were flattened to CFG rules and reconstructed afterward, with identical mappings favoring the most probable rule. For pruning, we binned nonterminals according to input span and degree of binarization, keeping the ten highest scoring items in each bin. the significantly larger “minimal subset” grammar. The sampled grammars outperform all of them. Nearly all of the rules of the best single iteration sampled grammar (100, 0.8, 500) are lexicalized (50,820 of 60,633), and almost half of them have a height greater than one (27,328). Constructing sampled grammars by summing across iterations improved over this in all cases, but at the expense of a much larger grammar. Figure 3 shows a histogram of subtree size taken from the counts of the subtrees (by token, not type) actually used in parsing WSJ§23. Parsing with the “minimal subset” grammar uses highly lexicalized subtrees, but they do not improve accuracy. We examined sentence-level F1 scores and found that the use of larger subtrees did correlate with accuracy; however, the low overall accuracy (and the fact that there are so many of these large subtrees available in the grammar) suggests that such rules are overfit. In contrast, the histogram of subtree sizes used in parsing with the sampled grammar matches the shape of the histogram from the grammar itself. Gibbs sampling with a DP prior chooses smaller but more general rules. Collapsed Gibbs sampling with a DP prior fits nicely with the task of learning a TSG. The sampled grammars are model-based, are simple to specify and extract, and take the expected shape over subtree size. They substantially outperform heuristically extracted grammars from previous work as well as our novel spinal grammar, and can do so with many fewer rules. Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.
A Generative Constituent-Context Model For Improved Grammar Induction We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unparsing results on the Experiments on Penn treebank sentences of comparalength show an even higher 71% on nontrivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. The task of inducing hierarchical syntactic structure from observed yields alone has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), or to build better language models (Baker, 1979; Chen, 1995). In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus (Klein and Manning, 2001b). However, it suffered from several drawbacks, primarily stemming from the conditional model used for induction. Here, we improve on that model in several ways. First, we construct a generative model which utilizes the same features. Then, we extend the model to allow multiple constituent types and multiple prior distributions over trees. The new model gives a 13% reduction in parsing error on WSJ sentence experiments, including a positive qualitative shift in error types. Additionally, it produces much more stable results, does not require heavy smoothing, and exhibits a reliable correspondence between the maximized objective and parsing accuracy. It is also much faster, not requiring a fitting phase for each iteration. Klein and Manning (2001b) and Clark (2001) take treebank part-of-speech sequences as input. We followed this for most experiments, but in section 4.3, we use distributionally induced tags as input. Performance with induced tags is somewhat reduced, but still gives better performance than previous models. Early work on grammar induction emphasized heuristic structure search, where the primary induction is done by incrementally adding new productions to an initially empty grammar (Olivier, 1968; Wolff, 1988). In the early 1990s, attempts were made to do grammar induction by parameter search, where the broad structure of the grammar is fixed in advance and only parameters are induced (Lari and Young, 1990; Carroll and Charniak, 1992).1 However, this appeared unpromising and most recent work has returned to using structure search. Note that both approaches are local. Structure search requires ways of deciding locally which merges will produce a coherent, globally good grammar. To the extent that such approaches work, they work because good local heuristics have been engineered (Klein and Manning, 2001a; Clark, 2001). Parameter search is also local; parameters which are locally optimal may be globally poor. A concrete example is the experiments from (Carroll and Charniak, 1992). They restricted the space of grammars to those isomorphic to a dependency grammar over the POS symbols in the Penn treebank, and then searched for parameters with the inside-outside algorithm (Baker, 1979) starting with 300 random production weight vectors. Each seed converged to a different locally optimal grammar, none of them nearly as good as the treebank grammar, measured either by parsing performance or data-likelihood. However, parameter search methods have a potential advantage. By aggregating over only valid, complete parses of each sentence, they naturally incorporate the constraint that constituents cannot cross – the bracketing decisions made by the grammar must be coherent. The Carroll and Charniak experiments had two primary causes for failure. First, random initialization is not always good, or necessary. The parameter space is riddled with local likelihood maxima, and starting with a very specific, but random, grammar should not be expected to work well. We duplicated their experiments, but used a uniform parameter initialization where all productions were equally likely. This allowed the interaction between the grammar and data to break the initial symmetry, and resulted in an induced grammar of higher quality than Carroll and Charniak reported. This grammar, which we refer to as DEP-PCFG will be evaluated in more detail in section 4. The second way in which their experiment was guaranteed to be somewhat unencouraging is that a delexicalized dependency grammar is a very poor model of language, even in a supervised setting. By the F1 measure used in the experiments in section 4, an induced dependency PCFG scores 48.2, compared to a score of 82.1 for a supervised PCFG read from local trees of the treebank. However, a supervised dependency PCFG scores only 53.5, not much better than the unsupervised version, and worse than a right-branching baseline (of 60.0). As an example of the inherent shortcomings of the dependency grammar, it is structurally unable to distinguish whether the subject or object should be attached to the verb first. Since both parses involve the same set of productions, both will have equal likelihood. To exploit the benefits of parameter search, we used a novel model which is designed specifically to enable a more felicitous search space. The fundamental assumption is a much weakened version of classic linguistic constituency tests (Radford, 1988): constituents appear in constituent contexts. A particular linguistic phenomenon that the system exploits is that long constituents often have short, common equivalents, or proforms, which appear in similar contexts and whose constituency is easily discovered (or guaranteed). Our model is designed to transfer the constituency of a sequence directly to its containing context, which is intended to then pressure new sequences that occur in that context into being parsed as constituents in the next round. The model is also designed to exploit the successes of distributional clustering, and can equally well be viewed as doing distributional clustering in the presence of no-overlap constraints. Unlike a PCFG, our model describes all contiguous subsequences of a sentence (spans), including empty spans, whether they are constituents or nonconstituents (distituents). A span encloses a sequence of terminals, or yield, α, such as DT JJ NN. A span occurs in a context x, such as o–VBZ, where x is the ordered pair of preceding and following terminals (o denotes a sentence boundary). A bracketing of a sentence is a boolean matrix B, which indicates which spans are constituents and which are not. Figure 1 shows a parse of a short sentence, the bracketing corresponding to that parse, and the labels, yields, and contexts of its constituent spans. Figure 2 shows several bracketings of the sentence in figure 1. A bracketing B of a sentence is non-crossing if, whenever two spans cross, at most one is a constituent in B. A non-crossing bracketing is tree-equivalent if the size-one terminal spans and the full-sentence span are constituents, and all size-zero spans are distituents. Figure 2(a) and (b) are tree-equivalent. Tree-equivalent bracketings B correspond to (unlabeled) trees in the obvious way. A bracketing is binary if it corresponds to a binary tree. Figure 2(b) is binary. We will induce trees by inducing tree-equivalent bracketings. Our generative model over sentences S has two phases. First, we choose a bracketing B according to some distribution P(B) and then generate the sentence given that bracketing: Given B, we fill in each span independently. The context and yield of each span are independent of each other, and generated conditionally on the constituency Bij of that span. The distribution P(αij |Bij) is a pair of multinomial distributions over the set of all possible yields: one for constituents (Bij = c) and one for distituents (Bij = d). Similarly for P(xij |Bij) and contexts. The marginal probability assigned to the sentence S is given by summing over all possible bracketings of S: P(S) = Y-B P(B)P(S|B).2 To induce structure, we run EM over this model, treating the sentences S as observed and the bracketings B as unobserved. The parameters 2 of 2Viewed as a model generating sentences, this model is deficient, placing mass on yield and context choices which will not tile into a valid sentence, either because specifications for positions conflict or because yields of incorrect lengths are chosen. However, we can renormalize by dividing by the mass placed on proper sentences and zeroing the probability of improper bracketings. The rest of the paper, and results, would be unchanged except for notation to track the renormalization constant. the model are the constituency-conditional yield and context distributions P(α|b) and P(x|b). If P(B) is uniform over all (possibly crossing) bracketings, then this procedure will be equivalent to softclustering with two equal-prior classes. There is reason to believe that such soft clusterings alone will not produce valuable distinctions, even with a significantly larger number of classes. The distituents must necessarily outnumber the constituents, and so such distributional clustering will result in mostly distituent classes. Clark (2001) finds exactly this effect, and must resort to a filtering heuristic to separate constituent and distituent clusters. To underscore the difference between the bracketing and labeling tasks, consider figure 3. In both plots, each point is a frequent tag sequence, assigned to the (normalized) vector of its context frequencies. Each plot has been projected onto the first two principal components of its respective data set. The left plot shows the most frequent sequences of three constituent types. Even in just two dimensions, the clusters seem coherent, and it is easy to believe that they would be found by a clustering algorithm in the full space. On the right, sequences have been labeled according to whether their occurrences are constituents more or less of the time than a cutoff (of 0.2). The distinction between constituent and distituent seems much less easily discernible. We can turn what at first seems to be distributional clustering into tree induction by confining P(B) to put mass only on tree-equivalent bracketings. In particular, consider Pbin(B) which is uniform over binary bracketings and zero elsewhere. If we take this bracketing distribution, then when we sum over data completions, we will only involve bracketings which correspond to valid binary trees. This restriction is the basis for our algorithm. We now essentially have our induction algorithm. We take P(B) to be Pbin(B), so that all binary trees are equally likely. We then apply the EM algorithm: E-Step: Find the conditional completion likelihoods P(BIS, O) according to the current O. M-Step: Fix P(B|S, O) and find the O' which maximizes EB P(BIS, O) logP(S, BIO'). The completions (bracketings) cannot be efficiently enumerated, and so a cubic dynamic program similar to the inside-outside algorithm is used to calculate the expected counts of each yield and context, both as constituents and distituents. Relative frequency estimates (which are the ML estimates for this model) are used to set O'. To begin the process, we did not begin at the Estep with an initial guess at O. Rather, we began at the M-step, using an initial distribution over completions. The initial distribution was not the uniform distribution over binary trees Pbin(B). That was undesirable as an initial point because, combinatorily, almost all trees are relatively balanced. On the other hand, in language, we want to allow unbalanced structures to have a reasonable chance to be discovered. Therefore, consider the following uniformsplitting process of generating binary trees over k terminals: choose a split point at random, then recursively build trees by this process on each side of the split. This process gives a distribution Psplit which puts relatively more weight on unbalanced trees, but only in a very general, non language-specific way. This distribution was not used in the model itself, however. It seemed to bias too strongly against balanced structures, and led to entirely linear-branching structures. The smoothing used was straightforward. For each yield α or context x, we added 10 counts of that item as a constituent and 50 as a distituent. This reflected the relative skew of random spans being more likely to be distituents. This contrasts with our previous work, which was sensitive to smoothing method, and required a massive amount of it. We performed most experiments on the 7422 sentences in the Penn treebank Wall Street Journal section which contained no more than 10 words after the removal of punctuation and null elements (WSJ-10). Evaluation was done by measuring unlabeled precision, recall, and their harmonic mean F1 against the treebank parses. Constituents which could not be gotten wrong (single words and entire sentences) were discarded.3 The basic experiments, as described above, do not label constituents. An advantage to having only a single constituent class is that it encourages constituents of one type to be found even when they occur in a context which canonically holds another type. For example, NPs and PPs both occur between a verb and the end of the sentence, and they can transfer constituency to each other through that context. Figure 4 shows the F1 score for various methods of parsing. RANDOM chooses a tree uniformly at random from the set of binary trees.4 This is the unsupervised baseline. DEP-PCFG is the result of duplicating the experiments of Carroll and Charniak (1992), using EM to train a dependencystructured PCFG. LBRANCH and RBRANCH choose the left- and right-branching structures, respectively. RBRANCH is a frequently used baseline for supervised parsing, but it should be stressed that it encodes a significant fact about English structure, and an induction system need not beat it to claim a degree of success. CCM is our system, as described above. SUP-PCFG is a supervised PCFG parser trained on a 90-10 split of this data, using the treebank grammar, with the Viterbi parse rightbinarized.5 UBOUND is the upper bound of how well a binary system can do against the treebank sentences, which are generally flatter than binary, limiting the maximum precision. CCM is doing quite well at 71.1%, substantially better than right-branching structure. One common issue with grammar induction systems is a tendency to chunk in a bottom-up fashion. Especially since the CCM does not model recursive structure explicitly, one might be concerned that the high overall accuracy is due to a high accuracy on short-span constituents. Figure 5 shows that this is not true. Recall drops slightly for mid-size constituents, but longer constituents are as reliably proposed as short ones. Another effect illustrated in this graph is that, for span 2, constituents have low precision for their recall. This contrast is primarily due to the single largest difference between the system’s induced structures and those in the treebank: the treebank does not parse into NPs such as DT JJ NN, while our system does, and generally does so correctly, identifying N units like JJ NN. This overproposal drops span-2 precision. In contrast, figure 5 also shows the F1 for DEP-PCFG, which does exhibit a drop in F1 over larger spans. The top row of figure 8 shows the recall of nontrivial brackets, split according the brackets’ labels in the treebank. Unsurprisingly, NP recall is highest, but other categories are also high. Because we ignore trivial constituents, the comparatively low S represents only embedded sentences, which are somewhat harder even for supervised systems. To facilitate comparison to other recent work, figure 6 shows the accuracy of our system when trained on the same WSJ data, but tested on the ATIS corpus, and evaluated according to the EVALB program.6 The F1 numbers are lower for this corpus and evaluation method.7 Still, CCM beats not only RBRANCH (by 8.3%), but also the previous conditional COND-CCM and the next closest unsupervised system (which does not beat RBRANCH in F1). Parsing figures can only be a component of evaluating an unsupervised induction system. Low scores may indicate systematic alternate analyses rather than true confusion, and the Penn treebank is a sometimes arbitrary or even inconsistent gold standard. To give a better sense of the kinds of errors the system is or is not making, we can look at which sequences are most often over-proposed, or most often under-proposed, compared to the treebank parses. forms MD VB verb groups systematically, and it attaches the possessive particle to the right, like a determiner, rather than to the left.8 It provides binarybranching analyses within NPs, normally resulting in correct extra N constituents, like JJ NN, which are not bracketed in the treebank. More seriously, it tends to attach post-verbal prepositions to the verb and gets confused by long sequences of nouns. A significant improvement over earlier systems is the absence of subject-verb groups, which disappeared when we switched to Psplit(B) for initial completions; the more balanced subject-verb analysis had a substantial combinatorial advantage with Pbin(B). We also ran the system with multiple constituent classes, using a slightly more complex generative model in which the bracketing generates a labeling which then generates the constituents and contexts. The set of labels for constituent spans and distituent spans are forced to be disjoint. Intuitively, it seems that more classes should help, by allowing the system to distinguish different types of constituents and constituent contexts. However, it seemed to slightly hurt parsing accuracy overall. Figure 8 compares the performance for 2 versus 12 classes; in both cases, only one of the classes was allocated for distituents. Overall F1 dropped very slightly with 12 classes, but the category recall numbers indicate that the errors shifted around substantially. PP accuracy is lower, which is not surprising considering that PPs tend to appear rather optionally and in contexts in which other, easier categories also frequently appear. On the other hand, embedded sentence recall is substantially higher, possibly because of more effective use of the top-level sentences which occur in the signature context o–o. The classes found, as might be expected, range from clearly identifiable to nonsense. Note that simply directly clustering all sequences into 12 categories produced almost entirely the latter, with clusters representing various distituent types. Figure 9 shows several of the 12 classes. Class 0 is the model’s distituent class. Its most frequent members are a mix of obvious distituents (IN DT, DT JJ, IN DT, NN VBZ) and seemingly good sequences like NNP NNP. However, there are many sequences of 3 or more NNP tags in a row, and not all adjacent pairs can possibly be constituents at the same time. Class 1 is mainly common NP sequences, class 2 is proper NPs, class 3 is NPs which involve numbers, and class 6 is N sequences, which tend to be linguistically right but unmarked in the treebank. Class 4 is a mix of seemingly good NPs, often from positions like VBZ–NN where they were not constituents, and other sequences that share such contexts with otherwise good NP sequences. This is a danger of not jointly modeling yield and context, and of not modeling any kind of recursive structure. Class 5 is mainly composed of verb phrases and verb groups. No class corresponded neatly to PPs: perhaps because they have no signature contexts. The 2-class model is effective at identifying them only because they share contexts with a range of other constituent types (such as NPs and VPs). A reasonable criticism of the experiments presented so far, and some other earlier work, is that we assume treebank part-of-speech tags as input. This criticism could be two-fold. First, state-of-the-art supervised PCFGs do not perform nearly so well with their input delexicalized. We may be reducing data sparsity and making it easier to see a broad picture of the grammar, but we are also limiting how well we can possibly do. It is certainly worth exploring methods which supplement or replace tagged input with lexical input. However, we address here the more serious criticism: that our results stem from clues latent in the treebank tagging information which are conceptually posterior to knowledge of structure. For instance, some treebank tag distinctions, such as particle (RP) vs. preposition (IN) or predeterminer (PDT) vs. determiner (DT) or adjective (JJ), could be said to import into the tagset distinctions that can only be made syntactically. To show results from a complete grammar induction system, we also did experiments starting with a clustering of the words in the treebank. We used basically the baseline method of word type clustering in (Sch¨utze, 1995) (which is close to the methods of (Finch, 1993)). For (all-lowercased) word types in the Penn treebank, a 1000 element vector was made by counting how often each co-occurred with each of the 500 most common words immediately to the left or right in Treebank text and additional 1994–96 WSJ newswire. These vectors were length-normalized, and then rank-reduced by an SVD, keeping the 50 largest singular vectors. The resulting vectors were clustered into 200 word classes by a weighted k-means algorithm, and then grammar induction operated over these classes. We do not believe that the quality of our tags matches that of the better methods of Sch¨utze (1995), much less the recent results of Clark (2000). Nevertheless, using these tags as input still gave induced structure substantially above right-branching. Figure 8 shows the performance with induced tags compared to correct tags. Overall F1 has dropped, but, interestingly, VP and S recall are higher. This seems to be due to a marked difference between the induced tags and the treebank tags: nouns are scattered among a disproportionally large number of induced tags, increasing the number of common NP sequences, but decreasing the frequency of each. Another issue with previous systems is their sensitivity to initial choices. The conditional model of Klein and Manning (2001b) had the drawback that the variance of final F1, and qualitative grammars found, was fairly high, depending on small differences in first-round random parses. The model presented here does not suffer from this: while it is clearly sensitive to the quality of the input tagging, it is robust with respect to smoothing parameters and data splits. Varying the smoothing counts a factor of ten in either direction did not change the overall F1 by more than 1%. Training on random subsets of the training data brought lower performance, but constantly lower over equal-size splits. Moreover, there are no first-round random decisions to be sensitive to; the soft EM procedure is deterministic. Figure 10 shows the overall F1 score and the data likelihood according to our model during convergence.9 Surprisingly, both are non-decreasing as the system iterates, indicating that data likelihood in this model corresponds well with parse accuracy.10 Figure 11 shows recall for various categories by iteration. NP recall exhibits the more typical pattern of a sharp rise followed by a slow fall, but the other categories, after some initial drops, all increase until convergence. These graphs stop at 40 iterations. The system actually converged in both likelihood and F1 by iteration 38, to within a tolerance of 10−10. The time to convergence varied according to smoothing amount, number of classes, and tags used, but the system almost always converged within 80 iterations, usually within 40. We have presented a simple generative model for the unsupervised distributional induction of hierarchical linguistic structure. The system achieves the best published unsupervised parsing scores on the WSJ-10 and ATIS data sets. The induction algorithm combines the benefits of EM-based parameter search and distributional clustering methods. We have shown that this method acquires a substantial amount of correct structure, to the point that the most frequent discrepancies between the induced trees and the treebank gold standard are systematic alternate analyses, many of which are linguistically plausible. We have shown that the system is not reliant on supervised POS tag input, and demonstrated increased accuracy, speed, simplicity, and stability compared to previous systems.
A Smorgasbord Of Features For Statistical Machine Translation We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate from an list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation. Despite the enormous progress in machine translation (MT) due to the use of statistical techniques in recent years, state-of-the-art statistical systems often produce translations with obvious errors. Grammatical errors include lack of a main verb, wrong word order, and wrong choice of function words. Frequent problems of a less grammatical nature include missing content words and incorrect punctuation. In this paper, we attempt to address these problems by exploring a variety of new features for scoring candidate translations. A high-quality statistical translation system is our baseline, and we add new features to the existing set, which are then combined in a log-linear model. To allow an easy integration of new features, the baseline system provides an n-best list of candidate translations which is then reranked using the new features. This framework allows us to incorporate different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based to part-of-speech tags and syntactic chunks, and then to features based on Treebank-based syntactic parses of the source and target sentences. The goal is the translation of a text given in some source language into a target language. We are given a source (‘Chinese’) sentence f = fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target (‘English’) sentence e = eI1 = e1, ... , ei, ... , eI Among all possible target sentences, we will choose the sentence with the highest probability: As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability Pr(eI1 fJ1 ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm(eI1, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter Am, m = 1, ... , M. The direct translation probability is given by: We obtain the following decision rule: The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting of S sentence pairs f(fs, es) : s = 1, ... , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words fJ1 are grouped to phrases ˜fK1 . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to πK1 ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases ˜eK1 constitutes the sequence of words eI1. Our baseline system incorporated the following feature functions: Alignment Template Selection Each alignment template is chosen with probability p(z |˜f), estimated by relative frequency. The corresponding feature function in our log-linear model is the log probability of the product of p(z |˜f) for all used alignment templates used. Word Selection This feature is based on the lexical translation probabilities p(e|f), estimated using relative frequencies according to the highest-probability wordlevel alignment for each training sentence. A translation probability conditioned on the source and target position within the alignment template p(e|f, i, j) is interpolated with the position-independent probability p(e|f). Phrase Alignment This feature favors monotonic alignment at the phrase level. It measures the ‘amount of non-monotonicity’ by summing over the distance (in the source language) of alignment templates which are consecutive in the target language. Language Model Features As a language model feature, we use a standard backing off word-based trigram language model (Ney, Generet, and Wessel, 1995). The baseline system actually includes four different language model features trained on four different corpora: the news part of the bilingual training data, a large Xinhua news corpus, a large AFP news corpus, and a set of Chinese news texts downloaded from the web. Word/Phrase Penalty This word penalty feature counts the length in words of the target sentence. Without this feature, the sentences produced tend to be too short. The phrase penalty feature counts the number of phrases produced, and can allow the model to prefer either short or long phrases. Phrases from Conventional Lexicon The baseline alignment template system makes use of the ChineseEnglish lexicon provided by LDC. Each lexicon entry is a potential phrase translation pair in the alignment template system. To score the use of these lexicon entries (which have no normal translation probability), this feature function counts the number of times such a lexicon entry is used. Additional Features A major advantage of the loglinear modeling approach is that it is easy to add new features. In this paper, we explore a variety of features based on successively deeper syntactic representations of the source and target sentences, and their alignment. For each of the new features discussed below, we added the feature value to the set of baseline features, re-estimated feature weights on development data, and obtained results on test data. We worked with the Chinese-English data from the recent evaluations, as both large amounts of sentence-aligned training corpora and multiple gold standard reference translations are available. This is a standard data set, making it possible to compare results with other systems. In addition, working on Chinese allows us to use the existing Chinese syntactic treebank and parsers based on it. For the baseline MT system, we distinguish the following three different sentence- or chunk-aligned parallel training corpora: most experiments described in this report this corpus consists of 993 sentences (about 25K words) in both languages. For development and test data, we have four English (reference) translations for each Chinese sentence. For each sentence in the development, test, and the blind test corpus a set of 16,384 different alternative translations has been produced using the baseline system. For extracting the n-best candidate translations, an A* search is used. These n-best candidate translations are the basis for discriminative training of the model parameters and for re-ranking. We used n-best reranking rather than implementing new search algorithms. The development of efficient search algorithms for long-range dependencies is very complicated and a research topic in itself. The reranking strategy enabled us to quickly try out a lot of new dependencies, which would not have been be possible if the search algorithm had to be changed for each new dependency. On the other hand, the use of n-best list rescoring limits the possibility of improvements to what is available in the n-best list. Hence, it is important to analyze the quality of the n-best lists by determining how much of an improvement would be possible given a perfect reranking algorithm. We computed the oracle translations, that is, the set of translations from our n-best list that yields the best BLEU score.1 We use the following two methods to compute the BLEU score of an oracle translation: 1Note that due to the corpus-level holistic nature of the BLEU score it is not trivial to compute the optimal set of oracle translations. We use a greedy search algorithm for the oracle translations that might find only a local optimum. Empirically, we do not observe a dependence on the starting point, hence we believe that this does not pose a significant problem. n-best list. The avBLEUr3 scores are computed with respect to three reference translations averaged over the four different choices of holding out one reference. The first method provides the theoretical upper bound of what BLEU score can be obtained by rescoring a given nbest list. Using this method with a 1000-best list, we obtain oracle translations that outperform the BLEU score of the human translations. The oracle translations achieve 113% against the human BLEU score on the test data (Table 1), while the first best translations obtain 79.2% against the human BLEU score. The second method uses a different references for selection and scoring. Here, using an 1000-best list, we obtain oracle translations with a relative human BLEU score of 88.5%. Based on the results of the oracle experiment, and in order to make rescoring computationally feasible for features requiring significant computation for each hypothesis, we used the top 1000 translation candidates for our experiments. The baseline system’s BLEU score is 31.6% on the test set (equivalent to the 1-best oracle in Table 1). This is the benchmark against which the contributions of the additional features described in the remainder of this paper are to be judged. As a precursor to developing the various syntactic features described in this report, the syntactic representations on which they are based needed to be computed. This involved part-of-speech tagging, chunking, and parsing both the Chinese and English side of our training, development, and test sets. Applying the part-of-speech tagger to the often ungrammatical MT output from our n-best lists sometimes led to unexpected results. Often the tagger tries to “fix up” ungrammatical sentences, for example by looking for a verb when none is present: China NNP 14 CD open JJ border NN cities NNS achievements VBZ remarkable JJ Here, although achievements has never been seen as a verb in the tagger’s training data, the prior for a verb in this position is high enough to cause a present tense verb tag to be produced. In addition to the inaccuracies of the MT system, the difference in genre from the tagger’s training text can cause problems. For example, while our MT data include news article headlines with no verb, headlines are not included in the Wall Street Journal text on which the tagger is trained. Similarly, the tagger is trained on full sentences with normalized punctuation, leading it to expect punctuation at the end of every sentence, and produce a punctuation tag even when the evidence does not support it: China NNP ’s POS economic JJ development NN and CC opening VBG up RP 14 CD border NN cities NNS remarkable JJ achievements . The same issues affect the parser. For example the parser can create verb phrases where none exist, as in the following example in which the tagger correctly did not identify a verb in the sentence: These effects have serious implications for designing syntactic feature functions. Features such “is there a verb phrase” may not do what you expect. One solution would be features that involve the probability of a parse subtree or tag sequence, allowing us to ask “how good a verb phrase is it?” Another solution is more detailed features examining more of the structure, such as “is there a verb phrase with a verb?” These features, directly based on the source and target strings of words, are intended to address such problems as translation choice, missing content words, and incorrect punctuation. We used IBM Model 1 (Brown et al., 1993) as one of the feature functions. Since Model 1 is a bag-of-word translation model and it gives the sum of all possible alignment probabilities, a lexical co-occurrence effect, or triggering effect, is expected. This captures a sort of topic or semantic coherence in translations. As defined by Brown et al. (1993), Model 1 gives a probability of any given translation pair, which is We used GIZA++ to train the model. The training data is a subset (30 million words on the English side) of the entire corpus that was used to train the baseline MT system. For a missing translation word pair or unknown words, where t(fj|ei) = 0 according to the model, a constant t(fj|ei) = 10−40 was used as a smoothing value. The average %BLEU score (average of the best four among different 20 search initial points) is 32.5. We also tried p(e|f; M1) as feature function, but did not obtain improvements which might be due to an overlap with the word selection feature in the baseline system. The Model 1 score is one of the best performing features. It seems to ’fix’ the tendency of our baseline system to delete content words and it improves word selection coherence by the triggering effect. It is also possible that the triggering effect might work on selecting a proper verb-noun combination, or a verb-preposition combination. As shown in Figure 1 the alignment templates (ATs) used in the baseline system can appear in various configurations which we will call left/right-monotone and left/right-continuous. We built 2 out of these 4 models to distinguish two types of lexicalized re-ordering of these ATs: The left-monotone model computes the total probability of all ATs being left monotone: where the lower left corner of the AT touches the upper right corner of the previous AT. Note that the first word in the current AT may or may not immediately follow the last word in the previous AT. The total probability is the product over all alignment templates i, either P(ATi is left-monotone) or 1 − P(ATi is left-monotone). The right-continuous model computes the total probability of all ATs being right continuous: where the lower left corner of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P(ATi is right-continuous) or 1 − P(ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: 14 (measure) open border cities The table shows an example tagging of an English hypothesis showing how it was generated from the Chinese sentence. The feature function is the log probability output by a trigram language model over this sequence. This is similar to the HMM Alignment model (Vogel, Ney, and Tillmann, 1996) but in this case movement is calculated on the basis of parts of speech. The Projected POS feature function was one of the strongest performing shallow syntactic feature functions, with a %BLEU score of 31.8. This feature function can be thought of as a trade-off between purely word-based models, and full generative models based upon shallow syntax. Syntax-based MT has shown promise in the work of, among others, Wu and Wong (1998) and Alshawi, Bangalore, and Douglas (2000). We hope that adding features based on Treebank-based syntactic analyses of the source and target sentences will address grammatical errors in the output of the baseline system. The most straightforward way to integrate a statistical parser in the system would be the use of the (log of the) parser probability as a feature function. Unfortunately, this feature function did not help to obtain better results (it actually seems to significantly hurt performance). To analyze the reason for this, we performed an experiment to test if the used statistical parser assigns a higher probability to presumably grammatical sentences. The following table shows the average log probability assigned by the Collins parser to the 1-best (produced), oracle and the reference translations: We observe that the average parser log-probability of the 1-best translation is higher than the average parse log probability of the oracle or the reference translations. Hence, it turns out that the parser is actually assigning higher probabilities to the ungrammatical MT output than to the presumably grammatical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f|T(e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English parse tree-Chinese sentence pairs. The probability of these operations θ(ek,) is assumed to depend on the edge of the tree being modified, eke, but independent of everything else, giving the following equation, where O varies over the possible alignments between the f and e and θ(ekj) is the particular operations (in O) for the edge eke. The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). An English phrase covered by a node can be directly translated into a Chinese phrase without regular reorderings, insertions, and leafword translations. The model was trained using about 780,000 English parse tree-Chinese sentence pairs. There are about 3 million words on the English side, and they were parsed by Collins’ parser. Since the model is computationally expensive, we added some limitations on the model operations. As the base MT system does not produce a translation with a big word jump, we restrict the model not to reorder child nodes when the node covers more than seven words. For a node that has more than four children, the reordering probability is set to be uniform. We also introduced pruning, which discards partial (subtree-substring) alignments if the probability is lower than a threshold. The model gives a sum of all possible alignment probabilities for a pair of a Chinese sentence and an English parse tree. We also calculate the probability of the best alignment according to the model. Thus, we have the folAs the model is computationally expensive, we sorted the n-best list by the sentence length, and processed them from the shorter ones to the longer ones. We used 10 CPUs for about five days, and 273/997 development sentences and 237/878 test sentences were processed. The average %BLEU score (average of the best four among different 20 search initial points) was 31.7 for both hTreeToStringSum and hTreeToStringViterbi. Among the processed development sentences, the model preferred the oracle sentences over the produced sentence in 61% of the cases. The biggest problem of this model is that it is computationally very expensive. It processed less than 30% of the n-best lists in long CPU hours. In addition, we processed short sentences only. For long sentences, it is not practical to use this model as it is. A tree-to-tree translation model makes use of syntactic tree for both the source and target language. As in the tree-to-string model, a set of operations apply, each with some probability, to transform one tree into another. However, when training the model, trees for both the source and target languages are provided, in our case from the Chinese and English parsers. We began with the tree-to-tree alignment model presented by Gildea (2003). The model was extended to handle dependency trees, and to make use of the word-level alignments produced by the baseline MT system. The probability assigned by the tree-to-tree alignment model, given the word-level alignment with which the candidate translation was generated, was used as a feature in our rescoring system. We trained the parameters of the tree transformation operations on 42,000 sentence pairs of parallel ChineseEnglish data from the Foreign Broadcast Information Service (FBIS) corpus. The lexical translation probabilities Pt were trained using IBM Model 1 on the 30 million word training corpus. This was done to overcome the sparseness of the lexical translation probabilities estimated while training the tree-to-tree model, which was not able to make use of as much training data. As a test of the tree-to-tree model’s discrimination, we performed an oracle experiment, comparing the model scores on the first sentence in the n-best list with candidate giving highest BLEU score. On the 1000-best list for the 993-sentence development set, restricting ourselves to sentences with no more than 60 words and a branching factor of no more than five in either the Chinese or English tree, we achieved results for 480, or 48% of the 993 sentences. Of these 480, the model preferred the produced over the oracle 52% of the time, indicating that it does not in fact seem likely to significantly improve BLEU scores when used for reranking. Using the probability of the source Chinese dependency parse aligning with the n-best hypothesis dependency parse as a feature function, making use of the word-level alignments, yields a 31.6 %BLEU score — identical to our baseline. The tree-based feature functions described so far have the following limitations: full parse tree models are expensive to compute for long sentences and for trees with flat constituents and there is limited reordering observed in the n-best lists that form the basis of our experiments. In addition to this, higher levels of parse tree are rarely observed to be reordered between source and target parse trees. In this section we attack these problems using a simple Markov model for tree-based alignments. It guarantees tractability: compared to a coverage of approximately 30% of the n-best list by the unconstrained tree-based models, using the Markov model approach provides 98% coverage of the n-best list. In addition, this approach is robust to inaccurate parse trees. The algorithm works as follows: we start with word alignments and two parameters: n for maximum number of words in tree fragment and k for maximum height of tree fragment. We proceed from left to right in the Chinese sentence and incrementally grow a pair of subtrees, one subtree in Chinese and the other in English, such that each word in the Chinese subtree is aligned to a word in the English subtree. We grow this pair of subtrees until we can no longer grow either subtree without violating the two parameter values n and k. Note that these aligned subtree pairs have properties similar to alignment templates. They can rearrange in complex ways between source and target. Figure 2 shows how subtree-pairs for parameters n = 3 and k = 3 can be drawn for this sentence pair. In our experiments, we use substantially bigger tree fragments with parameters set to n = 8 and k = 9. Once these subtree-pairs have been obtained, we can easily assert a Markov assumption for the tree-to-tree and tree-to-string translation models that exploits these pairings. Let consider a sentence pair in which we have discovered n subtree-pairs which we can call Frag0, ..., Fragn. We can then compute a feature function for the sentence pair using the tree-to-string translation model as follows: the Tree to String model described in Section 6.2 we obtain a coverage improvement to 98% coverage from the original 30%. The accuracy of the tree to string model also improved with a %BLEU score of 32.0 which is the best performing single syntactic feature. In this section, we consider another method for carving up the full parse tree. However, in this method, instead of subtree-pairs we consider a decomposition of parse trees that provides each word with a fragment of the original parse tree as shown in Figure 3. The formalism of TreeAdjoining Grammar (TAG) provides the definition what each tree fragment should be and in addition how to decompose the original parse trees to provide the fragments. Each fragment is a TAG elementary tree and the composition of these TAG elementary trees in a TAG derivation tree provides the decomposition of the parse trees. The decomposition into TAG elementary trees is done by augmenting the parse tree for source and target sentence with head-word and argument (or complement) information using heuristics that are common to most contemporary statistical parsers and easily available for both English and Chinese. Note that we do not use the word alignment information for the decomposition into TAG elementary trees. Once we have a TAG elementary tree per word, we can create several models that score word alignments by exploiting the alignments between TAG elementary trees between source and target. Let tfi and tei be the TAG elementary trees associated with the aligned words fi and ei respectively. We experimented with two models over alignments: unigram model over alignments: ni P(fi, tfi, ei, tei) and conditional model: Hi P(ei, tei  |fi, tfi) × P(fi+1, tfi+1  |fi,tfi) We trained both of these models using the SRI Language Modeling Toolkit using 60K aligned parse trees. We extracted 1300 TAG elementary trees each for Chinese and for English. The unigram model gets a %BLEU score of 31.7 and the conditional model gets a %BLEU score of 31.9. ture added to the baseline features on its own, and a combination of new features. The use of discriminative reranking of an n-best list produced with a state-of-the-art statistical MT system allowed us to rapidly evaluate the benefits of off-the-shelf parsers, chunkers, and POS taggers for improving syntactic well-formedness of the MT output. Results are summarized in Table 2; the best single new feature improved the %BLEU score from 31.6 to 32.5. The 95% confidence intervals computed with the bootstrap resampling method are about 0.8%. In addition to experiments with single features we also integrated multiple features using a greedy approach where we integrated at each step the feature that most improves the BLEU score. This feature integration produced a statistically significant improvement of absolute 1.3% to 32.9 %BLEU score. Our single best feature, and in fact the only single feature to produce a truly significant improvement, was the IBM Model 1 score. We attribute its success that it addresses the weakness of the baseline system to omit content words and that it improves word selection by employing a triggering effect. We hypothesize that this allows for better use of context in, for example, choosing among senses of the source language word. A major goal of this work was to find out if we can exploit annotated data such as treebanks for Chinese and English and make use of state-of-the-art deep or shallow parsers to improve MT quality. Unfortunately, none of the implemented syntactic features achieved a statistically significant improvement in the BLEU score. Potential reasons for this might be: tive to the grammaticality of MT output. This could not only make it difficult to see an improvement in the system’s output, but also potentially mislead the BLEU-based optimization of the feature weights. A significantly larger corpus for discriminative training and for evaluation would yield much smaller confidence intervals. • Our discriminative training technique, which directly optimizes the BLEU score on a development corpus, seems to have overfitting problems with large number of features. One could use a larger development corpus for discriminative training or investigate alternative discriminative training criteria. • The amount of annotated data that has been used to train the taggers and parsers is two orders of magnitude smaller than the parallel training data that has been used to train the baseline system (or the word-based features). Possibly, a comparable amount of annotated data (e.g. a treebank with 100 million words) is needed to obtain significant improvements. This is the first large scale integration of syntactic analysis operating on many different levels with a state-of-theart phrase-based MT system. The methodology of using a log-linear feature combination approach, discriminative reranking of n-best lists computed with a state-of-the-art baseline system allowed members of a large team to simultaneously experiment with hundreds of syntactic feature functions on a common platform. This material is based upon work supported by the National Science Foundation under Grant No. 0121285.
Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics. These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality. Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006). Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent. Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human judgments, making it difficult and expensive to use. In addition, because HTER treats all edits equally, no distinction is made between serious errors (errors in names or missing subjects) and minor edits (such as a difference in verb agreement or a missing determinator). Different types of translation errors vary in importance depending on the type of human judgment being used to evaluate the translation. For example, errors in tense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent. On the other hand, deletion of content words might not lower the fluency of a translation but the adequacy would suffer. In this paper, we examine these differences by taking an automatic evaluation metric and tuning it to these these human judgments and examining the resulting differences in the parameterization of the metric. To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments. Section 2 summarizes the TER metric and discusses how TERp improves on it. Correlation results with human judgments, including independent results from the 2008 NIST Metrics MATR evaluation, where TERp was consistently one of the top metrics, are presented in Section 3 to show the utility of TERp as an evaluation metric. The generation of paraphrases, as well as the effect of varying the source of paraphrases, is discussed in Section 4. Section 5 discusses the results of tuning TERp to Fluency, Adequacy and HTER, and how this affects the weights of various edit types. Both TER and TERp are automatic evaluation metrics for machine translation that score a translation, the hypothesis, of a foreign language text, the source, against a translation of the source text that was created by a human translator, called a reference translation. The set of possible correct translations is very large—possibly infinite— and any single reference translation is just a single point in that space. Usually multiple reference translations, typically 4, are provided to give broader sampling of the space of correct translations. Automatic MT evaluation metrics compare the hypothesis against this set of reference translations and assign a score to the similarity; higher scores are given to hypotheses that are more similar to the references. In addition to assigning a score to a hypothesis, the TER metric also provides an alignment between the hypothesis and the reference, enabling it to be useful beyond general translation evaluation. While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference translation divided by the length of the reference translation. Unlike speech recognition, there are many correct translations for any given foreign sentence. These correct translations differ not only in their word choice but also in the order in which the words occur. WER is generally seen as inadequate for evaluation for machine translation as it fails to combine knowledge from multiple reference translations and also fails to model the reordering of words and phrases in translation. TER addresses the latter failing of WER by allowing block movement of words, called shifts. within the hypothesis. Shifting a phrase has the same edit cost as inserting, deleting or substituting a word, regardless of the number of words being shifted. While a general solution to WER with block movement is NP-Complete (Lopresti and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Phrase Substitutions. TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980). Two words are determined to be synonyms if they share the same synonym set according to WordNet (Fellbaum, 1998). Sequences of words in the reference are considered to be paraphrases of a sequence of words in the hypothesis if that phrase pair occurs in the TERp phrase table. The TERp phrase table is discussed in more detail in Section 4. With the exception of the phrase substitutions, the cost for all other edit operations is the same regardless of what the words in question are. That is, once the edit cost of an operation is determined via optimization, that operation costs the same no matter what words are under consideration. The cost of a phrase substitution, on the other hand, is a function of the probability of the paraphrase and the number of edits needed to align the two phrases according to TERp. In effect, the probability of the paraphrase is used to determine how much to discount the alignment of the two phrases. Specifically, the cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: where w1, w2, w3, and w4 are the 4 free parameters of the edit cost, edit(p1,p2) is the edit cost according to TERp of aligning p1 to p2 (excluding phrase substitutions) and Pr(p1,p2) is the probability of paraphrasing p1 as p2, obtained from the TERp phrase table. The w parameters of the phrase substitution cost may be negative while still resulting in a positive phrase substitution cost, as w2 is multiplied by the log probability, which is always a negative number. In practice this term will dominate the phrase substitution edit cost. This edit cost for phrasal substitutions is, therefore, specified by four parameters, w1, w2, w3 and w4. Only paraphrases specified in the TERp phrase table are considered for phrase substitutions. In addition, the cost for a phrasal substitution is limited to values greater than or equal to 0, i.e., the substitution cost cannot be negative. In addition, the shifting constraints of TERp are also relaxed to allow shifting of paraphrases, stems, and synonyms. In total TERp uses 11 parameters out of which four represent the cost of phrasal substitutions. The match cost is held fixed at 0, so that only the 10 other parameters can vary during optimization. All edit costs, except for the phrasal substitution parameters, are also restricted to be positive. A simple hill-climbing search is used to optimize the edit costs by maximizing the correlation of human judgments with the TERp score. These correlations are measured at the sentence, or segment, level. Although it was done for the experiments described in this paper, optimization could also be performed to maximize document level correlation – such an optimization would give decreased weight to shorter segments as compared to the segment level optimization. The optimization of the TERp edit costs, and comparisons against several standard automatic evaluation metrics, using human judgments of Adequacy is first described in Section 3.1. We then summarize, in Section 3.2, results of the NIST Metrics MATR workshop where TERp was evaluated as one of 39 automatic metrics using many test conditions and types of human judgments. As part of the 2008 NIST Metrics MATR workshop (Przybocki et al., 2008), a development subset of translations from eight Arabic-to-English MT systems submitted to NIST’s MTEval 2006 was released that had been annotated for Adequacy. We divided this development set into an optimization set and a test set, which we then used to optimize the edit costs of TERp and compare it against other evaluation metrics. TERp was optimized to maximize the segment level Pearson correlation with adequacy on the optimization set. The edit costs determined by this optimization are shown in Table 1. We can compare TERp with other metrics by comparing their Pearson and Spearman correlations with Adequacy, at the segment, document and system level. Document level Adequacy scores are determined by taking the length weighted average of the segment level scores. System level scores are determined by taking the weighted average of the document level scores in the same manner. We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). The IBM version of BLEU was used in case insensitive mode with an ngram-size of 4 to calculate the BLEU scores. Case insensitivity was used with BLEU as it was found to have much higher correlation with Adequacy. In addition, we also examined BLEU using an ngram-size of 2 (labeled as BLEU-2), instead of the default ngram-size of 4, as it often has a higher correlation with human judgments. When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used. TER was also used in case insensitive mode. We show the Pearson and Spearman correlation numbers of TERp and the other automatic metrics on the optimization set and the test set in Tables 2 and 3. Correlation numbers that are statistically indistinguishable from the highest correlation, using a 95% confidence interval, are shown in bold and numbers that are actually not statistically significant correlations are marked with a †. TERp has the highest Pearson correlation in all conditions, although not all differences are statistically significant. When examining the Spearman correlation, TERp has the highest correlation on the segment and system levels, but performs worse than METEOR on the document level Spearman correlatons. TERp was one of 39 automatic metrics evaluated in the 2008 NIST Metrics MATR Challenge. In order to evaluate the state of automatic MT evaluation, NIST tested metrics across a number of conditions across 8 test sets. These conditions included segment, document and system level correlations with human judgments of preference, fluency, adequacy and HTER. The test sets included translations from Arabic-to-English, Chinese-toEnglish, Farsi-to-English, Arabic-to-French, and English-to-French MT systems involved in NIST’s MTEval 2008, the GALE (Olive, 2005) Phase 2 and Phrase 2.5 program, Transtac January and July 2007, and CESTA run 1 and run 2, covering multiple genres. The version of TERp submitted to this workshop was optimized as described in Section 3.1. The development data upon which TERp was optimized was not part of the test sets evaluated in the Challenge. Due to the wealth of testing conditions, a simple overall view of the official MATR08 results released by NIST is difficult. To facilitate this analysis, we examined the average rank of each metric across all conditions, where the rank was determined by their Pearson and Spearman correlation with human judgments. To incorporate statistical significance, we calculated the 95% confidence interval for each correlation coefficient and found the highest and lowest rank from which the correlation coefficient was statistically indistinguishable, resulting in lower and upper bounds of the rank for each metric in each condition. The average lower bound, actual, and upper bound ranks (where a rank of 1 indicates the highest correlation) of the top metrics, as well as BLEU and TER, are shown in Table 4, sorted by the average upper bound Pearson correlation. Full descriptions of the other metrics3, the evaluation results, and the test set composition are available from NIST (Przybocki et al., 2008). This analysis shows that TERp was consistently one of the top metrics across test conditions and had the highest average rank both in terms of Pearson and Spearman correlations. While this analysis is not comprehensive, it does give a general idea of the performance of all metrics by synthesizing the results into a single table. There are striking differences between the Spearman and Pearson correlations for other metrics, in particular the CDER metric (Leusch et al., 2006) had the second highest rank in Spearman correlations (after TERp), but was the sixth ranked metric according to the Pearson correlation. In several cases, TERp was not the best metric (if a metric was the best in all conditions, its average rank would be 1), although it performed well on average. In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER. TERp uses probabilistic phrasal substitutions to align phrases in the hypothesis with phrases in the reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify English-to-F phrasal correspondences, then map from English to English by following translation units from English to F and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they may be considered to be paraphrases of each other with the following probability: If there are several pivot phrases that link the two English phrases, then they are all used in computing the probability: The corpus used for extraction was an ArabicEnglish newswire bitext containing a million sentences. A few examples of the extracted paraphrase pairs that were actually used in a run of TERp on the Metrics MATR 2008 development set are shown below: (brief -* short) (controversy over -* polemic about) (by using power -* by force) (response -* reaction) A discussion of paraphrase quality is presented in Section 4.1, followed by a brief analysis of the effect of varying the pivot corpus used by the automatic paraphrase generation upon the correlation performance of the TERp metric in Section 4.2. We analyzed the utility of the paraphrase probability and found that it was not always a very reliable estimate of the degree to which the pair was semantically related. For example, we looked at all paraphrase pairs that had probabilities greater than 0.9, a set that should ideally contain pairs that are paraphrastic to a large degree. In our analysis, we found the following five kinds of paraphrases in this set: pairs only differ in the morphological form for one of the words. As the examples show, any knowledge that these pairs may provide is already available to TERp via stemming. (50 ton → 50 tons) (caused clouds → causing clouds) (syria deny → syria denies) Given this distribution of the pivot-based paraphrases, we experimented with a variant of TERp that did not use the paraphrase probability at all but instead only used the actual edit distance between the two phrases to determine the final cost of a phrase substitution. The results for this experiment are shown in the second row of Table 5. We can see that this variant works as well as the full version of TERp that utilizes paraphrase probabilities. This confirms our intuition that the probability computed via the pivot-method is not a very useful predictor of semantic equivalence for use in TERp. To determine the effect that the pivot language might have on the quality and utility of the extracted paraphrases in TERp, we used paraphrase pairsmade available by Callison-Burch (2008). These paraphrase pairs were extracted from Europarl data using each of 10 European languages (German, Italian, French etc.) as a pivot language separately and then combining the extracted paraphrase pairs. Callison-Burch (2008) also extracted and made available syntactically constrained paraphrase pairs from the same data that are more likely to be semantically related. We used both sets of paraphrases in TERp as alternatives to the paraphrase pairs that we extracted from the Arabic newswire bitext. The results are shown in the last four rows of Table 5 and show that using a pivot language other than the one that the MT system is actually translating yields results that are almost as good. It also shows that the syntactic constraints imposed by Callison-Burch (2008) on the pivot-based paraphrase extraction process are useful and yield improved results over the baseline pivot-method. The results further support our claim that the pivot paraphrase probability is not a very useful indicator of semantic relatedness. To evaluate the differences between human judgment types we first align the hypothesis to the references using a fixed set of edit costs, identical to the weights in Table 1, and then optimize the edit costs to maximize the correlation, without realigning. The separation of the edit costs used for alignment from those used for scoring allows us to remove the confusion of edit costs selected for alignment purposes from those selected to increase correlation. For Adequacy and Fluency judgments, the MTEval 2002 human judgement set4 was used. This set consists of the output of ten MT systems, 3 Arabic-to-English systems and 7 Chineseto-English systems, consisting of a total, across all systems and both language pairs, of 7,452 segments across 900 documents. To evaluate HTER, the GALE (Olive, 2005) 2007 (Phase 2.0) HTER scores were used. This set consists of the output of 6 MT systems, 3 Arabic-to-English systems and 3 Chinese-to-English systems, although each of the systems in question is the product of system combination. The HTER data consisted of a total, across all systems and language pairs, of 16,267 segments across a total of 1,568 documents. Because HTER annotation is especially expensive and difficult, it is rarely performed, and the only source, to the authors’ knowledge, of available HTER annotations is on GALE evaluation data for which no Fluency and Adequacy judgments have been made publicly available. The edit costs learned for each of these human judgments, along with the alignment edit costs are shown in Table 6. While all three types of human judgements differ from the alignment costs used in alignment, the HTER edit costs differ most significantly. Unlike Adequacy and Fluency which have a low edit cost for insertions and a very high cost for deletions, HTER has a balanced cost for the two edit types. Inserted words are strongly penalized against in HTER, as opposed to in Adequacy and Fluency, where such errors are largely forgiven. Stem and synonym edits are also penalized against while these are considered equivalent to a match for both Adequacy and Fluency. This penalty against stem matches can be attributed to Fluency requirements in HTER that specifically penalize against incorrect morphology. The cost of shifts is also increased in HTER, strongly penalizing the movement of phrases within the hypothesis, while Adequacy and Fluency give a much lower cost to such errors. Some of the differences between HTER and both fluency and adequacy can be attributed to the different systems used. The MT systems evaluated with HTER are all highly performing state of the art systems, while the systems used for adequacy and fluency are older MT systems. The differences between Adequacy and Fluency are smaller, but there are still significant differences. In particular, the cost of shifts is over twice as high for the fluency optimized system than the adequacy optimized system, indicating that the movement of phrases, as expected, is only slightly penalized when judging meaning, but can be much more harmful to the fluency of a translation. Fluency however favors paraphrases more strongly than the edit costs optimized for adequacy. This might indicate that paraphrases are used to generate a more fluent translation although at the potential loss of meaning. We introduced a new evaluation metric, TER-Plus, and showed that it is competitive with state-of-theart evaluation metrics when its predictions are correlated with human judgments. The inclusion of stem, synonym and paraphrase edits allows TERp to overcome some of the weaknesses of the TER metric and better align hypothesized translations with reference translations. These new edit costs can then be optimized to allow better correlation with human judgments. In addition, we have examined the use of other paraphrasing techniques, and shown that the paraphrase probabilities estimated by the pivot-method may not be fully adequate for judgments of whether a paraphrase in a translation indicates a correct translation. This line of research holds promise as an external evaluation method of various paraphrasing methods. However promising correlation results for an evaluation metric may be, the evaluation of the final output of an MT system is only a portion of the utility of an automatic translation metric. Optimization of the parameters of an MT system is now done using automatic metrics, primarily BLEU. It is likely that some features that make an evaluation metric good for evaluating the final output of a system would make it a poor metric for use in system tuning. In particular, a metric may have difficulty distinguishing between outputs of an MT system that been optimized for that same metric. BLEU, the metric most frequently used to optimize systems, might therefore perform poorly in evaluation tasks compared to recall oriented metrics such as METEOR and TERp (whose tuning in Table 1 indicates a preference towards recall). Future research into the use of TERp and other metrics as optimization metrics is needed to better understand these metrics and the interaction with parameter optimization. Finally, we explored the difference between three types of human judgments that are often used to evaluate both MT systems and automatic metrics, by optimizing TERp to these human judgments and examining the resulting edit costs. While this can make no judgement as to the preference of one type of human judgment over another, it indicates differences between these human judgment types, and in particular the difference between HTER and Adequacy and Fluency. This exploration is limited by the the lack of a large amount of diverse data annotated for all human judgment types, as well as the small number of edit types used by TERp. The inclusion of additional more specific edit types could lead to a more detailed understanding of which translation phenomenon and translation errors are most emphasized or ignored by which types of human judgments. This work was supported, in part, by BBN Technologies under the GALE Program, DARPA/IPTO Contract No. HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/.
Robust Applied Morphological Generation natural language generation sysit often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application. Most approaches to natural language generation (NLG) ignore morphological variation during word choice, postponing the computation of the actual word forms to be output to a final stage, sometimes termed clinearisation'. The advantage of this setup is that the syntactic/lexical realisation component does not have to consider all possible word forms corresponding to each lemma (Shieber et al., 1990). In practice, it is advantageous to have morphological generation as a postprocessing component that is separate from the rest of the NLG system. A benefit is that since there are no competing claims on the representation framework from other types of linguistic and non-linguistic knowledge, the developer of the morphological generator is free to express morphological information in a perspicuous and elegant manner. A further benefit is that localising morphological knowledge in a single component facilitates more systematic and reliable updating. From a software engineering perspective. modularisation is likely to reduce system development costs and increase system reliability. As an individual module, the morphological generator will be more easily shareable between several different NLG applications, and integrated into new ones. Finally, such a generator can be used on its own in other types of applications that do not contain a standard NLG syntactic/lexical realisation component, such as text simplification (see Section 3). In this paper we describe a fast and robust generator for the inflectional morphology of English that generates a word form given a specification of a lemma, a part-of-speech (PoS) label, and an inflectional type. The morphological generator was built using data from several large corpora and machine readable dictionaries. It does not contain an explicit lexicon or word-list, but instead comprises a set of morphological generalisations together with a list of exceptions for specific (irregular) word forms. This organisation into generalisations and exceptions can save time and effort in system development since the addition of new vocabulary that has regular morphology does not require any changes to the generator. In addition, the generalisation-exception architecture can be used to specify----and also override—preferences in cases where a lemma has more than one possible surface word form given a particular inflectional type and PoS label. The generator is packaged up as a Unix 'filter', making it easy to integrate into applications. It is based on efficient finite-state techniques, and is implemented using the widely available Unix Flex utility (a reimplementation of the AT&T Unix Lex tool) (Levine et al., 1992). The generator is freely available to the NLG research community (see Section 5 below). The paper is structured as follows. Section 2 describes the morphological generator and evaluates its accuracy. Section 3 outlines how the generator is put to .use in a prototype system for automatic simplification of text, and discusses a number of practical morphological and orthographic issues that we have encountered. Section 4 relates our work to that of others, and we conclude (Section 5) with directions for future work. The morphological generator covers the productive English affixes s for the plural form of nouns and the third person singular present tense of verbs, and ed for the past tense, en for the past participle, and ing for the present participle forms of verbs.' The generator is implemented in Flex. The standard use of Flex is to construct 'scanners', programs that recognise lexical patterns in text (Levine et al., 1992). A Flex description—the high-level description of a scanner that Flex takes as input—consists of a set of 'rules': pairs of regular expression patterns (which Flex compiles into deterministic finite-state automata (Aho et al., 1986)), and actions consisting of arbitrary C code. Flex creates as output a C program which at run-time scans a text looking for occurrences of the regular expressions. Whenever it finds one, it executes the corresponding C code. Flex is part of the Berkeley Unix distribution and as a result Flex programs are very portable. The standard version of Flex works with any ISO-8559 character set; Unicode support is also available. The morphological generator expects to receive as input a sequence of tokens of the form lemma inflection_label. where lemma specifies the lemma of the word form to be generated, inflection specifies the type of inflection (i.e. s, ed, en or ing), and label specifies the PoS of the word form. The PoS labels follow the same pattern as in the Lancaster CLAWS tag sets (Garside et al., 1987; Burnard, 1995). with noun tags starting with N. etc. The symbols + and _ are delimiters. An example of a morphological generator rule is given in (1). We do not curreutly cover comparative and superlative forms of adjectives or adverbs since t heir pro Ind ivit,: is much less predictable. .L....:=Iret.urn(nRatord_form(1,! !es&quot;));} The left-hand side of the rule is a regular expression. The braces signify exactly one occurrence of an element of the character set abbreviated by the symbol A; we assume here that A abbreviates the upper and lower case letters of the alphabet. The next symbol + specifies that there must be a se,querire of one anniore characters, each belonging to the character set abbreviated by A. Double quotes indicate literal character symbols. The right-hand side of the rule gives the C code to be executed when an input string matches the regular expression. When the Flex rule matches the input address-I-8_1V, for example, the C function np_vord_form (defined elsewhere in the generator) is called to determine the word form corresponding to the input: the function deletes the inflection type and PoS label specifications and the delimiters, removes the last character of the lemma, and finally attaches the characters es; the word form generated is thus addresses. Of course not all plural noun inflections are correctly generated by the rule in (1) since there are many irregularities and subregularities. These are dealt with using additional, more specific, rules. The order in which these rules are applied to the input follows exactly the order in which the rules appear in the Flex description. This makes for a very simple and perspicuous way to express generalizations and exceptions. For instance, the rule in (2) generates the plural form of many English nouns that originate from Latin, such as stimulus. With the input stimulus-f-s_N, the output is stimuli rather than the incorrect *stimuluses that would follow from the application of the more general rule in (1). By ensuring that this rule precedes the rule in (1) in the description, nouns such as stimutus get the correct plural form inflection. Some other words in this class, though, do not have the Latinate plural form (e.g. *boni as a plural form of bonus); in these cases the generator contains rules specifying the correct forms as exceptions. The rules constituting&quot;the generator do not necessarily have to be mutually exclusive, so they can be used to capture the inflectional morphology of lemmata that have more than one possible inflected form given a specific PoS label and inflectional type. An example of this is the multiple inflections of the noun cactus, which has not only the Latinate plural form cacti but also the English-plural-form cactuses: • In addition, inflections of some words differ according to dialect. For example, the past participle form of the verb to bear is borne in British English, whereas in American English the preferred word form is born. In cases where there is more than one possible inflection for a particular input lemma, the order of the rules in the Flex description determines the inflectional preference. For example, with the noun cactus, the fact that the rule in (2) precedes the one in (1) causes the generator to output the word form cacti rather than cactuses even though both rules are applicable.2 It is important to note, though, that the generator will always choose between multiple inflections: there is no way for it to output all possible word forms for a particular input.3 An important issue concerning morphological generation that is closely related to that of inflectional preference is consonant doubling. This phenomenon, occurring mainly in British English, involves the doubling of a consonant at the end of a lemma when the lemma is inflected. For example, the past tense/participle inflection of the verb to travel is travelled in British English, where the final consonant of the lemma is doubled before the suffix is attached. In American English the past tense/participle inflection of the verb to travel is usually spelt traveled. Consonant doubling is triggered on the basis of both orthographic and phonological information: when a word ends in one vowel 2ftu1e choice based on ordering in the description can in fact be overridden by arranging for the second or subsequent match to cover a larger part of the input so that the longest match heuristic applies (Levine et al., 1992). Hut note that the rules in (1) and (2) will always match the same input span_ 3Flex does not allow the use of rules that have Heniteal left-hand side regular expressions. followed by one consonant and the last part of the-word is - stressed-,--in: -general. the ,corisoriant is doubled (Procter, 1995). However there are exceptions to this, and in any case the input to the morphological generator does not contain information about stress. Consider the Flex rule in (3), where the symbols C and V abbreviate the character sets consisting of (upper and lower case) consonants and ,vowels,- respectively. Given the input submit-i-ed_V this rule correctly generates submitted. However, the verb to exhibit does not undergo consonant doubling so this rule will generate, incorrectly, the word form exhibitted. In order to ensure that the correct inflection of a verb is generated, the morphological generator uses a list of (around 1,100) lemmata that allow consonant doubling, extracted automatically from the British National Corpus (BNC; Burnard, 1995). The list is checked before inflecting verbs. Given the fact that there are many more verbs that do not allow consonant doubling, listing the verbs that do is the most economic solution. An added benefit is that if a lemma does allow consonant doubling but is not included in the list then the word form generated will still be correct with respect to American English. The morphological generator comprises a set of of approximately 1,650 rules expressing morphological regularities, subregularities, and exceptions for specific words; also around 350 lines of C/Flex code for program initialisation and defining the functions called by the rule actions. The rule set is in fact obtained by automatically reversing a morphological analyser. This is a much enhanced version of the analyser originally developed for the GATE system (Cunningham et al., 1996). Minnen and Carroll (Under review) describe in detail how the reversal is performed. The generator executable occupies around 700Kb on disc. The analyser--and therefore the generator includes exception lists derived from WordNet (version 1.5: Miller et al., 1993). In addition. we have incorporated data acquired semiautomatically from the following corpora and machine readable, dictionaries: the LOB corpus (Garside et al., 1987), the Penn Treebank (Marcus et al., 1993), the SUSANNE corpus (Sampson, 1995), the Spoken English Corpus (Taylor and Knowles, 1988), the Oxford Psycholinguistic Database (Quinlan, 1992), and the &quot;Computer-Usable&quot; version of the Oxford Advanced Learner's Dictionary of Current English (OALDCE; Mitton, 1992). Minnen and Carroll (Under review) report an evaluation of the accuracy of the morphological generator with respect to the CELEX lexical database (version 2.5; Baayen et al., 1993). This threw up a small number of errors which we have now fixed. We have rerun the CELEXbased evaluation: against the past tense, past and present participle, and third person singular present tense inflections of verbs, and all plural nouns. After excluding multi-word entries (phrasal verbs, etc.) we were left with 38,882 out of the original 160,595 word forms. For each of these word forms we fed the corresponding input (derived automatically from the lemmatisation and inflection specification provided by CELEX) to the generator. We compared the generator output with the original CELEX word forms, producing a list of mistakes apparently made by the generator, which we then checked by hand. In a number of cases either the CELEX lemmatisation was wrong in that it disagreed with the relevant entry in the Cambridge International Dictionary of English (Procter, 1995), or the output of the generator was correct even though it was not identical to the word form given in CELEX. We did not count these cases as mistakes. We also found that CELEX is inconsistent with respect to consonant doubling. For example, it includes the word form pettifogged,1 whereas it omits many consonant doubled words that are much more common (according to counts from the BNC). For example, the BNC contains around 850 occurrences of the word form programming tagged as a verb. but this form is not present in CELEX. The form programing does occur in CELEX. but does not in the BNC. 'A rare word, meaning to be overly concerned with small, unimportant details. We did not count these cases as mistakes either. Of the remaining 359% mistakes 346-cOneerned word forms that do not occur at all in the 100M words of the BNC. We categorised these as irrelevant for practical applications and so discarded them. Thus the type accuracy of the morphological analyser with respect to the CELEX lexical database is 99.97%. The token accuracy is 99.98% with respect to the 14,825,661 relevant tokens inthe BNC (Le—a.rate.of two errors per ten thousand words). We tested the processing speed of the generator on a Sun Ultra 10 workstation. In order to discount program startup times (which are anyway only of the order of 0.05 seconds) we used input files of 400K and 800K tokens and recorded the difference in timings; we took the averages of 10 runs. Despite its wide coverage the morphological generator is very fast: it generates at a rate of more than 80,000 words per second.5 The morphological generator forms part of a prototype system for automatic simplification of English newspaper text (Carroll et al., 1999). The goal is to help people with aphasia (a language impairment typically occurring as a result of a stroke or head injury) to better understand English newspaper text. The system comprises two main components: an analysis module which downloads the source newspaper texts from the web and computes syntactic analyses for the sentences in them, and a simplification module which operates on the output of the analyser to improve the comprehensibility of the text. Syntactic simplification (Canning and Tait, 1999) operates on the syntax trees produced in the analysis phase, for example converting sentences in the passive voice to active, and splitting long sentences at appropriate points. A subsequent lexical simplification stage (Devlin and Tait, 1998) replaces difficult or rare content words with simpler synonyms. The analysis component contains a morphological analyser, and it is the base forms of is likely that a modest increase in speed could he obtained by specifying optimisation levels in Flex and gcc that are higher than the defaults. words that are passed through the system; this eases the task of the lexical simplification module. The final processing stage in the system is therefore morphological generation, using the generator described in the previous section. We are currently testing the components of the simplification system on a corpus of 1000 news stories downloaded from the :Sund,erland Echo (a local newspaper in North-East England). In our testing we have found that newly encountered vocabulary only rarely necessitates any modification to the generator (or rather the analyser) source; if the word has regular morphology then it is handled by the rules expressing generalisations. Also, a side-effect of the fact that the generator is derived from the analyser is that the two modules have exactly the same coverage and are guaranteed to stay in step with each other. This is important in the context of an applied system. The accuracy of the generator is quite sufficient for this application; our experience is that typographical mistakes in the original newspaper text are much more common than errors in morphological processing. Some orthographic phenomena span more than one word. These cannot be dealt with in morphological generation since this works strictly a word at a time. We have therefore implemented a final orthographic postprocessing stage. Consider the sentence:6 (4) *Brian Cookman is the attraction at the King '8 Arms on Saturday night and he will be back on Sunday night for a acoustic jam session. This is incorrect orthographically because the determiner in the final noun phrase should be an, as in an acoustic jam session. In fact an must be used if the following word starts with a vowel sound, and a otherwise. We achieve this, again using a filter implemented in Flex, with a set of general rules keying off the next word's first letter (having skipped any intervening sentence-internal punctuation), together with a list of exceptions (e.g. heir, unanimous) •-,collected -us ingthe:pronunciabion information in the OALDCE, supplemented by further cases (e.g. unidimensional) found in the BNC. In the case of abbreviations or acronyms (recognised by the occurrence of non-word-initial capital letters and trailing full-stops) we key off the pronunciation of the first letter considered in isolation. Similarly, .the orthography of the .genitive marker cannot be determined without taking context into account, since it depends on the identity of the last letter of the preceding word. In the sentence in (4) we need only eliminate the space before the genitive marking, obtaining King's Arms. But, following the newspaper style guide, if the preceding word ends in s or z we have to 'reduce' the marker as in, for example, Stacey Edwards' skilful fingers. The generation of contractions presents more of a problem. For example, changing he will to he'll would make (4) more idiomatic. But there are cases where this type of contraction is not permissible. Since these cases seem to be dependent on syntactic context (see Section 4 below), and we have syntactic structure from the analysis phase, we are in a good position to make the correct choice. However, we have not yet tackled this issue and currently take the conservative approach of not contracting in any circumstances. We are following a well-established line of research into the use of finite-state techniques for lexical and shallow syntactic NLP tasks (e.g. Karttunen et al. (1996)). Lexical transducers have been used extensively for morphological analysis, and in theory a finite-state transducer implementing an analyser can be reversed to produce a generator. However, we are not aware of published research on finite-state morphological generators (1) establishing whether in practice they perform with similar efficiency to morphological analysers, (2) quantifying their type/token accuracy with respect to an independent, extensive 'gold standard', and (3) indicating how easily they can be integrated into Larger systems. Furthermore, although a number of finite-state compilation toolkits (e.g. Karttunen (1994)) are publicly available or can be licensed for research use, associated largescale linguistic ..descrip Lions—for-example:En— glish morphological lexicons---are usually commercial products and are therefore not freely available to the NLG research community. The work reported here is alsorelated to work on lexicon representation and morphological processing using the DATR representation language (Cahill, 1993; Evans and Gazdar, .1996). However, ..we,..adopt. less • of ar, theoreti-7. cal and more of an engineering perspective, focusing on morphological generation in the context of wide-coverage practical NLG applications. There are also parallels to research in the two-level morphology framework (Koskenniemi, 1983), although in contrast to our approach this framework has required exhaustive lexica and hand-crafted morphological (unification) grammars in addition to orthographic descriptions (van Noord, 1991; Ritchie et al., 1992). The SRI Core Language Engine (Alshawi, 1992) uses a set of declarative segmentation rules which are similar in content to our rules and are used in reverse to generate word forms. The system, however, is not freely available, again requires an exhaustive stem lexicon, and the rules are not compiled into an efficiently executable finite-state machine but are only interpreted. The work that is perhaps the most similar in spirit to ours is that of the LADL group, in their compilation of large lexicons of inflected word forms into finite-state transducers (Mohri, 1996). The resulting analysers run at a comparable speed to our generator and the (compacted) executables are of similar size. However, a full form lexicon is unwieldy and inconvenient to update, and a system derived from it cannot cope gracefully with unknown words because it does not contain generalisations about regular or subregular morphological behaviour. The morphological components of current widely-used NLG systems tend to consist of hard-wired procedural code that is tightly bound to the workings of the rest of the system. For instance, the Nigel grammar (Matthie.ssen, 1984) contains Lisp code that classifies verb, noun and adjective endings, and these classes are picked up by further code inside the KPIVEL system (Bateman., 2000) itself which performs inflectional generation by stripping off variable length trailing strings and concatenating suf, fixes_ Anorphologically..-suhregular4orrns must be entered explicitly in the lexicon, as well as irregular ones. The situation is similar in FUF/SURGE, morphological generation in the SURGE grammar (Elhadad and Robin, 1996) being performed by procedures which inspect lemma endings, strip off trailing strings when appropriate, and concatenate suffixes. -,..:Imicurrent,NLG:systesus,-,Prbhographic information is distributed throughout the lexicon and is applied via the grammar or by hard-wired code. This makes orthographic processing difficult to decouple from the rest of the system, compromising maintainability and ease of reuse. For example, in SURGE, markers for a/an usage can be added to lexical entries for nouns to indicate that their initial sound is consonantor vowel-like, and is contrary to what their orthography would suggest. (This is only a partial solution since adjectives, adverbs and more rarely other parts of speech—can follow the indefinite article and thus need the same treatment). The appropriate indefinite article is inserted by procedures associated with the gramma&quot;. In DRAFTER-2 (Power et aL, 1998), an a/an feature can be associated with any lexical entry, and its value is propagated up to the NP level through leftmost rule daughters in the grammar (Power, personal communication). Both of these systems interleave orthographic processing with other processes in realisation. In addition. neither has a mechanism for stating exceptions for whole subclasses of words, for example those starting us followed by a vowel— such as use and usual—which must be preceded by a. KPML appears not to perform this type of processing at all. We are not aware of any literature describing (practical) NLG systems that generate contractions. However, interesting linguistic research in this direction is reported by PulInni and Zwicky (In preparation. This work investigates the underlying syntactic structure of sentences that block auxiliary reductions, for example those with VP ellipsis as in (5). We have described a generator-for -English flectional morphology. The main features of the generator are: wide coverage and high accuracy It incorporates data from several large corpora and machine readable dictionaries. An evaluation has shown the error rate to be very low. robustness The generator does not contain an explicit lexicon or word-list, but instead comprises a set of morphological generalisations together with a list of exceptions for specific (irregular) words. Unknown words are very often handled correctly by the generalisations. maintainability and ease of use The organisation into generalisations and exceptions can save development time since addition of new vocabulary that has regular morphology does not require any changes to be made. The generator is packaged up as a Unix filter, making it easy to integrate into applications. speed and portability The generator is based on efficient finite-state techniques, and implemented using the widely available Unix Flex utility. freely available The morphological generator and the orthographic postprocessor are freely available to the NLG research community. See <http://www.cogs. susx.ac.uk/lab/nip/carroll/morph.html>. In future work we intend to investigate the use of phonological information in machine readable dictionaries for a inore principled solution to the consonant doubling problem. We also plan to further increase the flexibility of the generator by including an option that allows the user to choose whether it ha.s a preference for generating British or American English This work was funded by UK EPSRC project GR/L53175 PSET: Practical Simplification of English Text', and by an EPSRC Advanced Fellowship to the second author. The original version of the morphological analyser was kindly provided to us by the University of Sheffield GATE project—Chris -Brew, Dale, Gerdemann • • Adam Kilgarriff and Ehud Reiter have suggested improvements to the analyser/generator. Thanks also to the anonymous reviewers for insightful comments.
A Corpus-Based Approach For Building Semantic Lexicons Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon. Semantic information can be helpful in almost all aspects of natural language understanding, including word sense disambiguation, selectional restrictions, attachment decisions, and discourse processing. Semantic knowledge can add a great deal of power and accuracy to natural language processing systems. But semantic information is difficult to obtain. In most cases, semantic knowledge is encoded manually for each application. There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986). While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge. Many domains are characterized by their own sublanguage containing terms and jargon specific to the field. Representing all sublanguages in a single knowledge base would be nearly impossible. Furthermore, domain-specific semantic lexicons are useful for minimizing ambiguity problems. Within the context of a restricted domain, many polysemous words have a strong preference for one word sense, so knowing the most probable word sense in a domain can strongly constrain the ambiguity. We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically. Our system uses a text corpus and a small set of seed words for a category to identify other words that also belong to the category. The algorithm uses simple statistics and a bootstrapping mechanism to generate a ranked list of potential category words. A human then reviews the top words and selects the best ones for the dictionary. Our approach is geared toward fast semantic lexicon construction: given a handful of seed words for a category and a representative text corpus, one can build a semantic lexicon for a category in just a few minutes. In the first section, we describe the statistical bootstrapping algorithm for identifying candidate category words and ranking them. Next, we describe experimental results for five categories. Finally, we discuss our experiences with additional categories and seed word lists, and summarize our results. Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish). Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category. Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques. We begin with a small set of seed words for a category. We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well. As an example, the seed word lists used in our experiments are shown below. Energy: fuel gas gasoline oil power Financial: bank banking currency dollar money Military: army commander infantry soldier troop Vehicle: airplane car jeep plane truck Weapon: bomb dynamite explosives gun rifle The input to our system is a text corpus and an initial set of seed words for each category. Ideally, the text corpus should contain many references to the category. Our approach is designed for domainspecific text processing, so the text corpus should be a representative sample of texts for the domain and the categories should be semantic classes associated with the domain. Given a text corpus and an initial seed word list for a category C, the algorithm for building a semantic lexicon is as follows: The context windows do not cut across sentence boundaries. Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992). We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category. Note that this is not exactly a conditional probability because a single word occurrence can belong to more than one context window. For example, consider the sentence: I bought an AK-47 gun and an M-16 rifle. The word M-16 would be in the context windows for both gun and rifle even though there was just one occurrence of it in the sentence. Consequently, the category score for a word can be greater than 1. 4. Next, we remove stopwords, numbers, and any words with a corpus frequency < 5. We used a stopword list containing about 30 general nouns, mostly pronouns (e.g., I, he, she, they) and determiners (e.g., this, that, those). The stopwords and numbers are not specific to any category and are common across many domains, so we felt it was safe to remove them. The remaining nouns are sorted by category score and ranked so that the nouns most strongly associated with the category appear at the top. 5. The top five nouns that are not already seed words are added to the seed word list dynamically. We then go back to Step 1 and repeat the process. This bootstrapping mechanism dynamically grows the seed word list so that each iteration produces a larger category context. In our experiments, the top five nouns were added automatically without any human intervention, but this sometimes allows non-category words to dilute the growing seed word list. A few inappropriate words are not likely to have much impact, but many inappropriate words or a few highly frequent words can weaken the feedback process. One could have a person verify that each word belongs to the target category before adding it to the seed word list, but this would require human interaction at each iteration of the feedback cycle. We decided to see how well the technique could work without this additional human interaction, but the potential benefits of human feedback still need to be investigated. After several iterations, the seed word list typically contains many relevant category words. But more importantly, the ranked list contains many additional category words, especially near the top. The number of iterations can make a big difference in the quality of the ranked list. Since new seed words are generated dynamically without manual review, the quality of the ranked list can deteriorate rapidly when too many non-category words become seed words. In our experiments, we found that about eight iterations usually worked well. The output of the system is the ranked list of nouns after the final iteration. The seed word list is thrown away. Note that the original seed words were already known to be category members, and the new seed words are already in the ranked list because that is how they were selected.2 Finally, a user must review the ranked list and identify the words that are true category members. How one defines a &quot;true&quot; category member is subjective and may depend on the specific application, so we leave this exercise to a person. Typically, the words near the top of the ranked list are highly associated with the category but the density of category words decreases as one proceeds down the list. The user may scan down the list until a sufficient number of category words is found, or as long as time permits. The words selected by the user are added to a permanent semantic lexicon with the appropriate category label. Our goal is to allow a user to build a semantic lexicon for one or more categories using only a small set of known category members as seed words and a text corpus. The output is a ranked list of potential category words that a user can review to create a semantic lexicon quickly. The success of this approach depends on the quality of the ranked list, especially the density of category members near the top. In the next section, we describe experiments to evaluate our system. 2It is possible that a word may be near the top of the ranked list during one iteration (and subsequently become a seed word) but become buried at the bottom of the ranked list during later iterations. However, we have not observed this to be a problem so far. We performed experiments with five categories to evaluate the effectiveness and generality of our approach: energy, financial, military, vehicles, and weapons. The MUC-4 development corpus (1700 texts) was used as the text corpus (MUC-4 Proceedings, 1992). We chose these five categories because they represented relatively different semantic classes, they were prevalent in the MUC-4 corpus, and they seemed to be useful categories. For each category, we began with the seed word lists shown in Figure 1. We ran the bootstrapping algorithm for eight iterations, adding five new words to the seed word list after each cycle. After the final iteration, we had ranked lists of potential category words for each of the five categories. The top 45 words3 from each ranked list are shown in Figure 2. While the ranked lists are far from perfect, one can see that there are many category members near the top of each list. It is also apparent that a few additional heuristics could be used to remove many of the extraneous words. For example, our number processor failed to remove numbers with commas (e.g., 2,000). And the military category contains several ordinal numbers (e.g., 10th 3rd 1st) that could be easily identified and removed. But the key question is whether the ranked list contains many true category members. Since this is a subjective question, we set up an experiment involving human judges. For each category, we selected the top 200 words from its ranked list and presented them to a user. We presented the words in random order so that the user had no idea how our system had ranked the words. This was done to minimize contextual effects (e.g., seeing five category members in a row might make someone more inclined to judge the next word as relevant). Each category was judged by two people independently.4 The judges were asked to rate each word on a scale from 1 to 5 indicating how strongly it was associated with the category. Since category judgements can be highly subjective, we gave them guidelines to help establish uniform criteria. The instructions that were given to the judges are shown in Figure 3. We asked the judges to rate the words on a scale from 1 to 5 because different degrees of category membership might be acceptable for different applications. Some applications might require strict cat3 Note that some of these words are not nouns, such as boarded and U.S.-made. Our parser tags unknown words as nouns, so sometimes unknown words are mistakenly selected for context windows. 'The judges were members of our research group but not the authors. aLimon-Covenas refers to an oil pipeline. aLa_Aurora refers to an airport. CRITERIA: On a scale of 0 to 5, rate each word's strength of association with the given category using the following criteria. We'll use the category ANIMAL as an example. 5: CORE MEMBER OF THE CATEGORY: If a word is clearly a member of the category, then it deserves a 5. For example, dogs and sparrows are members of the ANIMAL category. If a word refers to a part of something that is a member of the category, then it deserves a 4. For example, feathers and tails are parts of ANIMALS. If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3. For example, zoos and nests are strongly associated with ANIMALS. If a word refers to something that can be associated with members of the category, but is also associated with many other types of things, then it deserves a 2. For example, bowls and parks are weakly associated with ANIMALS. 1: NO ASSOCIATION WITH THE CATEGORY: If a word has virtually no association with the category, then it deserves a 1. For example, tables and moons have virtually no association with ANIMALS. 0: UNKNOWN WORD: If you do not know what a word means, then it should be labeled with a 0. IMPORTANT! Many words have several distinct meanings. For example, the word &quot;horse&quot; can refer to an animal, a piece of gymnastics equipment, or it can mean to fool around (e.g., &quot;Don't horse around!&quot;). If a word has ANY meaning associated with the given category, then only consider that meaning when assigning numbers. For example, the word &quot;horse&quot; would be a 5 because one of its meanings refers to an ANIMAL. egory membership, for example only words like gun, rifle, and bomb should be labeled as weapons. But from a practical perspective, subparts of category members might also be acceptable. For example, if a cartridge or trigger is mentioned in the context of an event, then one can infer that a gun was used. And for some applications, any word that is strongly associated with a category might be useful to include in the semantic lexicon. For example, words like ammunition or bullets are highly suggestive of a weapon. In the UMass/MUC-4 information extraction system (Lehnert et al., 1992), the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions. The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category. Since the instructions allowed the users to assign a zero to a word if they did not know what it meant, we manually removed the zeros and assigned ratings that we thought were appropriate. We considered ignoring the zeros, but some of the categories would have been severely impacted. For example, many of the legitimate weapons (e.g., M-16 and AR-15) were not known to the judges. Fortunately, most of the unknown words were proper nouns with relatively unambiguous semantics, so we do not believe that this process compromised the integrity of the experiment. Finally, we graphed the results from the human judges. We counted the number of words judged as 5's by either judge, the number of words judged as 5's or 4's by either judge, the number of words judged as 5's, 4's, or 3's by either judge, and the number of words judged as either 5's, 4's, 3's, or 2's. We plotted the results after each 20 words, stepping down the ranked list, to see whether the words near the top of the list were more highly associated with the category than words farther down. We also wanted to see whether the number of category words leveled off or whether it continued to grow. The results from this experiment are shown in Figures 4-8. With the exception of the Energy category, we were able to find 25-45 words that were judged as 4's or 5's for each category. This was our strictest test because only true category members (or subparts of true category members) earned this rating. Although this might not seem like a lot of category words, 25-45 words is enough to produce a reasonable core semantic lexicon. For example, the words judged as 5's for each category are shown in Figure 9. Figure 9 illustrates an important benefit of the corpus-based approach. By sifting through a large text corpus, the algorithm can find many relevant category words that a user would probably not enter in a semantic lexicon on their own. For example, suppose a user wanted to build a dictionary of Vehicle words. Most people would probably define words such as car, truck, plane, and automobile. But it is doubtful that most people would think of words like gunships, fighter, carrier, and ambulances. The corpus-based algorithm is especially good at identifying words that are common in the text corpus even though they might not be commonly used in general. As another example, specific types of weapons (e.g., M-16, AR-15, M-60, or M-79) might not even be known to most users, but they are abundant in the MUC-4 corpus. If we consider all the words rated as 3's, 4's, or 5's, then we were able to find about 50-65 words for every category except Energy. Many of these words would be useful in a semantic dictionary for the category. For example, some of the words rated as 3's for the Vehicle category include: flight, flights, aviation, pilot, airport, and highways. Most of the words rated as 2's are not specific to the target category, but some of them might be useful for certain tasks. For example, some words judged as 2's for the Energy category are: spill, pole, tower, and fields. These words may appear in many different contexts, but in texts about Energy topics these words are likely to be relevant and probably should be defined in the dictionary. Therefore we expect that a user would likely keep some of these words in the semantic lexicon but would probably be very selective. Finally, the graphs show that most of the acquisition curves displayed positive slopes even at the end of the 200 words. This implies that more category words would likely have been found if the users had reviewed more than 200 words. The one exception, again, was the Energy category, which we will discuss in the next section. The size of the ranked lists ranged from 442 for the financial category to 919 for the military category, so it would be interesting to know how many category members would have been found if we had given the entire lists to our judges. When we first began this work, we were unsure about what types of categories would be amenable to this approach. So we experimented with a number of different categories. Fortunately, most of them worked fairly well, but some of them did not. We do not claim to understand exactly what types of categories will work well and which ones will not, but our early experiences did shed some light on the strengths and weaknesses of this approach. In addition to the previous five categories, we also experimented with categories for Location, Commercial, and Person. The Location category performed very well using seed words such as city, town, and province. We didn't formally evaluate this category because most of the category words were proper nouns and we did not expect that our judges would know what they were. But it is worth noting that this category achieved good results, presumably because location names often cluster together in appositives, conjunctions, and nominal compounds. For the Commercial category, we chose seed words such as store, shop, and market. Only a few new commercial words were identified, such as hotel and restaurant. In retrospect, we realized that there were probably few words in the MUC-4 corpus that referred to commercial establishments. (The MUC-4 corpus mainly contains reports of terrorist and military events.) The relatively poor performance of the Energy category was probably due to the same problem. If a category is not well-represented in the corpus then it is doomed because inappropriate words become seed words in the early iterations and quickly derail the feedback loop. The Person category produced mixed results. Some good category words were found, such as rebel, advisers, criminal, and citizen. But many of the words referred to organizations (e.g., FMLN), groups (e.g., forces), and actions (e.g., attacks). Some of these words seemed reasonable, but it was hard to draw a line between specific references to people and concepts like organizations and groups that may or may not consist entirely of people. The large proportion of action words also diluted the list. More experiments are needed to better understand whether this category is inherently difficult or whether a more carefully chosen set of seed words would improve performance. More experiments are also needed to evaluate different seed word lists. The algorithm is clearly sensitive to the initial seed words, but the degree of sensitivity is unknown. For the five categories reported in this paper, we arbitrarily chose a few words that were central members of the category. Our initial seed words worked well enough that we did not experiment with them very much. But we did perform a few experiments varying the number of seed words. In general, we found that additional seed words tend to improve performance, but the results were not substantially different using five seed words or using ten. Of course, there is also a law of diminishing returns: using a seed word list containing 60 category words is almost like creating a semantic lexicon for the category by hand! Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used. But there is no question that semantic knowledge is essential for many problems in natural language processing. Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically. Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994)). Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993)). Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context. To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge. The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing. Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)). Our corpus-based approach is designed to support fast semantic lexicon construction. A user only needs to supply a representative text corpus and a small set of seed words for each target category. Our experiments suggest that a core semantic lexicon can be built for each category with only 10-15 minutes of human interaction. While more work needs to be done to refine this procedure and characterize the types of categories it can handle, we believe that this is a promising approach for corpus-based semantic knowledge acquisition. This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee. We would like to thank David Bean, Jeff Lorenzen, and Kiri Wagstaff for their help in judging our category lists.
Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. The term idiom has been applied to a fuzzy category with prototypical examples such as by and large, kick the bucket, and let the cat out of the bag. Providing a definitive answer for what idioms are, and determining how they are learned and understood, are still subject to debate (Glucksberg, 1993; Nunberg et al., 1994). Nonetheless, they are often defined as phrases or sentences that involve some degree of lexical, syntactic, and/or semantic idiosyncrasy. Idiomatic expressions, as a part of the vast family of figurative language, are widely used both in colloquial speech and in written language. Moreover, a phrase develops its idiomaticity over time (Cacciari, 1993); consequently, new idioms come into existence on a daily basis (Cowie et al., 1983; Seaton and Macaulay, 2002). Idioms thus pose a serious challenge, both for the creation of widecoverage computational lexicons, and for the development of large-scale, linguistically plausible natural language processing (NLP) systems (Sag et al., 2002). One problem is due to the range of syntactic idiosyncrasy of idiomatic expressions. Some idioms, such as by and large, contain syntactic violations; these are often completely fixed and hence can be listed in a lexicon as “words with spaces” (Sag et al., 2002). However, among those idioms that are syntactically well-formed, some exhibit limited morphosyntactic flexibility, while others may be more syntactically flexible. For example, the idiom shoot the breeze undergoes verbal inflection (shot the breeze), but not internal modification or passivization (?shoot the fun breeze, ?the breeze was shot). In contrast, the idiom spill the beans undergoes verbal inflection, internal modification, and even passivization. Clearly, a words-withspaces approach does not capture the full range of behaviour of such idiomatic expressions. Another barrier to the appropriate handling of idioms in a computational system is their semantic idiosyncrasy. This is a particular issue for those idioms that conform to the grammar rules of the language. Such idiomatic expressions are indistinguishable on the surface from compositional (nonidiomatic) phrases, but a computational system must be capable of distinguishing the two. For example, a machine translation system should translate the idiom shoot the breeze as a single unit of meaning (“to chat”), whereas this is not the case for the literal phrase shoot the bird. In this study, we focus on a particular class of English phrasal idioms, i.e., those that involve the combination of a verb plus a noun in its direct object position. Examples include shoot the breeze, pull strings, and push one’s luck. We refer to these as verb+noun idiomatic combinations (VNICs). The class of VNICs accommodates a large number of idiomatic expressions (Cowie et al., 1983; Nunberg et al., 1994). Moreover, their peculiar behaviour signifies the need for a distinct treatment in a computational lexicon (Fellbaum, 2005). Despite this, VNICs have been granted relatively little attention within the computational linguistics community. We look into two closely related problems confronting the appropriate treatment of VNICs: (i) the problem of determining their degree of flexibility; and (ii) the problem of determining their level of idiomaticity. Section 2 elaborates on the lexicosyntactic flexibility of VNICs, and how this relates to their idiomaticity. In Section 3, we propose two linguistically-motivated statistical measures for quantifying the degree of lexical and syntactic inflexibility (or fixedness) of verb+noun combinations. Section 4 presents an evaluation of the proposed measures. In Section 5, we put forward a technique for determining the syntactic variations that a VNIC can undergo, and that should be included in its lexical representation. Section 6 summarizes our contributions. Although syntactically well-formed, VNICs involve a certain degree of semantic idiosyncrasy. Unlike compositional verb+noun combinations, the meaning of VNICs cannot be solely predicted from the meaning of their parts. There is much evidence in the linguistic literature that the semantic idiosyncrasy of idiomatic combinations is reflected in their lexical and/or syntactic behaviour. A limited number of idioms have one (or more) lexical variants, e.g., blow one’s own trumpet and toot one’s own horn (examples from Cowie et al. 1983). However, most are lexically fixed (nonproductive) to a large extent. Neither shoot the wind nor fling the breeze are typically recognized as variations of the idiom shoot the breeze. Similarly, spill the beans has an idiomatic meaning (“to reveal a secret”), while spill the peas and spread the beans have only literal interpretations. Idiomatic combinations are also syntactically peculiar: most VNICs cannot undergo syntactic variations and at the same time retain their idiomatic interpretations. It is important, however, to note that VNICs differ with respect to the degree of syntactic flexibility they exhibit. Some are syntactically inflexible for the most part, while others are more versatile; as illustrated in 1 and 2: Linguists have explained the lexical and syntactic flexibility of idiomatic combinations in terms of their semantic analyzability (e.g., Glucksberg 1993; Fellbaum 1993; Nunberg et al. 1994). Semantic analyzability is inversely related to idiomaticity. For example, the meaning of shoot the breeze, a highly idiomatic expression, has nothing to do with either shoot or breeze. In contrast, a less idiomatic expression, such as spill the beans, can be analyzed as spill corresponding to “reveal” and beans referring to “secret(s)”. Generally, the constituents of a semantically analyzable idiom can be mapped onto their corresponding referents in the idiomatic interpretation. Hence analyzable (less idiomatic) expressions are often more open to lexical substitution and syntactic variation. We use the observed connection between idiomaticity and (in)flexibility to devise statistical measures for automatically distinguishing idiomatic from literal verb+noun combinations. While VNICs vary in their degree of flexibility (cf. 1 and 2 above; see also Moon 1998), on the whole they contrast with compositional phrases, which are more lexically productive and appear in a wider range of syntactic forms. We thus propose to use the degree of lexical and syntactic flexibility of a given verb+noun combination to determine the level of idiomaticity of the expression. It is important to note that semantic analyzability is neither a necessary nor a sufficient condition for an idiomatic combination to be lexically or syntactically flexible. Other factors, such as the communicative intentions and pragmatic constraints, can motivate a speaker to use a variant in place of a canonical form (Glucksberg, 1993). Nevertheless, lexical and syntactic flexibility may well be used as partial indicators of semantic analyzability, and hence idiomaticity. Here we describe our measures for idiomaticity, which quantify the degree of lexical, syntactic, and overall fixedness of a given verb+noun combination, represented as a verb–noun pair. (Note that our measures quantify fixedness, not flexibility.) A VNIC is lexically fixed if the replacement of any of its constituents by a semantically (and syntactically) similar word generally does not result in another VNIC, but in an invalid or a literal expression. One way of measuring lexical fixedness of a given verb+noun combination is thus to examine the idiomaticity of its variants, i.e., expressions generated by replacing one of the constituents by a similar word. This approach has two main challenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it needs information on “similarity” among words. Inspired by Lin (1999), we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity. We use the automatically-built thesaurus of Lin (1998) to find similar words to the noun of the target expression, in order to automatically generate variants. Only the noun constituent is varied, since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC, as in keep/lose one’s cool (Nunberg et al., 1994). Let be the set of the most similar nouns to the noun of the target pair . We calculate the association strength for the target pair, and for each of its variants, , using pointwise mutual information (PMI) (Church et al., 1991): where and is the target noun; is the set of all transitive verbs in the corpus; is the set of all nouns appearing as the direct object of some verb; is the frequency of and occurring as a verb–object pair; is the total frequency of the target verb with any noun in ; is the total frequency of the noun in the direct object position of any verb in . Lin (1999) assumes that a target expression is non-compositional if and only if its value is significantly different from that of any of the variants. Instead, we propose a novel technique that brings together the association strengths ( values) of the target and the variant expressions into a single measure reflecting the degree of lexical fixedness for the target pair. We assume that the target pair is lexically fixed to the extent that its deviates from the average of its variants. Our measure calculates this deviation, normalized using the sample’s standard deviation: Compared to compositional verb+noun combinations, VNICs are expected to appear in more restricted syntactic forms. To quantify the syntactic fixedness of a target verb–noun pair, we thus need to: (i) identify relevant syntactic patterns, i.e., those that help distinguish VNICs from literal verb+noun combinations; (ii) translate the frequency distribution of the target pair in the identified patterns into a measure of syntactic fixedness. Determining a unique set of syntactic patterns appropriate for the recognition of all idiomatic combinations is difficult indeed: exactly which forms an idiomatic combination can occur in is not entirely predictable (Sag et al., 2002). Nonetheless, there are hypotheses about the difference in behaviour of VNICs and literal verb+noun combinations with respect to particular syntactic variations (Nunberg et al., 1994). Linguists note that semantic analyzability is related to the referential status of the noun constituent, which is in turn related to participation in certain morphosyntactic forms. In what follows, we describe three types of variation that are tolerated by literal combinations, but are prohibited by many VNICs. Passivization There is much evidence in the linguistic literature that VNICs often do not undergo passivization.1 Linguists mainly attribute this to the fact that only a referential noun can appear as the surface subject of a passive construction. Determiner Type A strong correlation exists between the flexibility of the determiner preceding the noun in a verb+noun combination and the overall flexibility of the phrase (Fellbaum, 1993). It is however important to note that the nature of the determiner is also affected by other factors, such as the semantic properties of the noun. Pluralization While the verb constituent of a VNIC is morphologically flexible, the morphological flexibility of the noun relates to its referential status. A non-referential noun constituent is expected to mainly appear in just one of the singular or plural forms. The pluralization of the noun is of course also affected by its semantic properties. Merging the three variation types results in a pattern set, , of distinct syntactic patterns, given in Table 1.2 The second step is to devise a statistical measure that quantifies the degree of syntactic fixedness of a verb–noun pair, with respect to the selected set of patterns, . We propose a measure that compares the “syntactic behaviour” of the target pair with that of a “typical” verb–noun pair. Syntactic behaviour of a typical pair is defined as the prior probability distribution over the patterns in . The prior probability of an individual pattern is estimated as: The syntactic behaviour of the target verb–noun pair is defined as the posterior probability distribution over the patterns, given the particular pair. The posterior probability of an individual pattern is estimated as: The degree of syntactic fixedness of the target verb–noun pair is estimated as the divergence of its syntactic behaviour (the posterior distribution 2We collapse some patterns since with a larger pattern set the measure may require larger corpora to perform reliably. over the patterns), from the typical syntactic behaviour (the prior distribution). The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: KL-divergence is always non-negative and is zero if and only if the two distributions are exactly the same. Thus, . KL-divergence is argued to be problematic because it is not a symmetric measure. Nonetheless, it has proven useful in many NLP applications (Resnik, 1999; Dagan et al., 1994). Moreover, the asymmetry is not an issue here since we are concerned with the relative distance of several posterior distributions from the same prior. VNICs are hypothesized to be, in most cases, both lexically and syntactically more fixed than literal verb+noun combinations (see Section 2). We thus propose a new measure of idiomaticity to be a measure of the overall fixedness of a given pair. We define as: where weights the relative contribution of the measures in predicting idiomaticity. To evaluate our proposed fixedness measures, we determine their appropriateness as indicators of idiomaticity. We pose a classification task in which idiomatic verb–noun pairs are distinguished from literal ones. We use each measure to assign scores to the experimental pairs (see Section 4.2 below). We then classify the pairs by setting a threshold, here the median score, where all expressions with scores higher than the threshold are labeled as idiomatic and the rest as literal. We assess the overall goodness of a measure by looking at its accuracy (Acc) and the relative reduction in error rate (RER) on the classification task described above. The RER of a measure reflects the improvement in its accuracy relative to another measure (often a baseline). We consider two baselines: (i) a random baseline, , that randomly assigns a label (literal or idiomatic) to each verb–noun pair; (ii) a more informed baseline, , an information-theoretic measure widely used for extracting statistically significant collocations.3 We use the British National Corpus (BNC; “http://www.natcorp.ox.ac.uk/”) to extract verb– noun pairs, along with information on the syntactic patterns they appear in. We automatically parse the corpus using the Collins parser (Collins, 1999), and further process it using TGrep2 (Rohde, 2004). For each instance of a transitive verb, we use heuristics to extract the noun phrase (NP) in either the direct object position (if the sentence is active), or the subject position (if the sentence is passive). We then use NP-head extraction software4 to get the head noun of the extracted NP, its number (singular or plural), and the determiner introducing it. We select our development and test expressions from verb–noun pairs that involve a member of a predefined list of (transitive) “basic” verbs. Basic verbs, in their literal use, refer to states or acts that are central to human experience. They are thus frequent, highly polysemous, and tend to combine with other words to form idiomatic combinations (Nunberg et al., 1994). An initial list of such verbs was selected from several linguistic and psycholinguistic studies on basic vocabulary (e.g., Pauwels 2000; Newman and Rice 2004). We further augmented this initial list with verbs that are semantically related to another verb already in the From the corpus, we extract all verb–noun pairs with minimum frequency of that contain a basic verb. From these, we semi-randomly select an idiomatic and a literal subset.5 A pair is considered idiomatic if it appears in a credible idiom dictionary, such as the Oxford Dictionary of Current Idiomatic English (ODCIE) (Cowie et al., 1983), or the Collins COBUILD Idioms Dictionary (CCID) (Seaton and Macaulay, 2002). Otherwise, the pair is considered literal. We then randomly pull out development and test pairs (half idiomatic and half literal), ensuring both low and high frequency items are included. Sample idioms corresponding to the extracted pairs are: kick the habit, move mountains, lose face, and keep one’s word. Development expressions are used in devising the fixedness measures, as well as in determining the values of the parameters in Eqn. (2) and in Eqn. (4). determines the maximum number of nouns similar to the target noun, to be considered in measuring the lexical fixedness of a given pair. The value of this parameter is determined by performing experiments over the development data, in which ranges from to by steps of ; is set to based on the results. We also experimented with different values of ranging from to by steps of . Based on the development results, the best value for is (giving more weight to the syntactic fixedness measure). Test expressions are saved as unseen data for the final evaluation. We further divide the set of all test expressions, TEST , into two sets corresponding to two frequency bands: TEST contains idiomatic and literal pairs, each with total frequency between and ( ); TEST consists of idiomatic and literal pairs, each with total frequency of or greater ( ). All frequency counts are over the entire BNC. We first examine the performance of the individual fixedness measures, and 5In selecting literal pairs, we choose those that involve a physical act corresponding to the basic semantics of the verb. , as well as that of the two baselines, and ; see Table 2. (Results for the overall measure are presented later in this section.) As can be seen, the informed baseline, , shows a large improvement over the random baseline ( error reduction). This shows that one can get relatively good performance by treating verb+noun idiomatic combinations as collocations. performs as well as the informed baseline ( error reduction). This result shows that, as hypothesized, lexical fixedness is a reasonably good predictor of idiomaticity. Nonetheless, the performance signifies a need for improvement. Possibly the most beneficial enhancement would be a change in the way we acquire the similar nouns for a target noun. The best performance (shown in boldface) belongs to , with error reduction over the random baseline, and error reduction over the informed baseline. These results demonstrate that syntactic fixedness is a good indicator of idiomaticity, better than a simple measure of collocation ( ), or a measure of lexical fixedness. These results further suggest that looking into deep linguistic properties of VNICs is both necessary and beneficial for the appropriate treatment of these expressions. is known to perform poorly on low frequency data. To examine the effect of frequency on the measures, we analyze their performance on the two divisions of the test data, corresponding to the two frequency bands, TEST and TEST . Results are given in Table 3, with the best performance shown in boldface. As expected, the performance of drops substantially for low frequency items. Interestingly, although it is a PMI-based measure, performs slightly better when the data is separated based on frequency. The performance of improves quite a bit when it is applied to high frequency items, while it improves only slightly on the low frequency items. These results show that both Fixedness measures perform better on homogeneous data, while retaining comparably good performance on heterogeneous data. These results reflect that our fixedness measures are not as sensitive to frequency as . Hence they can be used with a higher degree of confidence, especially when applied to data that is heterogeneous with regard to frequency. This is important because while some VNICs are very common, others have very low frequency. Table 4 presents the performance of the hybrid measure, , repeating that of and for comparison. outperforms both lexical and syntactic fixedness measures, with a substantial improvement over , and a small, but notable, improvement over . Each of the lexical and syntactic fixedness measures is a good indicator of idiomaticity on its own, with syntactic fixedness being a better predictor. Here we demonstrate that combining them into a single measure of fixedness, while giving more weight to the better measure, results in a more effective predictor of idiomaticity. Our evaluation of the fixedness measures demonstrates their usefulness for the automatic recognition of idiomatic verb–noun pairs. To represent such pairs in a lexicon, however, we must determine their canonical form(s)—Cforms henceforth. For example, the lexical representation of shoot, breeze should include shoot the breeze as a Cform. Since VNICs are syntactically fixed, they are mostly expected to have a single Cform. Nonetheless, there are idioms with two or more acceptable forms. For example, hold fire and hold one’s fire are both listed in CCID as variations of the same idiom. Our approach should thus be capable of predicting all allowable forms for a given idiomatic verb–noun pair. We expect a VNIC to occur in its Cform(s) more frequently than it occurs in any other syntactic patterns. To discover the Cform(s) for a given idiomatic verb–noun pair, we thus examine its frequency of occurrence in each syntactic pattern in . Since it is possible for an idiom to have more than one Cform, we cannot simply take the most dominant pattern as the canonical one. Instead, we calculate a -score for the target pair and each pattern : in which is the mean and the standard deviation over the sample . The statistic indicates how far and in which direction the frequency of occurrence of the pair in pattern deviates from the sample’s mean, expressed in units of the sample’s standard deviation. To decide whether is a canonical pattern for the target pair, we check whether , where is a threshold. For evaluation, we set to , based on the distribution of and through examining the development data. We evaluate the appropriateness of this approach in determining the Cform(s) of idiomatic pairs by verifying its predicted forms against ODCIE and CCID. Specifically, for each of the idiomatic pairs in TEST , we calculate the precision and recall of its predicted Cforms (those whose -scores are above ), compared to the Cforms listed in the two dictionaries. The average precision across the 100 test pairs is 81.7%, and the average recall is 88.0% (with 69 of the pairs having 100% precision and 100% recall). Moreover, we find that for the overwhelming majority of the pairs, , the predicted Cform with the highest -score appears in the dictionary entry of the pair. Thus, our method of detecting Cforms performs quite well. The significance of the role idioms play in language has long been recognized. However, due to their peculiar behaviour, idioms have been mostly overlooked by the NLP community. Recently, there has been growing awareness of the importance of identifying non-compositional multiword expressions (MWEs). Nonetheless, most research on the topic has focused on compound nouns and verb particle constructions. Earlier work on idioms have only touched the surface of the problem, failing to propose explicit mechanisms for appropriately handling them. Here, we provide effective mechanisms for the treatment of a broadly documented and crosslinguistically frequent class of idioms, i.e., VNICs. Earlier research on the lexical encoding of idioms mainly relied on the existence of human annotations, especially for detecting which syntactic variations (e.g., passivization) an idiom can undergo (Villavicencio et al., 2004). We propose techniques for the automatic acquisition and encoding of knowledge about the lexicosyntactic behaviour of idiomatic combinations. We put forward a means for automatically discovering the set of syntactic variations that are tolerated by a VNIC and that should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal (compositional) combinations. Moreover, we suggest novel techniques for translating such characteristics into measures that predict the idiomaticity level of verb+noun combinations. More specifically, we propose statistical measures that quantify the degree of lexical, syntactic, and overall fixedness of such combinations. We demonstrate that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non-idiomatic ones. We also show that our syntactic and overall fixedness measures substantially outperform a widely used measure of collocation, , even when the latter takes syntactic relations into account. Others have also drawn on the notion of syntactic fixedness for idiom detection, though specific to a highly constrained type of idiom (Widdows and Dorow, 2005). Our syntactic fixedness measure looks into a broader set of patterns associated with a large class of idiomatic expressions. Moreover, our approach is general and can be easily extended to other idiomatic combinations. Each measure we use to identify VNICs captures a different aspect of idiomaticity: reflects the statistical idiosyncrasy of VNICs, while the fixedness measures draw on their lexicosyntactic peculiarities. Our ongoing work focuses on combining these measures to distinguish VNICs from other idiosyncratic verb+noun combinations that are neither purely idiomatic nor completely literal, so that we can identify linguistically plausible classes of verb+noun combinations on this continuum (Fazly and Stevenson, 2005).
